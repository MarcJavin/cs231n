{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.389737\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.641340 analytic: -0.641340, relative error: 1.506883e-08\n",
      "numerical: -0.010003 analytic: -0.010003, relative error: 2.795222e-06\n",
      "numerical: -0.931224 analytic: -0.931224, relative error: 1.112500e-09\n",
      "numerical: 5.497173 analytic: 5.497173, relative error: 1.381852e-08\n",
      "numerical: 1.769021 analytic: 1.769021, relative error: 7.679968e-09\n",
      "numerical: -0.175865 analytic: -0.175865, relative error: 9.618376e-08\n",
      "numerical: 2.201989 analytic: 2.201989, relative error: 1.375673e-08\n",
      "numerical: -1.213086 analytic: -1.213086, relative error: 2.448342e-08\n",
      "numerical: 1.680312 analytic: 1.680312, relative error: 8.085596e-09\n",
      "numerical: -1.093710 analytic: -1.093710, relative error: 9.963322e-09\n",
      "numerical: 2.067876 analytic: 2.067876, relative error: 3.833581e-08\n",
      "numerical: -0.352411 analytic: -0.352411, relative error: 6.593495e-08\n",
      "numerical: -0.490671 analytic: -0.490671, relative error: 2.357455e-08\n",
      "numerical: 1.578729 analytic: 1.578729, relative error: 1.045418e-08\n",
      "numerical: 0.095244 analytic: 0.095244, relative error: 2.199291e-08\n",
      "numerical: -0.782655 analytic: -0.782655, relative error: 9.102653e-09\n",
      "numerical: -0.671784 analytic: -0.671784, relative error: 2.476542e-09\n",
      "numerical: -2.412400 analytic: -2.412400, relative error: 5.985043e-09\n",
      "numerical: 0.195333 analytic: 0.195333, relative error: 1.443088e-07\n",
      "numerical: 1.682716 analytic: 1.682716, relative error: 1.847372e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.389737e+00 computed in 0.281843s\n",
      "vectorized loss: 2.389737e+00 computed in 0.011399s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 979.177070\n",
      "iteration 100 / 5000: loss 2.275389\n",
      "iteration 200 / 5000: loss 2.177235\n",
      "iteration 300 / 5000: loss 2.159611\n",
      "iteration 400 / 5000: loss 2.074882\n",
      "iteration 500 / 5000: loss 2.102919\n",
      "iteration 600 / 5000: loss 2.073061\n",
      "iteration 700 / 5000: loss 2.101845\n",
      "iteration 800 / 5000: loss 2.160585\n",
      "iteration 900 / 5000: loss 2.199714\n",
      "iteration 1000 / 5000: loss 2.063867\n",
      "iteration 1100 / 5000: loss 2.151420\n",
      "iteration 1200 / 5000: loss 2.123039\n",
      "iteration 1300 / 5000: loss 2.080241\n",
      "iteration 1400 / 5000: loss 2.109586\n",
      "iteration 1500 / 5000: loss 2.155760\n",
      "iteration 1600 / 5000: loss 2.111534\n",
      "iteration 1700 / 5000: loss 2.053014\n",
      "iteration 1800 / 5000: loss 2.103996\n",
      "iteration 1900 / 5000: loss 2.128814\n",
      "iteration 2000 / 5000: loss 2.175843\n",
      "iteration 2100 / 5000: loss 2.155338\n",
      "iteration 2200 / 5000: loss 2.119367\n",
      "iteration 2300 / 5000: loss 2.036227\n",
      "iteration 2400 / 5000: loss 2.163935\n",
      "iteration 2500 / 5000: loss 2.154829\n",
      "iteration 2600 / 5000: loss 2.134813\n",
      "iteration 2700 / 5000: loss 2.123298\n",
      "iteration 2800 / 5000: loss 2.106436\n",
      "iteration 2900 / 5000: loss 2.074994\n",
      "iteration 3000 / 5000: loss 2.159039\n",
      "iteration 3100 / 5000: loss 2.079549\n",
      "iteration 3200 / 5000: loss 2.095197\n",
      "iteration 3300 / 5000: loss 2.041555\n",
      "iteration 3400 / 5000: loss 2.111258\n",
      "iteration 3500 / 5000: loss 2.143836\n",
      "iteration 3600 / 5000: loss 2.119624\n",
      "iteration 3700 / 5000: loss 2.140034\n",
      "iteration 3800 / 5000: loss 2.020532\n",
      "iteration 3900 / 5000: loss 2.126885\n",
      "iteration 4000 / 5000: loss 2.097021\n",
      "iteration 4100 / 5000: loss 2.141770\n",
      "iteration 4200 / 5000: loss 2.099469\n",
      "iteration 4300 / 5000: loss 2.131778\n",
      "iteration 4400 / 5000: loss 2.163260\n",
      "iteration 4500 / 5000: loss 2.147221\n",
      "iteration 4600 / 5000: loss 2.080577\n",
      "iteration 4700 / 5000: loss 2.145313\n",
      "iteration 4800 / 5000: loss 2.134210\n",
      "iteration 4900 / 5000: loss 2.121983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/50 [00:39<32:17, 39.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 994.547868\n",
      "iteration 100 / 5000: loss 2.462375\n",
      "iteration 200 / 5000: loss 2.084955\n",
      "iteration 300 / 5000: loss 2.099155\n",
      "iteration 400 / 5000: loss 2.157966\n",
      "iteration 500 / 5000: loss 2.123094\n",
      "iteration 600 / 5000: loss 2.137115\n",
      "iteration 700 / 5000: loss 2.101366\n",
      "iteration 800 / 5000: loss 2.137940\n",
      "iteration 900 / 5000: loss 2.134885\n",
      "iteration 1000 / 5000: loss 2.130627\n",
      "iteration 1100 / 5000: loss 2.117720\n",
      "iteration 1200 / 5000: loss 2.089474\n",
      "iteration 1300 / 5000: loss 2.079600\n",
      "iteration 1400 / 5000: loss 2.137095\n",
      "iteration 1500 / 5000: loss 2.160907\n",
      "iteration 1600 / 5000: loss 2.155051\n",
      "iteration 1700 / 5000: loss 2.110549\n",
      "iteration 1800 / 5000: loss 2.101808\n",
      "iteration 1900 / 5000: loss 2.111972\n",
      "iteration 2000 / 5000: loss 2.077877\n",
      "iteration 2100 / 5000: loss 2.200947\n",
      "iteration 2200 / 5000: loss 2.152299\n",
      "iteration 2300 / 5000: loss 2.104822\n",
      "iteration 2400 / 5000: loss 2.110177\n",
      "iteration 2500 / 5000: loss 2.060208\n",
      "iteration 2600 / 5000: loss 2.119262\n",
      "iteration 2700 / 5000: loss 2.133822\n",
      "iteration 2800 / 5000: loss 2.088184\n",
      "iteration 2900 / 5000: loss 2.130632\n",
      "iteration 3000 / 5000: loss 2.093043\n",
      "iteration 3100 / 5000: loss 2.062221\n",
      "iteration 3200 / 5000: loss 2.171466\n",
      "iteration 3300 / 5000: loss 2.097669\n",
      "iteration 3400 / 5000: loss 2.121526\n",
      "iteration 3500 / 5000: loss 2.158140\n",
      "iteration 3600 / 5000: loss 2.062539\n",
      "iteration 3700 / 5000: loss 2.118169\n",
      "iteration 3800 / 5000: loss 2.124755\n",
      "iteration 3900 / 5000: loss 2.166429\n",
      "iteration 4000 / 5000: loss 2.121707\n",
      "iteration 4100 / 5000: loss 2.067958\n",
      "iteration 4200 / 5000: loss 2.172087\n",
      "iteration 4300 / 5000: loss 2.111075\n",
      "iteration 4400 / 5000: loss 2.092788\n",
      "iteration 4500 / 5000: loss 2.095261\n",
      "iteration 4600 / 5000: loss 2.131138\n",
      "iteration 4700 / 5000: loss 2.096005\n",
      "iteration 4800 / 5000: loss 2.135979\n",
      "iteration 4900 / 5000: loss 2.111886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 2/50 [01:11<28:38, 35.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1303.418173\n",
      "iteration 100 / 5000: loss 2.139424\n",
      "iteration 200 / 5000: loss 2.099782\n",
      "iteration 300 / 5000: loss 2.160238\n",
      "iteration 400 / 5000: loss 2.168006\n",
      "iteration 500 / 5000: loss 2.108503\n",
      "iteration 600 / 5000: loss 2.155467\n",
      "iteration 700 / 5000: loss 2.138136\n",
      "iteration 800 / 5000: loss 2.151273\n",
      "iteration 900 / 5000: loss 2.131764\n",
      "iteration 1000 / 5000: loss 2.127975\n",
      "iteration 1100 / 5000: loss 2.114101\n",
      "iteration 1200 / 5000: loss 2.107372\n",
      "iteration 1300 / 5000: loss 2.157415\n",
      "iteration 1400 / 5000: loss 2.134489\n",
      "iteration 1500 / 5000: loss 2.107706\n",
      "iteration 1600 / 5000: loss 2.137461\n",
      "iteration 1700 / 5000: loss 2.142921\n",
      "iteration 1800 / 5000: loss 2.105078\n",
      "iteration 1900 / 5000: loss 2.143049\n",
      "iteration 2000 / 5000: loss 2.096221\n",
      "iteration 2100 / 5000: loss 2.090590\n",
      "iteration 2200 / 5000: loss 2.155473\n",
      "iteration 2300 / 5000: loss 2.117272\n",
      "iteration 2400 / 5000: loss 2.082443\n",
      "iteration 2500 / 5000: loss 2.116282\n",
      "iteration 2600 / 5000: loss 2.170967\n",
      "iteration 2700 / 5000: loss 2.164446\n",
      "iteration 2800 / 5000: loss 2.111973\n",
      "iteration 2900 / 5000: loss 2.186744\n",
      "iteration 3000 / 5000: loss 2.132181\n",
      "iteration 3100 / 5000: loss 2.160340\n",
      "iteration 3200 / 5000: loss 2.133340\n",
      "iteration 3300 / 5000: loss 2.065254\n",
      "iteration 3400 / 5000: loss 2.124764\n",
      "iteration 3500 / 5000: loss 2.167581\n",
      "iteration 3600 / 5000: loss 2.095406\n",
      "iteration 3700 / 5000: loss 2.111019\n",
      "iteration 3800 / 5000: loss 2.169630\n",
      "iteration 3900 / 5000: loss 2.119945\n",
      "iteration 4000 / 5000: loss 2.143360\n",
      "iteration 4100 / 5000: loss 2.115598\n",
      "iteration 4200 / 5000: loss 2.214294\n",
      "iteration 4300 / 5000: loss 2.177379\n",
      "iteration 4400 / 5000: loss 2.086020\n",
      "iteration 4500 / 5000: loss 2.173883\n",
      "iteration 4600 / 5000: loss 2.104465\n",
      "iteration 4700 / 5000: loss 2.162294\n",
      "iteration 4800 / 5000: loss 2.149883\n",
      "iteration 4900 / 5000: loss 2.072470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 3/50 [01:44<27:15, 34.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1123.229253\n",
      "iteration 100 / 5000: loss 2.121554\n",
      "iteration 200 / 5000: loss 2.162520\n",
      "iteration 300 / 5000: loss 2.104881\n",
      "iteration 400 / 5000: loss 2.163413\n",
      "iteration 500 / 5000: loss 2.158080\n",
      "iteration 600 / 5000: loss 2.123891\n",
      "iteration 700 / 5000: loss 2.116639\n",
      "iteration 800 / 5000: loss 2.123347\n",
      "iteration 900 / 5000: loss 2.157111\n",
      "iteration 1000 / 5000: loss 2.111837\n",
      "iteration 1100 / 5000: loss 2.122925\n",
      "iteration 1200 / 5000: loss 2.157375\n",
      "iteration 1300 / 5000: loss 2.118248\n",
      "iteration 1400 / 5000: loss 2.142293\n",
      "iteration 1500 / 5000: loss 2.098664\n",
      "iteration 1600 / 5000: loss 2.163052\n",
      "iteration 1700 / 5000: loss 2.144038\n",
      "iteration 1800 / 5000: loss 2.153837\n",
      "iteration 1900 / 5000: loss 2.133310\n",
      "iteration 2000 / 5000: loss 2.180761\n",
      "iteration 2100 / 5000: loss 2.171177\n",
      "iteration 2200 / 5000: loss 2.116312\n",
      "iteration 2300 / 5000: loss 2.154898\n",
      "iteration 2400 / 5000: loss 2.111991\n",
      "iteration 2500 / 5000: loss 2.117673\n",
      "iteration 2600 / 5000: loss 2.123851\n",
      "iteration 2700 / 5000: loss 2.038758\n",
      "iteration 2800 / 5000: loss 2.144373\n",
      "iteration 2900 / 5000: loss 2.152125\n",
      "iteration 3000 / 5000: loss 2.131352\n",
      "iteration 3100 / 5000: loss 2.190298\n",
      "iteration 3200 / 5000: loss 2.097060\n",
      "iteration 3300 / 5000: loss 2.113659\n",
      "iteration 3400 / 5000: loss 2.127732\n",
      "iteration 3500 / 5000: loss 2.172198\n",
      "iteration 3600 / 5000: loss 2.145658\n",
      "iteration 3700 / 5000: loss 2.143313\n",
      "iteration 3800 / 5000: loss 2.100575\n",
      "iteration 3900 / 5000: loss 2.165811\n",
      "iteration 4000 / 5000: loss 2.143966\n",
      "iteration 4100 / 5000: loss 2.203464\n",
      "iteration 4200 / 5000: loss 2.104257\n",
      "iteration 4300 / 5000: loss 2.086950\n",
      "iteration 4400 / 5000: loss 2.137120\n",
      "iteration 4500 / 5000: loss 2.145567\n",
      "iteration 4600 / 5000: loss 2.129544\n",
      "iteration 4700 / 5000: loss 2.101524\n",
      "iteration 4800 / 5000: loss 2.105296\n",
      "iteration 4900 / 5000: loss 2.163523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|▊         | 4/50 [02:17<26:19, 34.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1310.394113\n",
      "iteration 100 / 5000: loss 7.396658\n",
      "iteration 200 / 5000: loss 2.224821\n",
      "iteration 300 / 5000: loss 2.098866\n",
      "iteration 400 / 5000: loss 2.097737\n",
      "iteration 500 / 5000: loss 2.158907\n",
      "iteration 600 / 5000: loss 2.112738\n",
      "iteration 700 / 5000: loss 2.160389\n",
      "iteration 800 / 5000: loss 2.145921\n",
      "iteration 900 / 5000: loss 2.121456\n",
      "iteration 1000 / 5000: loss 2.185911\n",
      "iteration 1100 / 5000: loss 2.149455\n",
      "iteration 1200 / 5000: loss 2.116525\n",
      "iteration 1300 / 5000: loss 2.129231\n",
      "iteration 1400 / 5000: loss 2.135823\n",
      "iteration 1500 / 5000: loss 2.098330\n",
      "iteration 1600 / 5000: loss 2.133951\n",
      "iteration 1700 / 5000: loss 2.065221\n",
      "iteration 1800 / 5000: loss 2.101670\n",
      "iteration 1900 / 5000: loss 2.145398\n",
      "iteration 2000 / 5000: loss 2.159864\n",
      "iteration 2100 / 5000: loss 2.168037\n",
      "iteration 2200 / 5000: loss 2.096486\n",
      "iteration 2300 / 5000: loss 2.084857\n",
      "iteration 2400 / 5000: loss 2.232294\n",
      "iteration 2500 / 5000: loss 2.160345\n",
      "iteration 2600 / 5000: loss 2.125962\n",
      "iteration 2700 / 5000: loss 2.119918\n",
      "iteration 2800 / 5000: loss 2.117615\n",
      "iteration 2900 / 5000: loss 2.076123\n",
      "iteration 3000 / 5000: loss 2.136539\n",
      "iteration 3100 / 5000: loss 2.141220\n",
      "iteration 3200 / 5000: loss 2.154581\n",
      "iteration 3300 / 5000: loss 2.099526\n",
      "iteration 3400 / 5000: loss 2.176150\n",
      "iteration 3500 / 5000: loss 2.078147\n",
      "iteration 3600 / 5000: loss 2.116499\n",
      "iteration 3700 / 5000: loss 2.148053\n",
      "iteration 3800 / 5000: loss 2.103882\n",
      "iteration 3900 / 5000: loss 2.172560\n",
      "iteration 4000 / 5000: loss 2.090220\n",
      "iteration 4100 / 5000: loss 2.137909\n",
      "iteration 4200 / 5000: loss 2.126710\n",
      "iteration 4300 / 5000: loss 2.132724\n",
      "iteration 4400 / 5000: loss 2.078953\n",
      "iteration 4500 / 5000: loss 2.144802\n",
      "iteration 4600 / 5000: loss 2.180644\n",
      "iteration 4700 / 5000: loss 2.151064\n",
      "iteration 4800 / 5000: loss 2.155582\n",
      "iteration 4900 / 5000: loss 2.130928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 5/50 [02:49<25:29, 34.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1008.045017\n",
      "iteration 100 / 5000: loss 2.095771\n",
      "iteration 200 / 5000: loss 2.141976\n",
      "iteration 300 / 5000: loss 2.051020\n",
      "iteration 400 / 5000: loss 2.102944\n",
      "iteration 500 / 5000: loss 2.100574\n",
      "iteration 600 / 5000: loss 2.093017\n",
      "iteration 700 / 5000: loss 2.175996\n",
      "iteration 800 / 5000: loss 2.149513\n",
      "iteration 900 / 5000: loss 2.064503\n",
      "iteration 1000 / 5000: loss 2.137650\n",
      "iteration 1100 / 5000: loss 2.129987\n",
      "iteration 1200 / 5000: loss 2.148293\n",
      "iteration 1300 / 5000: loss 2.079563\n",
      "iteration 1400 / 5000: loss 2.151501\n",
      "iteration 1500 / 5000: loss 2.071024\n",
      "iteration 1600 / 5000: loss 2.139373\n",
      "iteration 1700 / 5000: loss 2.150404\n",
      "iteration 1800 / 5000: loss 2.144088\n",
      "iteration 1900 / 5000: loss 2.182502\n",
      "iteration 2000 / 5000: loss 2.089800\n",
      "iteration 2100 / 5000: loss 2.084987\n",
      "iteration 2200 / 5000: loss 2.144656\n",
      "iteration 2300 / 5000: loss 2.114051\n",
      "iteration 2400 / 5000: loss 2.098259\n",
      "iteration 2500 / 5000: loss 2.088087\n",
      "iteration 2600 / 5000: loss 2.089278\n",
      "iteration 2700 / 5000: loss 2.140806\n",
      "iteration 2800 / 5000: loss 2.142786\n",
      "iteration 2900 / 5000: loss 2.079180\n",
      "iteration 3000 / 5000: loss 2.150933\n",
      "iteration 3100 / 5000: loss 2.095585\n",
      "iteration 3200 / 5000: loss 2.128380\n",
      "iteration 3300 / 5000: loss 2.104566\n",
      "iteration 3400 / 5000: loss 2.145808\n",
      "iteration 3500 / 5000: loss 2.112051\n",
      "iteration 3600 / 5000: loss 2.133862\n",
      "iteration 3700 / 5000: loss 2.148739\n",
      "iteration 3800 / 5000: loss 2.168700\n",
      "iteration 3900 / 5000: loss 2.143286\n",
      "iteration 4000 / 5000: loss 2.157648\n",
      "iteration 4100 / 5000: loss 2.157537\n",
      "iteration 4200 / 5000: loss 2.130447\n",
      "iteration 4300 / 5000: loss 2.113431\n",
      "iteration 4400 / 5000: loss 2.138177\n",
      "iteration 4500 / 5000: loss 2.085617\n",
      "iteration 4600 / 5000: loss 2.109399\n",
      "iteration 4700 / 5000: loss 2.109016\n",
      "iteration 4800 / 5000: loss 2.158268\n",
      "iteration 4900 / 5000: loss 2.216106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 6/50 [03:22<24:46, 33.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 958.988959\n",
      "iteration 100 / 5000: loss 2.191759\n",
      "iteration 200 / 5000: loss 2.078888\n",
      "iteration 300 / 5000: loss 2.159583\n",
      "iteration 400 / 5000: loss 2.128137\n",
      "iteration 500 / 5000: loss 2.115926\n",
      "iteration 600 / 5000: loss 2.131551\n",
      "iteration 700 / 5000: loss 2.113164\n",
      "iteration 800 / 5000: loss 2.092195\n",
      "iteration 900 / 5000: loss 2.116950\n",
      "iteration 1000 / 5000: loss 2.064943\n",
      "iteration 1100 / 5000: loss 2.138578\n",
      "iteration 1200 / 5000: loss 2.145620\n",
      "iteration 1300 / 5000: loss 2.130554\n",
      "iteration 1400 / 5000: loss 2.124577\n",
      "iteration 1500 / 5000: loss 2.125376\n",
      "iteration 1600 / 5000: loss 2.117575\n",
      "iteration 1700 / 5000: loss 2.151894\n",
      "iteration 1800 / 5000: loss 2.122592\n",
      "iteration 1900 / 5000: loss 2.087553\n",
      "iteration 2000 / 5000: loss 2.143419\n",
      "iteration 2100 / 5000: loss 2.058086\n",
      "iteration 2200 / 5000: loss 2.130477\n",
      "iteration 2300 / 5000: loss 2.069828\n",
      "iteration 2400 / 5000: loss 2.120830\n",
      "iteration 2500 / 5000: loss 2.084630\n",
      "iteration 2600 / 5000: loss 2.062777\n",
      "iteration 2700 / 5000: loss 2.185323\n",
      "iteration 2800 / 5000: loss 2.164442\n",
      "iteration 2900 / 5000: loss 2.113808\n",
      "iteration 3000 / 5000: loss 2.134251\n",
      "iteration 3100 / 5000: loss 2.158912\n",
      "iteration 3200 / 5000: loss 2.062536\n",
      "iteration 3300 / 5000: loss 2.119068\n",
      "iteration 3400 / 5000: loss 2.155392\n",
      "iteration 3500 / 5000: loss 2.134549\n",
      "iteration 3600 / 5000: loss 2.087638\n",
      "iteration 3700 / 5000: loss 2.115325\n",
      "iteration 3800 / 5000: loss 2.089224\n",
      "iteration 3900 / 5000: loss 2.095491\n",
      "iteration 4000 / 5000: loss 2.085316\n",
      "iteration 4100 / 5000: loss 2.172710\n",
      "iteration 4200 / 5000: loss 2.095143\n",
      "iteration 4300 / 5000: loss 2.141622\n",
      "iteration 4400 / 5000: loss 2.016037\n",
      "iteration 4500 / 5000: loss 2.173040\n",
      "iteration 4600 / 5000: loss 2.120718\n",
      "iteration 4700 / 5000: loss 2.153288\n",
      "iteration 4800 / 5000: loss 2.111167\n",
      "iteration 4900 / 5000: loss 2.081077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 14%|█▍        | 7/50 [03:54<24:01, 33.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 599.331631\n",
      "iteration 100 / 5000: loss 42.495211\n",
      "iteration 200 / 5000: loss 4.818585\n",
      "iteration 300 / 5000: loss 2.209457\n",
      "iteration 400 / 5000: loss 2.123215\n",
      "iteration 500 / 5000: loss 2.062336\n",
      "iteration 600 / 5000: loss 2.088251\n",
      "iteration 700 / 5000: loss 2.020768\n",
      "iteration 800 / 5000: loss 2.070628\n",
      "iteration 900 / 5000: loss 2.105532\n",
      "iteration 1000 / 5000: loss 2.077367\n",
      "iteration 1100 / 5000: loss 2.074301\n",
      "iteration 1200 / 5000: loss 2.086513\n",
      "iteration 1300 / 5000: loss 2.025959\n",
      "iteration 1400 / 5000: loss 2.045367\n",
      "iteration 1500 / 5000: loss 2.087624\n",
      "iteration 1600 / 5000: loss 2.075173\n",
      "iteration 1700 / 5000: loss 2.080326\n",
      "iteration 1800 / 5000: loss 2.073435\n",
      "iteration 1900 / 5000: loss 2.089229\n",
      "iteration 2000 / 5000: loss 2.087130\n",
      "iteration 2100 / 5000: loss 2.057695\n",
      "iteration 2200 / 5000: loss 2.048933\n",
      "iteration 2300 / 5000: loss 2.073082\n",
      "iteration 2400 / 5000: loss 2.114783\n",
      "iteration 2500 / 5000: loss 2.068182\n",
      "iteration 2600 / 5000: loss 2.079487\n",
      "iteration 2700 / 5000: loss 2.021547\n",
      "iteration 2800 / 5000: loss 2.104968\n",
      "iteration 2900 / 5000: loss 2.091308\n",
      "iteration 3000 / 5000: loss 2.004794\n",
      "iteration 3100 / 5000: loss 2.089905\n",
      "iteration 3200 / 5000: loss 2.036894\n",
      "iteration 3300 / 5000: loss 2.009632\n",
      "iteration 3400 / 5000: loss 2.002040\n",
      "iteration 3500 / 5000: loss 2.127167\n",
      "iteration 3600 / 5000: loss 2.114081\n",
      "iteration 3700 / 5000: loss 2.087093\n",
      "iteration 3800 / 5000: loss 2.084149\n",
      "iteration 3900 / 5000: loss 2.023927\n",
      "iteration 4000 / 5000: loss 1.980857\n",
      "iteration 4100 / 5000: loss 2.059342\n",
      "iteration 4200 / 5000: loss 2.002084\n",
      "iteration 4300 / 5000: loss 2.008054\n",
      "iteration 4400 / 5000: loss 2.100383\n",
      "iteration 4500 / 5000: loss 2.032204\n",
      "iteration 4600 / 5000: loss 2.061783\n",
      "iteration 4700 / 5000: loss 2.051761\n",
      "iteration 4800 / 5000: loss 2.059581\n",
      "iteration 4900 / 5000: loss 2.117344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▌        | 8/50 [04:27<23:25, 33.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 729.582128\n",
      "iteration 100 / 5000: loss 120.595123\n",
      "iteration 200 / 5000: loss 21.413727\n",
      "iteration 300 / 5000: loss 5.256270\n",
      "iteration 400 / 5000: loss 2.597574\n",
      "iteration 500 / 5000: loss 2.193696\n",
      "iteration 600 / 5000: loss 2.063432\n",
      "iteration 700 / 5000: loss 2.126754\n",
      "iteration 800 / 5000: loss 2.067751\n",
      "iteration 900 / 5000: loss 2.083489\n",
      "iteration 1000 / 5000: loss 2.077184\n",
      "iteration 1100 / 5000: loss 2.032148\n",
      "iteration 1200 / 5000: loss 2.081408\n",
      "iteration 1300 / 5000: loss 2.080582\n",
      "iteration 1400 / 5000: loss 2.076740\n",
      "iteration 1500 / 5000: loss 2.116990\n",
      "iteration 1600 / 5000: loss 2.058509\n",
      "iteration 1700 / 5000: loss 2.049587\n",
      "iteration 1800 / 5000: loss 2.066490\n",
      "iteration 1900 / 5000: loss 2.111963\n",
      "iteration 2000 / 5000: loss 2.111224\n",
      "iteration 2100 / 5000: loss 2.039895\n",
      "iteration 2200 / 5000: loss 2.108876\n",
      "iteration 2300 / 5000: loss 2.061191\n",
      "iteration 2400 / 5000: loss 2.062848\n",
      "iteration 2500 / 5000: loss 2.099192\n",
      "iteration 2600 / 5000: loss 2.063311\n",
      "iteration 2700 / 5000: loss 2.071178\n",
      "iteration 2800 / 5000: loss 2.089141\n",
      "iteration 2900 / 5000: loss 2.059503\n",
      "iteration 3000 / 5000: loss 2.082550\n",
      "iteration 3100 / 5000: loss 2.081602\n",
      "iteration 3200 / 5000: loss 2.164159\n",
      "iteration 3300 / 5000: loss 2.083632\n",
      "iteration 3400 / 5000: loss 2.048523\n",
      "iteration 3500 / 5000: loss 2.035345\n",
      "iteration 3600 / 5000: loss 2.072238\n",
      "iteration 3700 / 5000: loss 2.057447\n",
      "iteration 3800 / 5000: loss 2.070557\n",
      "iteration 3900 / 5000: loss 2.085866\n",
      "iteration 4000 / 5000: loss 2.022795\n",
      "iteration 4100 / 5000: loss 2.063697\n",
      "iteration 4200 / 5000: loss 2.041395\n",
      "iteration 4300 / 5000: loss 2.117560\n",
      "iteration 4400 / 5000: loss 2.062632\n",
      "iteration 4500 / 5000: loss 2.083407\n",
      "iteration 4600 / 5000: loss 2.010979\n",
      "iteration 4700 / 5000: loss 2.062211\n",
      "iteration 4800 / 5000: loss 2.025935\n",
      "iteration 4900 / 5000: loss 2.112656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 9/50 [04:59<22:46, 33.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 597.771983\n",
      "iteration 100 / 5000: loss 9.219007\n",
      "iteration 200 / 5000: loss 2.203315\n",
      "iteration 300 / 5000: loss 2.021861\n",
      "iteration 400 / 5000: loss 2.084029\n",
      "iteration 500 / 5000: loss 2.086776\n",
      "iteration 600 / 5000: loss 2.015648\n",
      "iteration 700 / 5000: loss 2.058647\n",
      "iteration 800 / 5000: loss 2.109667\n",
      "iteration 900 / 5000: loss 2.040024\n",
      "iteration 1000 / 5000: loss 2.035009\n",
      "iteration 1100 / 5000: loss 2.093676\n",
      "iteration 1200 / 5000: loss 2.013373\n",
      "iteration 1300 / 5000: loss 2.069042\n",
      "iteration 1400 / 5000: loss 2.046324\n",
      "iteration 1500 / 5000: loss 2.052126\n",
      "iteration 1600 / 5000: loss 2.117181\n",
      "iteration 1700 / 5000: loss 2.065755\n",
      "iteration 1800 / 5000: loss 2.035433\n",
      "iteration 1900 / 5000: loss 2.086988\n",
      "iteration 2000 / 5000: loss 2.059760\n",
      "iteration 2100 / 5000: loss 2.069357\n",
      "iteration 2200 / 5000: loss 2.143317\n",
      "iteration 2300 / 5000: loss 2.036089\n",
      "iteration 2400 / 5000: loss 2.161500\n",
      "iteration 2500 / 5000: loss 2.091972\n",
      "iteration 2600 / 5000: loss 2.052018\n",
      "iteration 2700 / 5000: loss 2.046967\n",
      "iteration 2800 / 5000: loss 2.109063\n",
      "iteration 2900 / 5000: loss 2.018596\n",
      "iteration 3000 / 5000: loss 2.064522\n",
      "iteration 3100 / 5000: loss 2.029565\n",
      "iteration 3200 / 5000: loss 2.092880\n",
      "iteration 3300 / 5000: loss 2.026156\n",
      "iteration 3400 / 5000: loss 2.006983\n",
      "iteration 3500 / 5000: loss 2.130699\n",
      "iteration 3600 / 5000: loss 2.115376\n",
      "iteration 3700 / 5000: loss 2.045472\n",
      "iteration 3800 / 5000: loss 2.035625\n",
      "iteration 3900 / 5000: loss 2.144266\n",
      "iteration 4000 / 5000: loss 2.055681\n",
      "iteration 4100 / 5000: loss 2.094273\n",
      "iteration 4200 / 5000: loss 2.080937\n",
      "iteration 4300 / 5000: loss 2.059677\n",
      "iteration 4400 / 5000: loss 2.129065\n",
      "iteration 4500 / 5000: loss 2.025858\n",
      "iteration 4600 / 5000: loss 2.135890\n",
      "iteration 4700 / 5000: loss 2.106377\n",
      "iteration 4800 / 5000: loss 2.108892\n",
      "iteration 4900 / 5000: loss 2.028812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 10/50 [05:34<22:16, 33.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 776.308900\n",
      "iteration 100 / 5000: loss 70.712952\n",
      "iteration 200 / 5000: loss 8.278018\n",
      "iteration 300 / 5000: loss 2.663842\n",
      "iteration 400 / 5000: loss 2.119147\n",
      "iteration 500 / 5000: loss 2.071185\n",
      "iteration 600 / 5000: loss 2.101263\n",
      "iteration 700 / 5000: loss 2.038321\n",
      "iteration 800 / 5000: loss 2.093881\n",
      "iteration 900 / 5000: loss 2.040557\n",
      "iteration 1000 / 5000: loss 2.114752\n",
      "iteration 1100 / 5000: loss 2.112796\n",
      "iteration 1200 / 5000: loss 2.035729\n",
      "iteration 1300 / 5000: loss 2.114651\n",
      "iteration 1400 / 5000: loss 2.082441\n",
      "iteration 1500 / 5000: loss 2.109737\n",
      "iteration 1600 / 5000: loss 2.042839\n",
      "iteration 1700 / 5000: loss 2.070948\n",
      "iteration 1800 / 5000: loss 2.040741\n",
      "iteration 1900 / 5000: loss 2.051760\n",
      "iteration 2000 / 5000: loss 2.046720\n",
      "iteration 2100 / 5000: loss 2.072192\n",
      "iteration 2200 / 5000: loss 2.117224\n",
      "iteration 2300 / 5000: loss 2.119774\n",
      "iteration 2400 / 5000: loss 2.110385\n",
      "iteration 2500 / 5000: loss 2.044842\n",
      "iteration 2600 / 5000: loss 2.018171\n",
      "iteration 2700 / 5000: loss 2.067994\n",
      "iteration 2800 / 5000: loss 2.091099\n",
      "iteration 2900 / 5000: loss 2.066732\n",
      "iteration 3000 / 5000: loss 2.095551\n",
      "iteration 3100 / 5000: loss 2.090869\n",
      "iteration 3200 / 5000: loss 2.148358\n",
      "iteration 3300 / 5000: loss 2.060884\n",
      "iteration 3400 / 5000: loss 2.075574\n",
      "iteration 3500 / 5000: loss 2.085052\n",
      "iteration 3600 / 5000: loss 2.048955\n",
      "iteration 3700 / 5000: loss 2.085737\n",
      "iteration 3800 / 5000: loss 2.011112\n",
      "iteration 3900 / 5000: loss 2.056186\n",
      "iteration 4000 / 5000: loss 2.099015\n",
      "iteration 4100 / 5000: loss 2.064262\n",
      "iteration 4200 / 5000: loss 2.101785\n",
      "iteration 4300 / 5000: loss 2.159531\n",
      "iteration 4400 / 5000: loss 2.074642\n",
      "iteration 4500 / 5000: loss 2.043027\n",
      "iteration 4600 / 5000: loss 2.025248\n",
      "iteration 4700 / 5000: loss 2.075218\n",
      "iteration 4800 / 5000: loss 2.094291\n",
      "iteration 4900 / 5000: loss 2.118545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 11/50 [06:09<21:49, 33.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 628.878049\n",
      "iteration 100 / 5000: loss 246.609929\n",
      "iteration 200 / 5000: loss 97.508494\n",
      "iteration 300 / 5000: loss 39.571021\n",
      "iteration 400 / 5000: loss 16.636844\n",
      "iteration 500 / 5000: loss 7.798117\n",
      "iteration 600 / 5000: loss 4.332154\n",
      "iteration 700 / 5000: loss 2.906807\n",
      "iteration 800 / 5000: loss 2.429694\n",
      "iteration 900 / 5000: loss 2.250907\n",
      "iteration 1000 / 5000: loss 2.139984\n",
      "iteration 1100 / 5000: loss 2.118374\n",
      "iteration 1200 / 5000: loss 2.125409\n",
      "iteration 1300 / 5000: loss 2.064804\n",
      "iteration 1400 / 5000: loss 2.016134\n",
      "iteration 1500 / 5000: loss 2.053786\n",
      "iteration 1600 / 5000: loss 1.981966\n",
      "iteration 1700 / 5000: loss 2.038869\n",
      "iteration 1800 / 5000: loss 2.098067\n",
      "iteration 1900 / 5000: loss 2.064171\n",
      "iteration 2000 / 5000: loss 2.018825\n",
      "iteration 2100 / 5000: loss 1.994011\n",
      "iteration 2200 / 5000: loss 2.065234\n",
      "iteration 2300 / 5000: loss 2.052272\n",
      "iteration 2400 / 5000: loss 2.125984\n",
      "iteration 2500 / 5000: loss 2.103001\n",
      "iteration 2600 / 5000: loss 2.114465\n",
      "iteration 2700 / 5000: loss 2.031185\n",
      "iteration 2800 / 5000: loss 2.070506\n",
      "iteration 2900 / 5000: loss 2.079832\n",
      "iteration 3000 / 5000: loss 2.183240\n",
      "iteration 3100 / 5000: loss 2.048939\n",
      "iteration 3200 / 5000: loss 2.106389\n",
      "iteration 3300 / 5000: loss 2.074729\n",
      "iteration 3400 / 5000: loss 2.064496\n",
      "iteration 3500 / 5000: loss 2.089146\n",
      "iteration 3600 / 5000: loss 2.014328\n",
      "iteration 3700 / 5000: loss 2.020998\n",
      "iteration 3800 / 5000: loss 2.038791\n",
      "iteration 3900 / 5000: loss 2.006135\n",
      "iteration 4000 / 5000: loss 2.125431\n",
      "iteration 4100 / 5000: loss 2.038500\n",
      "iteration 4200 / 5000: loss 2.068657\n",
      "iteration 4300 / 5000: loss 2.063553\n",
      "iteration 4400 / 5000: loss 2.104262\n",
      "iteration 4500 / 5000: loss 2.041327\n",
      "iteration 4600 / 5000: loss 2.002596\n",
      "iteration 4700 / 5000: loss 2.107292\n",
      "iteration 4800 / 5000: loss 2.087180\n",
      "iteration 4900 / 5000: loss 2.112826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 12/50 [06:45<21:24, 33.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 611.214819\n",
      "iteration 100 / 5000: loss 246.818789\n",
      "iteration 200 / 5000: loss 100.433136\n",
      "iteration 300 / 5000: loss 41.706379\n",
      "iteration 400 / 5000: loss 17.890989\n",
      "iteration 500 / 5000: loss 8.444670\n",
      "iteration 600 / 5000: loss 4.655013\n",
      "iteration 700 / 5000: loss 3.140203\n",
      "iteration 800 / 5000: loss 2.468196\n",
      "iteration 900 / 5000: loss 2.257656\n",
      "iteration 1000 / 5000: loss 2.148365\n",
      "iteration 1100 / 5000: loss 2.120480\n",
      "iteration 1200 / 5000: loss 2.098670\n",
      "iteration 1300 / 5000: loss 2.040298\n",
      "iteration 1400 / 5000: loss 2.114791\n",
      "iteration 1500 / 5000: loss 2.032614\n",
      "iteration 1600 / 5000: loss 2.021661\n",
      "iteration 1700 / 5000: loss 2.082434\n",
      "iteration 1800 / 5000: loss 2.043504\n",
      "iteration 1900 / 5000: loss 2.042308\n",
      "iteration 2000 / 5000: loss 2.070208\n",
      "iteration 2100 / 5000: loss 2.022227\n",
      "iteration 2200 / 5000: loss 2.046505\n",
      "iteration 2300 / 5000: loss 2.125910\n",
      "iteration 2400 / 5000: loss 2.136293\n",
      "iteration 2500 / 5000: loss 2.093281\n",
      "iteration 2600 / 5000: loss 2.056377\n",
      "iteration 2700 / 5000: loss 2.077973\n",
      "iteration 2800 / 5000: loss 2.052620\n",
      "iteration 2900 / 5000: loss 2.084435\n",
      "iteration 3000 / 5000: loss 2.036292\n",
      "iteration 3100 / 5000: loss 2.043398\n",
      "iteration 3200 / 5000: loss 2.062538\n",
      "iteration 3300 / 5000: loss 2.026925\n",
      "iteration 3400 / 5000: loss 2.074975\n",
      "iteration 3500 / 5000: loss 2.016624\n",
      "iteration 3600 / 5000: loss 2.070590\n",
      "iteration 3700 / 5000: loss 2.046497\n",
      "iteration 3800 / 5000: loss 2.019053\n",
      "iteration 3900 / 5000: loss 2.076401\n",
      "iteration 4000 / 5000: loss 2.022982\n",
      "iteration 4100 / 5000: loss 1.999162\n",
      "iteration 4200 / 5000: loss 2.099799\n",
      "iteration 4300 / 5000: loss 2.107816\n",
      "iteration 4400 / 5000: loss 2.104285\n",
      "iteration 4500 / 5000: loss 2.024007\n",
      "iteration 4600 / 5000: loss 2.108507\n",
      "iteration 4700 / 5000: loss 2.061005\n",
      "iteration 4800 / 5000: loss 2.035576\n",
      "iteration 4900 / 5000: loss 2.071744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 13/50 [07:20<20:54, 33.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 897.263044\n",
      "iteration 100 / 5000: loss 102.801983\n",
      "iteration 200 / 5000: loss 13.446804\n",
      "iteration 300 / 5000: loss 3.319814\n",
      "iteration 400 / 5000: loss 2.222177\n",
      "iteration 500 / 5000: loss 2.052125\n",
      "iteration 600 / 5000: loss 2.078972\n",
      "iteration 700 / 5000: loss 2.062697\n",
      "iteration 800 / 5000: loss 2.066009\n",
      "iteration 900 / 5000: loss 2.104363\n",
      "iteration 1000 / 5000: loss 2.101780\n",
      "iteration 1100 / 5000: loss 2.133828\n",
      "iteration 1200 / 5000: loss 2.105122\n",
      "iteration 1300 / 5000: loss 2.059204\n",
      "iteration 1400 / 5000: loss 2.083710\n",
      "iteration 1500 / 5000: loss 2.047828\n",
      "iteration 1600 / 5000: loss 2.118895\n",
      "iteration 1700 / 5000: loss 2.085447\n",
      "iteration 1800 / 5000: loss 2.130769\n",
      "iteration 1900 / 5000: loss 2.061909\n",
      "iteration 2000 / 5000: loss 2.139024\n",
      "iteration 2100 / 5000: loss 2.122423\n",
      "iteration 2200 / 5000: loss 2.084407\n",
      "iteration 2300 / 5000: loss 2.112154\n",
      "iteration 2400 / 5000: loss 2.111736\n",
      "iteration 2500 / 5000: loss 2.121892\n",
      "iteration 2600 / 5000: loss 2.097860\n",
      "iteration 2700 / 5000: loss 2.048210\n",
      "iteration 2800 / 5000: loss 2.068231\n",
      "iteration 2900 / 5000: loss 2.128416\n",
      "iteration 3000 / 5000: loss 2.103176\n",
      "iteration 3100 / 5000: loss 2.053365\n",
      "iteration 3200 / 5000: loss 2.085982\n",
      "iteration 3300 / 5000: loss 2.073937\n",
      "iteration 3400 / 5000: loss 2.137923\n",
      "iteration 3500 / 5000: loss 2.121450\n",
      "iteration 3600 / 5000: loss 2.128923\n",
      "iteration 3700 / 5000: loss 2.082398\n",
      "iteration 3800 / 5000: loss 2.066436\n",
      "iteration 3900 / 5000: loss 2.047256\n",
      "iteration 4000 / 5000: loss 2.115005\n",
      "iteration 4100 / 5000: loss 2.063606\n",
      "iteration 4200 / 5000: loss 2.065662\n",
      "iteration 4300 / 5000: loss 2.125075\n",
      "iteration 4400 / 5000: loss 2.074005\n",
      "iteration 4500 / 5000: loss 2.094121\n",
      "iteration 4600 / 5000: loss 2.137137\n",
      "iteration 4700 / 5000: loss 2.095798\n",
      "iteration 4800 / 5000: loss 2.128603\n",
      "iteration 4900 / 5000: loss 2.121634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 14/50 [07:56<20:24, 34.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1016.748406\n",
      "iteration 100 / 5000: loss 159.718195\n",
      "iteration 200 / 5000: loss 26.688810\n",
      "iteration 300 / 5000: loss 5.953804\n",
      "iteration 400 / 5000: loss 2.757842\n",
      "iteration 500 / 5000: loss 2.182051\n",
      "iteration 600 / 5000: loss 2.086445\n",
      "iteration 700 / 5000: loss 2.115117\n",
      "iteration 800 / 5000: loss 2.098956\n",
      "iteration 900 / 5000: loss 2.144187\n",
      "iteration 1000 / 5000: loss 2.077173\n",
      "iteration 1100 / 5000: loss 2.129665\n",
      "iteration 1200 / 5000: loss 2.102852\n",
      "iteration 1300 / 5000: loss 2.147753\n",
      "iteration 1400 / 5000: loss 2.140375\n",
      "iteration 1500 / 5000: loss 2.063405\n",
      "iteration 1600 / 5000: loss 2.057020\n",
      "iteration 1700 / 5000: loss 2.133566\n",
      "iteration 1800 / 5000: loss 2.110058\n",
      "iteration 1900 / 5000: loss 2.128390\n",
      "iteration 2000 / 5000: loss 2.113809\n",
      "iteration 2100 / 5000: loss 2.102695\n",
      "iteration 2200 / 5000: loss 2.076760\n",
      "iteration 2300 / 5000: loss 2.152501\n",
      "iteration 2400 / 5000: loss 2.082778\n",
      "iteration 2500 / 5000: loss 2.131761\n",
      "iteration 2600 / 5000: loss 2.119604\n",
      "iteration 2700 / 5000: loss 2.052369\n",
      "iteration 2800 / 5000: loss 2.170759\n",
      "iteration 2900 / 5000: loss 2.076300\n",
      "iteration 3000 / 5000: loss 2.130375\n",
      "iteration 3100 / 5000: loss 2.101202\n",
      "iteration 3200 / 5000: loss 2.117277\n",
      "iteration 3300 / 5000: loss 2.089794\n",
      "iteration 3400 / 5000: loss 2.102938\n",
      "iteration 3500 / 5000: loss 2.135983\n",
      "iteration 3600 / 5000: loss 2.109851\n",
      "iteration 3700 / 5000: loss 2.126075\n",
      "iteration 3800 / 5000: loss 2.113599\n",
      "iteration 3900 / 5000: loss 2.116239\n",
      "iteration 4000 / 5000: loss 2.132631\n",
      "iteration 4100 / 5000: loss 2.112085\n",
      "iteration 4200 / 5000: loss 2.062381\n",
      "iteration 4300 / 5000: loss 2.126863\n",
      "iteration 4400 / 5000: loss 2.107862\n",
      "iteration 4500 / 5000: loss 2.139065\n",
      "iteration 4600 / 5000: loss 2.108156\n",
      "iteration 4700 / 5000: loss 2.116842\n",
      "iteration 4800 / 5000: loss 2.138046\n",
      "iteration 4900 / 5000: loss 2.102676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 15/50 [08:29<19:48, 33.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1541.456471\n",
      "iteration 100 / 5000: loss 3.028370\n",
      "iteration 200 / 5000: loss 2.114370\n",
      "iteration 300 / 5000: loss 2.130399\n",
      "iteration 400 / 5000: loss 2.181542\n",
      "iteration 500 / 5000: loss 2.127915\n",
      "iteration 600 / 5000: loss 2.138920\n",
      "iteration 700 / 5000: loss 2.143470\n",
      "iteration 800 / 5000: loss 2.164006\n",
      "iteration 900 / 5000: loss 2.176408\n",
      "iteration 1000 / 5000: loss 2.115753\n",
      "iteration 1100 / 5000: loss 2.145734\n",
      "iteration 1200 / 5000: loss 2.149839\n",
      "iteration 1300 / 5000: loss 2.139179\n",
      "iteration 1400 / 5000: loss 2.083742\n",
      "iteration 1500 / 5000: loss 2.124598\n",
      "iteration 1600 / 5000: loss 2.103289\n",
      "iteration 1700 / 5000: loss 2.168548\n",
      "iteration 1800 / 5000: loss 2.142369\n",
      "iteration 1900 / 5000: loss 2.115974\n",
      "iteration 2000 / 5000: loss 2.142955\n",
      "iteration 2100 / 5000: loss 2.220529\n",
      "iteration 2200 / 5000: loss 2.158228\n",
      "iteration 2300 / 5000: loss 2.170080\n",
      "iteration 2400 / 5000: loss 2.144334\n",
      "iteration 2500 / 5000: loss 2.164655\n",
      "iteration 2600 / 5000: loss 2.198175\n",
      "iteration 2700 / 5000: loss 2.167445\n",
      "iteration 2800 / 5000: loss 2.098524\n",
      "iteration 2900 / 5000: loss 2.104917\n",
      "iteration 3000 / 5000: loss 2.149884\n",
      "iteration 3100 / 5000: loss 2.149204\n",
      "iteration 3200 / 5000: loss 2.146238\n",
      "iteration 3300 / 5000: loss 2.131891\n",
      "iteration 3400 / 5000: loss 2.155181\n",
      "iteration 3500 / 5000: loss 2.086876\n",
      "iteration 3600 / 5000: loss 2.147868\n",
      "iteration 3700 / 5000: loss 2.143105\n",
      "iteration 3800 / 5000: loss 2.146705\n",
      "iteration 3900 / 5000: loss 2.148747\n",
      "iteration 4000 / 5000: loss 2.118748\n",
      "iteration 4100 / 5000: loss 2.131149\n",
      "iteration 4200 / 5000: loss 2.141534\n",
      "iteration 4300 / 5000: loss 2.151546\n",
      "iteration 4400 / 5000: loss 2.130276\n",
      "iteration 4500 / 5000: loss 2.141566\n",
      "iteration 4600 / 5000: loss 2.168630\n",
      "iteration 4700 / 5000: loss 2.105154\n",
      "iteration 4800 / 5000: loss 2.142862\n",
      "iteration 4900 / 5000: loss 2.125196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|███▏      | 16/50 [09:01<19:09, 33.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1181.735294\n",
      "iteration 100 / 5000: loss 63.568314\n",
      "iteration 200 / 5000: loss 5.339268\n",
      "iteration 300 / 5000: loss 2.249518\n",
      "iteration 400 / 5000: loss 2.149523\n",
      "iteration 500 / 5000: loss 2.153682\n",
      "iteration 600 / 5000: loss 2.095926\n",
      "iteration 700 / 5000: loss 2.128302\n",
      "iteration 800 / 5000: loss 2.135048\n",
      "iteration 900 / 5000: loss 2.096005\n",
      "iteration 1000 / 5000: loss 2.097670\n",
      "iteration 1100 / 5000: loss 2.199379\n",
      "iteration 1200 / 5000: loss 2.184747\n",
      "iteration 1300 / 5000: loss 2.168222\n",
      "iteration 1400 / 5000: loss 2.098018\n",
      "iteration 1500 / 5000: loss 2.099745\n",
      "iteration 1600 / 5000: loss 2.133365\n",
      "iteration 1700 / 5000: loss 2.120094\n",
      "iteration 1800 / 5000: loss 2.095655\n",
      "iteration 1900 / 5000: loss 2.101326\n",
      "iteration 2000 / 5000: loss 2.101671\n",
      "iteration 2100 / 5000: loss 2.128816\n",
      "iteration 2200 / 5000: loss 2.096544\n",
      "iteration 2300 / 5000: loss 2.171596\n",
      "iteration 2400 / 5000: loss 2.100368\n",
      "iteration 2500 / 5000: loss 2.088485\n",
      "iteration 2600 / 5000: loss 2.085528\n",
      "iteration 2700 / 5000: loss 2.124421\n",
      "iteration 2800 / 5000: loss 2.124783\n",
      "iteration 2900 / 5000: loss 2.086826\n",
      "iteration 3000 / 5000: loss 2.122264\n",
      "iteration 3100 / 5000: loss 2.196706\n",
      "iteration 3200 / 5000: loss 2.144555\n",
      "iteration 3300 / 5000: loss 2.148093\n",
      "iteration 3400 / 5000: loss 2.150825\n",
      "iteration 3500 / 5000: loss 2.081865\n",
      "iteration 3600 / 5000: loss 2.173865\n",
      "iteration 3700 / 5000: loss 2.115982\n",
      "iteration 3800 / 5000: loss 2.166596\n",
      "iteration 3900 / 5000: loss 2.152286\n",
      "iteration 4000 / 5000: loss 2.083538\n",
      "iteration 4100 / 5000: loss 2.088475\n",
      "iteration 4200 / 5000: loss 2.163069\n",
      "iteration 4300 / 5000: loss 2.065453\n",
      "iteration 4400 / 5000: loss 2.153415\n",
      "iteration 4500 / 5000: loss 2.132730\n",
      "iteration 4600 / 5000: loss 2.091811\n",
      "iteration 4700 / 5000: loss 2.109940\n",
      "iteration 4800 / 5000: loss 2.134712\n",
      "iteration 4900 / 5000: loss 2.129236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 17/50 [09:33<18:33, 33.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 717.383443\n",
      "iteration 100 / 5000: loss 211.485772\n",
      "iteration 200 / 5000: loss 63.460899\n",
      "iteration 300 / 5000: loss 20.138083\n",
      "iteration 400 / 5000: loss 7.344468\n",
      "iteration 500 / 5000: loss 3.662695\n",
      "iteration 600 / 5000: loss 2.518640\n",
      "iteration 700 / 5000: loss 2.209111\n",
      "iteration 800 / 5000: loss 2.163977\n",
      "iteration 900 / 5000: loss 2.111022\n",
      "iteration 1000 / 5000: loss 2.118877\n",
      "iteration 1100 / 5000: loss 2.121944\n",
      "iteration 1200 / 5000: loss 2.141336\n",
      "iteration 1300 / 5000: loss 2.024594\n",
      "iteration 1400 / 5000: loss 2.033275\n",
      "iteration 1500 / 5000: loss 2.126450\n",
      "iteration 1600 / 5000: loss 2.057799\n",
      "iteration 1700 / 5000: loss 2.087858\n",
      "iteration 1800 / 5000: loss 2.092429\n",
      "iteration 1900 / 5000: loss 2.131730\n",
      "iteration 2000 / 5000: loss 2.045026\n",
      "iteration 2100 / 5000: loss 2.087352\n",
      "iteration 2200 / 5000: loss 2.093551\n",
      "iteration 2300 / 5000: loss 2.096730\n",
      "iteration 2400 / 5000: loss 2.130675\n",
      "iteration 2500 / 5000: loss 2.082983\n",
      "iteration 2600 / 5000: loss 2.061521\n",
      "iteration 2700 / 5000: loss 2.102596\n",
      "iteration 2800 / 5000: loss 2.072153\n",
      "iteration 2900 / 5000: loss 2.032582\n",
      "iteration 3000 / 5000: loss 2.126157\n",
      "iteration 3100 / 5000: loss 2.044853\n",
      "iteration 3200 / 5000: loss 2.163958\n",
      "iteration 3300 / 5000: loss 2.068055\n",
      "iteration 3400 / 5000: loss 2.100020\n",
      "iteration 3500 / 5000: loss 2.038296\n",
      "iteration 3600 / 5000: loss 2.075391\n",
      "iteration 3700 / 5000: loss 2.052489\n",
      "iteration 3800 / 5000: loss 2.107228\n",
      "iteration 3900 / 5000: loss 2.110027\n",
      "iteration 4000 / 5000: loss 2.066750\n",
      "iteration 4100 / 5000: loss 2.061300\n",
      "iteration 4200 / 5000: loss 2.134905\n",
      "iteration 4300 / 5000: loss 2.070970\n",
      "iteration 4400 / 5000: loss 2.059848\n",
      "iteration 4500 / 5000: loss 2.070876\n",
      "iteration 4600 / 5000: loss 2.076379\n",
      "iteration 4700 / 5000: loss 2.096990\n",
      "iteration 4800 / 5000: loss 2.100030\n",
      "iteration 4900 / 5000: loss 2.067289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 18/50 [10:05<17:56, 33.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 876.338950\n",
      "iteration 100 / 5000: loss 52.739486\n",
      "iteration 200 / 5000: loss 5.040007\n",
      "iteration 300 / 5000: loss 2.304399\n",
      "iteration 400 / 5000: loss 2.078091\n",
      "iteration 500 / 5000: loss 2.104631\n",
      "iteration 600 / 5000: loss 2.054968\n",
      "iteration 700 / 5000: loss 2.086355\n",
      "iteration 800 / 5000: loss 2.036738\n",
      "iteration 900 / 5000: loss 2.100032\n",
      "iteration 1000 / 5000: loss 2.160720\n",
      "iteration 1100 / 5000: loss 2.162353\n",
      "iteration 1200 / 5000: loss 2.112524\n",
      "iteration 1300 / 5000: loss 2.107935\n",
      "iteration 1400 / 5000: loss 2.094391\n",
      "iteration 1500 / 5000: loss 2.096405\n",
      "iteration 1600 / 5000: loss 2.058158\n",
      "iteration 1700 / 5000: loss 2.132862\n",
      "iteration 1800 / 5000: loss 2.053786\n",
      "iteration 1900 / 5000: loss 2.123924\n",
      "iteration 2000 / 5000: loss 2.143183\n",
      "iteration 2100 / 5000: loss 2.058362\n",
      "iteration 2200 / 5000: loss 2.035741\n",
      "iteration 2300 / 5000: loss 2.069736\n",
      "iteration 2400 / 5000: loss 2.055748\n",
      "iteration 2500 / 5000: loss 2.102892\n",
      "iteration 2600 / 5000: loss 2.092591\n",
      "iteration 2700 / 5000: loss 2.067135\n",
      "iteration 2800 / 5000: loss 2.083264\n",
      "iteration 2900 / 5000: loss 2.125183\n",
      "iteration 3000 / 5000: loss 2.157830\n",
      "iteration 3100 / 5000: loss 2.093252\n",
      "iteration 3200 / 5000: loss 2.073696\n",
      "iteration 3300 / 5000: loss 2.105728\n",
      "iteration 3400 / 5000: loss 2.054300\n",
      "iteration 3500 / 5000: loss 2.063541\n",
      "iteration 3600 / 5000: loss 2.080075\n",
      "iteration 3700 / 5000: loss 2.038435\n",
      "iteration 3800 / 5000: loss 2.086489\n",
      "iteration 3900 / 5000: loss 2.118422\n",
      "iteration 4000 / 5000: loss 2.089649\n",
      "iteration 4100 / 5000: loss 2.087742\n",
      "iteration 4200 / 5000: loss 2.120239\n",
      "iteration 4300 / 5000: loss 2.064603\n",
      "iteration 4400 / 5000: loss 2.123487\n",
      "iteration 4500 / 5000: loss 2.098251\n",
      "iteration 4600 / 5000: loss 2.112697\n",
      "iteration 4700 / 5000: loss 2.095860\n",
      "iteration 4800 / 5000: loss 2.047431\n",
      "iteration 4900 / 5000: loss 2.072923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 38%|███▊      | 19/50 [10:42<17:27, 33.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 657.721107\n",
      "iteration 100 / 5000: loss 19.979581\n",
      "iteration 200 / 5000: loss 2.540453\n",
      "iteration 300 / 5000: loss 2.060325\n",
      "iteration 400 / 5000: loss 2.059901\n",
      "iteration 500 / 5000: loss 2.102609\n",
      "iteration 600 / 5000: loss 2.105195\n",
      "iteration 700 / 5000: loss 2.100022\n",
      "iteration 800 / 5000: loss 2.072578\n",
      "iteration 900 / 5000: loss 2.058272\n",
      "iteration 1000 / 5000: loss 2.055194\n",
      "iteration 1100 / 5000: loss 2.105394\n",
      "iteration 1200 / 5000: loss 2.085281\n",
      "iteration 1300 / 5000: loss 2.114060\n",
      "iteration 1400 / 5000: loss 2.076420\n",
      "iteration 1500 / 5000: loss 2.106486\n",
      "iteration 1600 / 5000: loss 2.072990\n",
      "iteration 1700 / 5000: loss 2.175947\n",
      "iteration 1800 / 5000: loss 2.107607\n",
      "iteration 1900 / 5000: loss 2.076820\n",
      "iteration 2000 / 5000: loss 2.146927\n",
      "iteration 2100 / 5000: loss 2.115315\n",
      "iteration 2200 / 5000: loss 2.102560\n",
      "iteration 2300 / 5000: loss 2.026077\n",
      "iteration 2400 / 5000: loss 2.083491\n",
      "iteration 2500 / 5000: loss 2.067131\n",
      "iteration 2600 / 5000: loss 2.096717\n",
      "iteration 2700 / 5000: loss 2.102991\n",
      "iteration 2800 / 5000: loss 2.075938\n",
      "iteration 2900 / 5000: loss 2.066734\n",
      "iteration 3000 / 5000: loss 2.070195\n",
      "iteration 3100 / 5000: loss 2.119049\n",
      "iteration 3200 / 5000: loss 2.073117\n",
      "iteration 3300 / 5000: loss 2.096713\n",
      "iteration 3400 / 5000: loss 2.106078\n",
      "iteration 3500 / 5000: loss 2.032786\n",
      "iteration 3600 / 5000: loss 2.014862\n",
      "iteration 3700 / 5000: loss 2.020646\n",
      "iteration 3800 / 5000: loss 2.082675\n",
      "iteration 3900 / 5000: loss 2.039910\n",
      "iteration 4000 / 5000: loss 2.097244\n",
      "iteration 4100 / 5000: loss 2.037429\n",
      "iteration 4200 / 5000: loss 2.084023\n",
      "iteration 4300 / 5000: loss 2.045557\n",
      "iteration 4400 / 5000: loss 2.121627\n",
      "iteration 4500 / 5000: loss 2.096771\n",
      "iteration 4600 / 5000: loss 2.065027\n",
      "iteration 4700 / 5000: loss 2.159141\n",
      "iteration 4800 / 5000: loss 2.106880\n",
      "iteration 4900 / 5000: loss 2.058830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 20/50 [11:13<16:50, 33.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1088.680257\n",
      "iteration 100 / 5000: loss 28.787431\n",
      "iteration 200 / 5000: loss 2.800442\n",
      "iteration 300 / 5000: loss 2.130218\n",
      "iteration 400 / 5000: loss 2.138721\n",
      "iteration 500 / 5000: loss 2.131341\n",
      "iteration 600 / 5000: loss 2.079238\n",
      "iteration 700 / 5000: loss 2.161587\n",
      "iteration 800 / 5000: loss 2.121084\n",
      "iteration 900 / 5000: loss 2.093572\n",
      "iteration 1000 / 5000: loss 2.115151\n",
      "iteration 1100 / 5000: loss 2.098554\n",
      "iteration 1200 / 5000: loss 2.113904\n",
      "iteration 1300 / 5000: loss 2.065333\n",
      "iteration 1400 / 5000: loss 2.069262\n",
      "iteration 1500 / 5000: loss 2.136550\n",
      "iteration 1600 / 5000: loss 2.119153\n",
      "iteration 1700 / 5000: loss 2.137927\n",
      "iteration 1800 / 5000: loss 2.074369\n",
      "iteration 1900 / 5000: loss 2.142429\n",
      "iteration 2000 / 5000: loss 2.108314\n",
      "iteration 2100 / 5000: loss 2.112104\n",
      "iteration 2200 / 5000: loss 2.134132\n",
      "iteration 2300 / 5000: loss 2.109897\n",
      "iteration 2400 / 5000: loss 2.128292\n",
      "iteration 2500 / 5000: loss 2.126252\n",
      "iteration 2600 / 5000: loss 2.081120\n",
      "iteration 2700 / 5000: loss 2.095494\n",
      "iteration 2800 / 5000: loss 2.096794\n",
      "iteration 2900 / 5000: loss 2.084515\n",
      "iteration 3000 / 5000: loss 2.040093\n",
      "iteration 3100 / 5000: loss 2.118452\n",
      "iteration 3200 / 5000: loss 2.141220\n",
      "iteration 3300 / 5000: loss 2.154725\n",
      "iteration 3400 / 5000: loss 2.127691\n",
      "iteration 3500 / 5000: loss 2.123261\n",
      "iteration 3600 / 5000: loss 2.154456\n",
      "iteration 3700 / 5000: loss 2.092506\n",
      "iteration 3800 / 5000: loss 2.106563\n",
      "iteration 3900 / 5000: loss 2.151904\n",
      "iteration 4000 / 5000: loss 2.150186\n",
      "iteration 4100 / 5000: loss 2.106432\n",
      "iteration 4200 / 5000: loss 2.076605\n",
      "iteration 4300 / 5000: loss 2.196418\n",
      "iteration 4400 / 5000: loss 2.126188\n",
      "iteration 4500 / 5000: loss 2.149659\n",
      "iteration 4600 / 5000: loss 2.090199\n",
      "iteration 4700 / 5000: loss 2.125622\n",
      "iteration 4800 / 5000: loss 2.144301\n",
      "iteration 4900 / 5000: loss 2.123409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 21/50 [11:45<16:14, 33.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1460.017617\n",
      "iteration 100 / 5000: loss 2.169170\n",
      "iteration 200 / 5000: loss 2.164223\n",
      "iteration 300 / 5000: loss 2.150606\n",
      "iteration 400 / 5000: loss 2.170887\n",
      "iteration 500 / 5000: loss 2.106009\n",
      "iteration 600 / 5000: loss 2.153817\n",
      "iteration 700 / 5000: loss 2.218116\n",
      "iteration 800 / 5000: loss 2.127439\n",
      "iteration 900 / 5000: loss 2.095973\n",
      "iteration 1000 / 5000: loss 2.147303\n",
      "iteration 1100 / 5000: loss 2.086799\n",
      "iteration 1200 / 5000: loss 2.119157\n",
      "iteration 1300 / 5000: loss 2.151298\n",
      "iteration 1400 / 5000: loss 2.155259\n",
      "iteration 1500 / 5000: loss 2.172109\n",
      "iteration 1600 / 5000: loss 2.131522\n",
      "iteration 1700 / 5000: loss 2.121223\n",
      "iteration 1800 / 5000: loss 2.168820\n",
      "iteration 1900 / 5000: loss 2.118720\n",
      "iteration 2000 / 5000: loss 2.202616\n",
      "iteration 2100 / 5000: loss 2.152517\n",
      "iteration 2200 / 5000: loss 2.136055\n",
      "iteration 2300 / 5000: loss 2.152028\n",
      "iteration 2400 / 5000: loss 2.143983\n",
      "iteration 2500 / 5000: loss 2.107680\n",
      "iteration 2600 / 5000: loss 2.143436\n",
      "iteration 2700 / 5000: loss 2.139467\n",
      "iteration 2800 / 5000: loss 2.182062\n",
      "iteration 2900 / 5000: loss 2.113201\n",
      "iteration 3000 / 5000: loss 2.157159\n",
      "iteration 3100 / 5000: loss 2.146150\n",
      "iteration 3200 / 5000: loss 2.187629\n",
      "iteration 3300 / 5000: loss 2.127901\n",
      "iteration 3400 / 5000: loss 2.145400\n",
      "iteration 3500 / 5000: loss 2.074996\n",
      "iteration 3600 / 5000: loss 2.151558\n",
      "iteration 3700 / 5000: loss 2.110182\n",
      "iteration 3800 / 5000: loss 2.138104\n",
      "iteration 3900 / 5000: loss 2.161564\n",
      "iteration 4000 / 5000: loss 2.160147\n",
      "iteration 4100 / 5000: loss 2.142367\n",
      "iteration 4200 / 5000: loss 2.132004\n",
      "iteration 4300 / 5000: loss 2.186691\n",
      "iteration 4400 / 5000: loss 2.097259\n",
      "iteration 4500 / 5000: loss 2.152220\n",
      "iteration 4600 / 5000: loss 2.132089\n",
      "iteration 4700 / 5000: loss 2.187865\n",
      "iteration 4800 / 5000: loss 2.157758\n",
      "iteration 4900 / 5000: loss 2.120470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 44%|████▍     | 22/50 [12:17<15:38, 33.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1063.996814\n",
      "iteration 100 / 5000: loss 26.539566\n",
      "iteration 200 / 5000: loss 2.688047\n",
      "iteration 300 / 5000: loss 2.118872\n",
      "iteration 400 / 5000: loss 2.134956\n",
      "iteration 500 / 5000: loss 2.155221\n",
      "iteration 600 / 5000: loss 2.134023\n",
      "iteration 700 / 5000: loss 2.095019\n",
      "iteration 800 / 5000: loss 2.089895\n",
      "iteration 900 / 5000: loss 2.111240\n",
      "iteration 1000 / 5000: loss 2.118660\n",
      "iteration 1100 / 5000: loss 2.115416\n",
      "iteration 1200 / 5000: loss 2.135765\n",
      "iteration 1300 / 5000: loss 2.093349\n",
      "iteration 1400 / 5000: loss 2.090500\n",
      "iteration 1500 / 5000: loss 2.141986\n",
      "iteration 1600 / 5000: loss 2.127849\n",
      "iteration 1700 / 5000: loss 2.108829\n",
      "iteration 1800 / 5000: loss 2.103807\n",
      "iteration 1900 / 5000: loss 2.167316\n",
      "iteration 2000 / 5000: loss 2.062995\n",
      "iteration 2100 / 5000: loss 2.080248\n",
      "iteration 2200 / 5000: loss 2.106688\n",
      "iteration 2300 / 5000: loss 2.090699\n",
      "iteration 2400 / 5000: loss 2.085307\n",
      "iteration 2500 / 5000: loss 2.174543\n",
      "iteration 2600 / 5000: loss 2.088340\n",
      "iteration 2700 / 5000: loss 2.121975\n",
      "iteration 2800 / 5000: loss 2.052434\n",
      "iteration 2900 / 5000: loss 2.130862\n",
      "iteration 3000 / 5000: loss 2.126987\n",
      "iteration 3100 / 5000: loss 2.100528\n",
      "iteration 3200 / 5000: loss 2.092952\n",
      "iteration 3300 / 5000: loss 2.112907\n",
      "iteration 3400 / 5000: loss 2.136456\n",
      "iteration 3500 / 5000: loss 2.062758\n",
      "iteration 3600 / 5000: loss 2.127201\n",
      "iteration 3700 / 5000: loss 2.135315\n",
      "iteration 3800 / 5000: loss 2.061337\n",
      "iteration 3900 / 5000: loss 2.121688\n",
      "iteration 4000 / 5000: loss 2.137257\n",
      "iteration 4100 / 5000: loss 2.082434\n",
      "iteration 4200 / 5000: loss 2.125682\n",
      "iteration 4300 / 5000: loss 2.146157\n",
      "iteration 4400 / 5000: loss 2.092307\n",
      "iteration 4500 / 5000: loss 2.127113\n",
      "iteration 4600 / 5000: loss 2.118395\n",
      "iteration 4700 / 5000: loss 2.131425\n",
      "iteration 4800 / 5000: loss 2.185593\n",
      "iteration 4900 / 5000: loss 2.098850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▌     | 23/50 [12:49<15:03, 33.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1242.648792\n",
      "iteration 100 / 5000: loss 2.112374\n",
      "iteration 200 / 5000: loss 2.166673\n",
      "iteration 300 / 5000: loss 2.163484\n",
      "iteration 400 / 5000: loss 2.106262\n",
      "iteration 500 / 5000: loss 2.142121\n",
      "iteration 600 / 5000: loss 2.132318\n",
      "iteration 700 / 5000: loss 2.162602\n",
      "iteration 800 / 5000: loss 2.146994\n",
      "iteration 900 / 5000: loss 2.090195\n",
      "iteration 1000 / 5000: loss 2.114862\n",
      "iteration 1100 / 5000: loss 2.131351\n",
      "iteration 1200 / 5000: loss 2.145909\n",
      "iteration 1300 / 5000: loss 2.183133\n",
      "iteration 1400 / 5000: loss 2.109568\n",
      "iteration 1500 / 5000: loss 2.113964\n",
      "iteration 1600 / 5000: loss 2.151853\n",
      "iteration 1700 / 5000: loss 2.121911\n",
      "iteration 1800 / 5000: loss 2.133303\n",
      "iteration 1900 / 5000: loss 2.214848\n",
      "iteration 2000 / 5000: loss 2.131125\n",
      "iteration 2100 / 5000: loss 2.153330\n",
      "iteration 2200 / 5000: loss 2.145175\n",
      "iteration 2300 / 5000: loss 2.113397\n",
      "iteration 2400 / 5000: loss 2.116893\n",
      "iteration 2500 / 5000: loss 2.167704\n",
      "iteration 2600 / 5000: loss 2.121781\n",
      "iteration 2700 / 5000: loss 2.106462\n",
      "iteration 2800 / 5000: loss 2.121704\n",
      "iteration 2900 / 5000: loss 2.146943\n",
      "iteration 3000 / 5000: loss 2.119310\n",
      "iteration 3100 / 5000: loss 2.166290\n",
      "iteration 3200 / 5000: loss 2.099138\n",
      "iteration 3300 / 5000: loss 2.140174\n",
      "iteration 3400 / 5000: loss 2.133462\n",
      "iteration 3500 / 5000: loss 2.139718\n",
      "iteration 3600 / 5000: loss 2.142802\n",
      "iteration 3700 / 5000: loss 2.108330\n",
      "iteration 3800 / 5000: loss 2.085715\n",
      "iteration 3900 / 5000: loss 2.151288\n",
      "iteration 4000 / 5000: loss 2.093448\n",
      "iteration 4100 / 5000: loss 2.144019\n",
      "iteration 4200 / 5000: loss 2.114320\n",
      "iteration 4300 / 5000: loss 2.095205\n",
      "iteration 4400 / 5000: loss 2.110047\n",
      "iteration 4500 / 5000: loss 2.136882\n",
      "iteration 4600 / 5000: loss 2.151658\n",
      "iteration 4700 / 5000: loss 2.127220\n",
      "iteration 4800 / 5000: loss 2.124681\n",
      "iteration 4900 / 5000: loss 2.115066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 24/50 [13:24<14:31, 33.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 551.067667\n",
      "iteration 100 / 5000: loss 218.300554\n",
      "iteration 200 / 5000: loss 87.383894\n",
      "iteration 300 / 5000: loss 35.801545\n",
      "iteration 400 / 5000: loss 15.449076\n",
      "iteration 500 / 5000: loss 7.346006\n",
      "iteration 600 / 5000: loss 4.212063\n",
      "iteration 700 / 5000: loss 2.843136\n",
      "iteration 800 / 5000: loss 2.361301\n",
      "iteration 900 / 5000: loss 2.258998\n",
      "iteration 1000 / 5000: loss 2.094868\n",
      "iteration 1100 / 5000: loss 2.023618\n",
      "iteration 1200 / 5000: loss 2.005529\n",
      "iteration 1300 / 5000: loss 2.015515\n",
      "iteration 1400 / 5000: loss 2.058425\n",
      "iteration 1500 / 5000: loss 2.067010\n",
      "iteration 1600 / 5000: loss 2.056608\n",
      "iteration 1700 / 5000: loss 2.099716\n",
      "iteration 1800 / 5000: loss 2.019606\n",
      "iteration 1900 / 5000: loss 2.034469\n",
      "iteration 2000 / 5000: loss 2.059421\n",
      "iteration 2100 / 5000: loss 2.053396\n",
      "iteration 2200 / 5000: loss 2.021813\n",
      "iteration 2300 / 5000: loss 2.082617\n",
      "iteration 2400 / 5000: loss 2.017663\n",
      "iteration 2500 / 5000: loss 2.040133\n",
      "iteration 2600 / 5000: loss 1.981205\n",
      "iteration 2700 / 5000: loss 2.055723\n",
      "iteration 2800 / 5000: loss 2.078100\n",
      "iteration 2900 / 5000: loss 1.997136\n",
      "iteration 3000 / 5000: loss 2.090230\n",
      "iteration 3100 / 5000: loss 2.045827\n",
      "iteration 3200 / 5000: loss 2.101154\n",
      "iteration 3300 / 5000: loss 2.065825\n",
      "iteration 3400 / 5000: loss 2.045482\n",
      "iteration 3500 / 5000: loss 2.042555\n",
      "iteration 3600 / 5000: loss 2.061419\n",
      "iteration 3700 / 5000: loss 2.033863\n",
      "iteration 3800 / 5000: loss 2.045730\n",
      "iteration 3900 / 5000: loss 2.003377\n",
      "iteration 4000 / 5000: loss 2.028127\n",
      "iteration 4100 / 5000: loss 2.073463\n",
      "iteration 4200 / 5000: loss 2.003567\n",
      "iteration 4300 / 5000: loss 1.996169\n",
      "iteration 4400 / 5000: loss 2.014535\n",
      "iteration 4500 / 5000: loss 2.031615\n",
      "iteration 4600 / 5000: loss 2.092467\n",
      "iteration 4700 / 5000: loss 2.014695\n",
      "iteration 4800 / 5000: loss 2.098403\n",
      "iteration 4900 / 5000: loss 2.038356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 25/50 [14:04<14:04, 33.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1180.339271\n",
      "iteration 100 / 5000: loss 110.998572\n",
      "iteration 200 / 5000: loss 12.170395\n",
      "iteration 300 / 5000: loss 3.080612\n",
      "iteration 400 / 5000: loss 2.205700\n",
      "iteration 500 / 5000: loss 2.115443\n",
      "iteration 600 / 5000: loss 2.143248\n",
      "iteration 700 / 5000: loss 2.143847\n",
      "iteration 800 / 5000: loss 2.128916\n",
      "iteration 900 / 5000: loss 2.143656\n",
      "iteration 1000 / 5000: loss 2.167618\n",
      "iteration 1100 / 5000: loss 2.142532\n",
      "iteration 1200 / 5000: loss 2.088732\n",
      "iteration 1300 / 5000: loss 2.091327\n",
      "iteration 1400 / 5000: loss 2.090002\n",
      "iteration 1500 / 5000: loss 2.102008\n",
      "iteration 1600 / 5000: loss 2.097405\n",
      "iteration 1700 / 5000: loss 2.089395\n",
      "iteration 1800 / 5000: loss 2.088252\n",
      "iteration 1900 / 5000: loss 2.156070\n",
      "iteration 2000 / 5000: loss 2.111044\n",
      "iteration 2100 / 5000: loss 2.114898\n",
      "iteration 2200 / 5000: loss 2.174530\n",
      "iteration 2300 / 5000: loss 2.119542\n",
      "iteration 2400 / 5000: loss 2.116407\n",
      "iteration 2500 / 5000: loss 2.092897\n",
      "iteration 2600 / 5000: loss 2.157365\n",
      "iteration 2700 / 5000: loss 2.052522\n",
      "iteration 2800 / 5000: loss 2.153607\n",
      "iteration 2900 / 5000: loss 2.088954\n",
      "iteration 3000 / 5000: loss 2.098043\n",
      "iteration 3100 / 5000: loss 2.135356\n",
      "iteration 3200 / 5000: loss 2.104840\n",
      "iteration 3300 / 5000: loss 2.121009\n",
      "iteration 3400 / 5000: loss 2.091842\n",
      "iteration 3500 / 5000: loss 2.081050\n",
      "iteration 3600 / 5000: loss 2.131496\n",
      "iteration 3700 / 5000: loss 2.182180\n",
      "iteration 3800 / 5000: loss 2.134129\n",
      "iteration 3900 / 5000: loss 2.104034\n",
      "iteration 4000 / 5000: loss 2.128273\n",
      "iteration 4100 / 5000: loss 2.113140\n",
      "iteration 4200 / 5000: loss 2.175509\n",
      "iteration 4300 / 5000: loss 2.116841\n",
      "iteration 4400 / 5000: loss 2.117015\n",
      "iteration 4500 / 5000: loss 2.121718\n",
      "iteration 4600 / 5000: loss 2.147423\n",
      "iteration 4700 / 5000: loss 2.111351\n",
      "iteration 4800 / 5000: loss 2.148888\n",
      "iteration 4900 / 5000: loss 2.091655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 26/50 [14:43<13:35, 33.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 552.149156\n",
      "iteration 100 / 5000: loss 191.354172\n",
      "iteration 200 / 5000: loss 67.444653\n",
      "iteration 300 / 5000: loss 24.595917\n",
      "iteration 400 / 5000: loss 9.860785\n",
      "iteration 500 / 5000: loss 4.762646\n",
      "iteration 600 / 5000: loss 2.932258\n",
      "iteration 700 / 5000: loss 2.414862\n",
      "iteration 800 / 5000: loss 2.179448\n",
      "iteration 900 / 5000: loss 2.081942\n",
      "iteration 1000 / 5000: loss 2.110262\n",
      "iteration 1100 / 5000: loss 2.014077\n",
      "iteration 1200 / 5000: loss 2.043837\n",
      "iteration 1300 / 5000: loss 2.075325\n",
      "iteration 1400 / 5000: loss 2.089165\n",
      "iteration 1500 / 5000: loss 2.020267\n",
      "iteration 1600 / 5000: loss 2.020982\n",
      "iteration 1700 / 5000: loss 2.024280\n",
      "iteration 1800 / 5000: loss 2.152077\n",
      "iteration 1900 / 5000: loss 2.047754\n",
      "iteration 2000 / 5000: loss 2.077270\n",
      "iteration 2100 / 5000: loss 2.112440\n",
      "iteration 2200 / 5000: loss 2.153144\n",
      "iteration 2300 / 5000: loss 2.050502\n",
      "iteration 2400 / 5000: loss 2.075729\n",
      "iteration 2500 / 5000: loss 2.059069\n",
      "iteration 2600 / 5000: loss 2.115491\n",
      "iteration 2700 / 5000: loss 2.044172\n",
      "iteration 2800 / 5000: loss 2.086026\n",
      "iteration 2900 / 5000: loss 2.005280\n",
      "iteration 3000 / 5000: loss 2.032674\n",
      "iteration 3100 / 5000: loss 2.061767\n",
      "iteration 3200 / 5000: loss 2.069809\n",
      "iteration 3300 / 5000: loss 1.994387\n",
      "iteration 3400 / 5000: loss 2.064056\n",
      "iteration 3500 / 5000: loss 2.104317\n",
      "iteration 3600 / 5000: loss 2.030385\n",
      "iteration 3700 / 5000: loss 2.021669\n",
      "iteration 3800 / 5000: loss 2.018016\n",
      "iteration 3900 / 5000: loss 2.044653\n",
      "iteration 4000 / 5000: loss 2.087131\n",
      "iteration 4100 / 5000: loss 2.118704\n",
      "iteration 4200 / 5000: loss 2.054294\n",
      "iteration 4300 / 5000: loss 2.105978\n",
      "iteration 4400 / 5000: loss 1.986182\n",
      "iteration 4500 / 5000: loss 2.089690\n",
      "iteration 4600 / 5000: loss 2.066317\n",
      "iteration 4700 / 5000: loss 2.042861\n",
      "iteration 4800 / 5000: loss 1.996217\n",
      "iteration 4900 / 5000: loss 2.052052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▍    | 27/50 [15:22<13:06, 34.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 701.702747\n",
      "iteration 100 / 5000: loss 123.150049\n",
      "iteration 200 / 5000: loss 23.098524\n",
      "iteration 300 / 5000: loss 5.719697\n",
      "iteration 400 / 5000: loss 2.736338\n",
      "iteration 500 / 5000: loss 2.160623\n",
      "iteration 600 / 5000: loss 2.132302\n",
      "iteration 700 / 5000: loss 2.100630\n",
      "iteration 800 / 5000: loss 2.053936\n",
      "iteration 900 / 5000: loss 2.078737\n",
      "iteration 1000 / 5000: loss 2.109831\n",
      "iteration 1100 / 5000: loss 2.059928\n",
      "iteration 1200 / 5000: loss 2.079124\n",
      "iteration 1300 / 5000: loss 2.123144\n",
      "iteration 1400 / 5000: loss 2.053747\n",
      "iteration 1500 / 5000: loss 2.063618\n",
      "iteration 1600 / 5000: loss 2.083361\n",
      "iteration 1700 / 5000: loss 2.131177\n",
      "iteration 1800 / 5000: loss 2.035982\n",
      "iteration 1900 / 5000: loss 2.036913\n",
      "iteration 2000 / 5000: loss 2.086729\n",
      "iteration 2100 / 5000: loss 2.125239\n",
      "iteration 2200 / 5000: loss 2.065219\n",
      "iteration 2300 / 5000: loss 2.081561\n",
      "iteration 2400 / 5000: loss 2.027916\n",
      "iteration 2500 / 5000: loss 2.066472\n",
      "iteration 2600 / 5000: loss 2.076690\n",
      "iteration 2700 / 5000: loss 2.096700\n",
      "iteration 2800 / 5000: loss 2.024339\n",
      "iteration 2900 / 5000: loss 2.042214\n",
      "iteration 3000 / 5000: loss 2.092499\n",
      "iteration 3100 / 5000: loss 2.051030\n",
      "iteration 3200 / 5000: loss 2.106678\n",
      "iteration 3300 / 5000: loss 2.128346\n",
      "iteration 3400 / 5000: loss 2.130030\n",
      "iteration 3500 / 5000: loss 2.098664\n",
      "iteration 3600 / 5000: loss 2.123548\n",
      "iteration 3700 / 5000: loss 2.071647\n",
      "iteration 3800 / 5000: loss 2.020447\n",
      "iteration 3900 / 5000: loss 2.061590\n",
      "iteration 4000 / 5000: loss 2.093055\n",
      "iteration 4100 / 5000: loss 2.079948\n",
      "iteration 4200 / 5000: loss 2.095923\n",
      "iteration 4300 / 5000: loss 2.098077\n",
      "iteration 4400 / 5000: loss 2.076016\n",
      "iteration 4500 / 5000: loss 2.115162\n",
      "iteration 4600 / 5000: loss 2.045892\n",
      "iteration 4700 / 5000: loss 2.051807\n",
      "iteration 4800 / 5000: loss 2.058929\n",
      "iteration 4900 / 5000: loss 2.087995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 56%|█████▌    | 28/50 [16:02<12:36, 34.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1372.128419\n",
      "iteration 100 / 5000: loss 45.485215\n",
      "iteration 200 / 5000: loss 3.464390\n",
      "iteration 300 / 5000: loss 2.144148\n",
      "iteration 400 / 5000: loss 2.105071\n",
      "iteration 500 / 5000: loss 2.200544\n",
      "iteration 600 / 5000: loss 2.146614\n",
      "iteration 700 / 5000: loss 2.114993\n",
      "iteration 800 / 5000: loss 2.196402\n",
      "iteration 900 / 5000: loss 2.140241\n",
      "iteration 1000 / 5000: loss 2.145487\n",
      "iteration 1100 / 5000: loss 2.161734\n",
      "iteration 1200 / 5000: loss 2.097391\n",
      "iteration 1300 / 5000: loss 2.151726\n",
      "iteration 1400 / 5000: loss 2.169021\n",
      "iteration 1500 / 5000: loss 2.176981\n",
      "iteration 1600 / 5000: loss 2.150009\n",
      "iteration 1700 / 5000: loss 2.144577\n",
      "iteration 1800 / 5000: loss 2.089755\n",
      "iteration 1900 / 5000: loss 2.116120\n",
      "iteration 2000 / 5000: loss 2.149102\n",
      "iteration 2100 / 5000: loss 2.079098\n",
      "iteration 2200 / 5000: loss 2.113905\n",
      "iteration 2300 / 5000: loss 2.143996\n",
      "iteration 2400 / 5000: loss 2.083719\n",
      "iteration 2500 / 5000: loss 2.145534\n",
      "iteration 2600 / 5000: loss 2.141393\n",
      "iteration 2700 / 5000: loss 2.101423\n",
      "iteration 2800 / 5000: loss 2.168179\n",
      "iteration 2900 / 5000: loss 2.089944\n",
      "iteration 3000 / 5000: loss 2.122588\n",
      "iteration 3100 / 5000: loss 2.177959\n",
      "iteration 3200 / 5000: loss 2.127058\n",
      "iteration 3300 / 5000: loss 2.134892\n",
      "iteration 3400 / 5000: loss 2.142185\n",
      "iteration 3500 / 5000: loss 2.080551\n",
      "iteration 3600 / 5000: loss 2.169422\n",
      "iteration 3700 / 5000: loss 2.144085\n",
      "iteration 3800 / 5000: loss 2.131977\n",
      "iteration 3900 / 5000: loss 2.136017\n",
      "iteration 4000 / 5000: loss 2.135693\n",
      "iteration 4100 / 5000: loss 2.127280\n",
      "iteration 4200 / 5000: loss 2.167853\n",
      "iteration 4300 / 5000: loss 2.136465\n",
      "iteration 4400 / 5000: loss 2.100077\n",
      "iteration 4500 / 5000: loss 2.141608\n",
      "iteration 4600 / 5000: loss 2.158541\n",
      "iteration 4700 / 5000: loss 2.131341\n",
      "iteration 4800 / 5000: loss 2.154601\n",
      "iteration 4900 / 5000: loss 2.181479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 29/50 [16:38<12:03, 34.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1481.111345\n",
      "iteration 100 / 5000: loss 24.743901\n",
      "iteration 200 / 5000: loss 2.499639\n",
      "iteration 300 / 5000: loss 2.116303\n",
      "iteration 400 / 5000: loss 2.155264\n",
      "iteration 500 / 5000: loss 2.144720\n",
      "iteration 600 / 5000: loss 2.118301\n",
      "iteration 700 / 5000: loss 2.113768\n",
      "iteration 800 / 5000: loss 2.166563\n",
      "iteration 900 / 5000: loss 2.139326\n",
      "iteration 1000 / 5000: loss 2.148867\n",
      "iteration 1100 / 5000: loss 2.156356\n",
      "iteration 1200 / 5000: loss 2.138859\n",
      "iteration 1300 / 5000: loss 2.110857\n",
      "iteration 1400 / 5000: loss 2.112597\n",
      "iteration 1500 / 5000: loss 2.173994\n",
      "iteration 1600 / 5000: loss 2.162767\n",
      "iteration 1700 / 5000: loss 2.144512\n",
      "iteration 1800 / 5000: loss 2.148719\n",
      "iteration 1900 / 5000: loss 2.111390\n",
      "iteration 2000 / 5000: loss 2.164987\n",
      "iteration 2100 / 5000: loss 2.160143\n",
      "iteration 2200 / 5000: loss 2.173507\n",
      "iteration 2300 / 5000: loss 2.108545\n",
      "iteration 2400 / 5000: loss 2.168232\n",
      "iteration 2500 / 5000: loss 2.164030\n",
      "iteration 2600 / 5000: loss 2.105026\n",
      "iteration 2700 / 5000: loss 2.190718\n",
      "iteration 2800 / 5000: loss 2.145447\n",
      "iteration 2900 / 5000: loss 2.093495\n",
      "iteration 3000 / 5000: loss 2.098896\n",
      "iteration 3100 / 5000: loss 2.137085\n",
      "iteration 3200 / 5000: loss 2.133269\n",
      "iteration 3300 / 5000: loss 2.166373\n",
      "iteration 3400 / 5000: loss 2.099610\n",
      "iteration 3500 / 5000: loss 2.146852\n",
      "iteration 3600 / 5000: loss 2.158491\n",
      "iteration 3700 / 5000: loss 2.101072\n",
      "iteration 3800 / 5000: loss 2.116210\n",
      "iteration 3900 / 5000: loss 2.111098\n",
      "iteration 4000 / 5000: loss 2.143097\n",
      "iteration 4100 / 5000: loss 2.135708\n",
      "iteration 4200 / 5000: loss 2.123735\n",
      "iteration 4300 / 5000: loss 2.152847\n",
      "iteration 4400 / 5000: loss 2.170723\n",
      "iteration 4500 / 5000: loss 2.119356\n",
      "iteration 4600 / 5000: loss 2.144550\n",
      "iteration 4700 / 5000: loss 2.159852\n",
      "iteration 4800 / 5000: loss 2.140754\n",
      "iteration 4900 / 5000: loss 2.169422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 30/50 [17:12<11:28, 34.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 812.481137\n",
      "iteration 100 / 5000: loss 4.550741\n",
      "iteration 200 / 5000: loss 2.055764\n",
      "iteration 300 / 5000: loss 2.118273\n",
      "iteration 400 / 5000: loss 2.169503\n",
      "iteration 500 / 5000: loss 2.100130\n",
      "iteration 600 / 5000: loss 2.052640\n",
      "iteration 700 / 5000: loss 2.118627\n",
      "iteration 800 / 5000: loss 2.056925\n",
      "iteration 900 / 5000: loss 2.101009\n",
      "iteration 1000 / 5000: loss 2.128009\n",
      "iteration 1100 / 5000: loss 2.072160\n",
      "iteration 1200 / 5000: loss 2.152258\n",
      "iteration 1300 / 5000: loss 2.109557\n",
      "iteration 1400 / 5000: loss 2.106352\n",
      "iteration 1500 / 5000: loss 2.084600\n",
      "iteration 1600 / 5000: loss 2.085983\n",
      "iteration 1700 / 5000: loss 2.114808\n",
      "iteration 1800 / 5000: loss 2.075054\n",
      "iteration 1900 / 5000: loss 2.036471\n",
      "iteration 2000 / 5000: loss 2.047760\n",
      "iteration 2100 / 5000: loss 2.126230\n",
      "iteration 2200 / 5000: loss 2.142584\n",
      "iteration 2300 / 5000: loss 2.106491\n",
      "iteration 2400 / 5000: loss 2.053032\n",
      "iteration 2500 / 5000: loss 2.033156\n",
      "iteration 2600 / 5000: loss 2.092182\n",
      "iteration 2700 / 5000: loss 2.176955\n",
      "iteration 2800 / 5000: loss 2.104980\n",
      "iteration 2900 / 5000: loss 2.152276\n",
      "iteration 3000 / 5000: loss 2.109808\n",
      "iteration 3100 / 5000: loss 2.101723\n",
      "iteration 3200 / 5000: loss 2.079751\n",
      "iteration 3300 / 5000: loss 2.086113\n",
      "iteration 3400 / 5000: loss 2.069173\n",
      "iteration 3500 / 5000: loss 2.036007\n",
      "iteration 3600 / 5000: loss 2.127041\n",
      "iteration 3700 / 5000: loss 2.092365\n",
      "iteration 3800 / 5000: loss 2.036152\n",
      "iteration 3900 / 5000: loss 2.059902\n",
      "iteration 4000 / 5000: loss 2.108135\n",
      "iteration 4100 / 5000: loss 2.057678\n",
      "iteration 4200 / 5000: loss 2.115133\n",
      "iteration 4300 / 5000: loss 2.098475\n",
      "iteration 4400 / 5000: loss 2.115513\n",
      "iteration 4500 / 5000: loss 2.083794\n",
      "iteration 4600 / 5000: loss 2.145389\n",
      "iteration 4700 / 5000: loss 2.082774\n",
      "iteration 4800 / 5000: loss 2.123453\n",
      "iteration 4900 / 5000: loss 2.094020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 62%|██████▏   | 31/50 [17:46<10:53, 34.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1424.812710\n",
      "iteration 100 / 5000: loss 119.694730\n",
      "iteration 200 / 5000: loss 11.864861\n",
      "iteration 300 / 5000: loss 2.961365\n",
      "iteration 400 / 5000: loss 2.207706\n",
      "iteration 500 / 5000: loss 2.137422\n",
      "iteration 600 / 5000: loss 2.190128\n",
      "iteration 700 / 5000: loss 2.162374\n",
      "iteration 800 / 5000: loss 2.136148\n",
      "iteration 900 / 5000: loss 2.154243\n",
      "iteration 1000 / 5000: loss 2.121275\n",
      "iteration 1100 / 5000: loss 2.111047\n",
      "iteration 1200 / 5000: loss 2.141637\n",
      "iteration 1300 / 5000: loss 2.132232\n",
      "iteration 1400 / 5000: loss 2.160579\n",
      "iteration 1500 / 5000: loss 2.095237\n",
      "iteration 1600 / 5000: loss 2.123459\n",
      "iteration 1700 / 5000: loss 2.119120\n",
      "iteration 1800 / 5000: loss 2.086048\n",
      "iteration 1900 / 5000: loss 2.141621\n",
      "iteration 2000 / 5000: loss 2.126034\n",
      "iteration 2100 / 5000: loss 2.158698\n",
      "iteration 2200 / 5000: loss 2.134543\n",
      "iteration 2300 / 5000: loss 2.156734\n",
      "iteration 2400 / 5000: loss 2.185836\n",
      "iteration 2500 / 5000: loss 2.122050\n",
      "iteration 2600 / 5000: loss 2.217920\n",
      "iteration 2700 / 5000: loss 2.115616\n",
      "iteration 2800 / 5000: loss 2.145512\n",
      "iteration 2900 / 5000: loss 2.134536\n",
      "iteration 3000 / 5000: loss 2.114594\n",
      "iteration 3100 / 5000: loss 2.109312\n",
      "iteration 3200 / 5000: loss 2.189354\n",
      "iteration 3300 / 5000: loss 2.091833\n",
      "iteration 3400 / 5000: loss 2.140822\n",
      "iteration 3500 / 5000: loss 2.118631\n",
      "iteration 3600 / 5000: loss 2.122247\n",
      "iteration 3700 / 5000: loss 2.080828\n",
      "iteration 3800 / 5000: loss 2.174067\n",
      "iteration 3900 / 5000: loss 2.124856\n",
      "iteration 4000 / 5000: loss 2.131720\n",
      "iteration 4100 / 5000: loss 2.171472\n",
      "iteration 4200 / 5000: loss 2.131974\n",
      "iteration 4300 / 5000: loss 2.165457\n",
      "iteration 4400 / 5000: loss 2.138830\n",
      "iteration 4500 / 5000: loss 2.116209\n",
      "iteration 4600 / 5000: loss 2.123786\n",
      "iteration 4700 / 5000: loss 2.184305\n",
      "iteration 4800 / 5000: loss 2.125059\n",
      "iteration 4900 / 5000: loss 2.093223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 32/50 [18:28<10:23, 34.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 592.134248\n",
      "iteration 100 / 5000: loss 228.488915\n",
      "iteration 200 / 5000: loss 89.164223\n",
      "iteration 300 / 5000: loss 35.480838\n",
      "iteration 400 / 5000: loss 14.956744\n",
      "iteration 500 / 5000: loss 6.946031\n",
      "iteration 600 / 5000: loss 4.017560\n",
      "iteration 700 / 5000: loss 2.832779\n",
      "iteration 800 / 5000: loss 2.360277\n",
      "iteration 900 / 5000: loss 2.199759\n",
      "iteration 1000 / 5000: loss 2.140693\n",
      "iteration 1100 / 5000: loss 2.100477\n",
      "iteration 1200 / 5000: loss 2.066879\n",
      "iteration 1300 / 5000: loss 2.021376\n",
      "iteration 1400 / 5000: loss 2.027394\n",
      "iteration 1500 / 5000: loss 2.029190\n",
      "iteration 1600 / 5000: loss 2.113108\n",
      "iteration 1700 / 5000: loss 1.999629\n",
      "iteration 1800 / 5000: loss 2.066138\n",
      "iteration 1900 / 5000: loss 2.090845\n",
      "iteration 2000 / 5000: loss 2.022721\n",
      "iteration 2100 / 5000: loss 2.045559\n",
      "iteration 2200 / 5000: loss 2.065684\n",
      "iteration 2300 / 5000: loss 2.072336\n",
      "iteration 2400 / 5000: loss 2.025871\n",
      "iteration 2500 / 5000: loss 2.008230\n",
      "iteration 2600 / 5000: loss 2.047602\n",
      "iteration 2700 / 5000: loss 2.079816\n",
      "iteration 2800 / 5000: loss 2.037506\n",
      "iteration 2900 / 5000: loss 2.064935\n",
      "iteration 3000 / 5000: loss 2.031073\n",
      "iteration 3100 / 5000: loss 2.038705\n",
      "iteration 3200 / 5000: loss 2.075028\n",
      "iteration 3300 / 5000: loss 2.089682\n",
      "iteration 3400 / 5000: loss 2.061795\n",
      "iteration 3500 / 5000: loss 2.101160\n",
      "iteration 3600 / 5000: loss 2.018296\n",
      "iteration 3700 / 5000: loss 2.061762\n",
      "iteration 3800 / 5000: loss 2.087126\n",
      "iteration 3900 / 5000: loss 2.072933\n",
      "iteration 4000 / 5000: loss 2.095465\n",
      "iteration 4100 / 5000: loss 2.049097\n",
      "iteration 4200 / 5000: loss 2.067057\n",
      "iteration 4300 / 5000: loss 2.018400\n",
      "iteration 4400 / 5000: loss 2.099141\n",
      "iteration 4500 / 5000: loss 2.075229\n",
      "iteration 4600 / 5000: loss 2.078366\n",
      "iteration 4700 / 5000: loss 2.018690\n",
      "iteration 4800 / 5000: loss 2.104878\n",
      "iteration 4900 / 5000: loss 2.041360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████▌   | 33/50 [19:00<09:47, 34.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 752.327234\n",
      "iteration 100 / 5000: loss 30.610659\n",
      "iteration 200 / 5000: loss 3.143253\n",
      "iteration 300 / 5000: loss 2.105753\n",
      "iteration 400 / 5000: loss 2.110787\n",
      "iteration 500 / 5000: loss 2.049849\n",
      "iteration 600 / 5000: loss 2.087877\n",
      "iteration 700 / 5000: loss 2.047353\n",
      "iteration 800 / 5000: loss 2.089748\n",
      "iteration 900 / 5000: loss 2.108219\n",
      "iteration 1000 / 5000: loss 2.066366\n",
      "iteration 1100 / 5000: loss 2.063732\n",
      "iteration 1200 / 5000: loss 2.090654\n",
      "iteration 1300 / 5000: loss 2.095492\n",
      "iteration 1400 / 5000: loss 2.070495\n",
      "iteration 1500 / 5000: loss 2.088118\n",
      "iteration 1600 / 5000: loss 2.016430\n",
      "iteration 1700 / 5000: loss 2.081466\n",
      "iteration 1800 / 5000: loss 2.047560\n",
      "iteration 1900 / 5000: loss 2.149862\n",
      "iteration 2000 / 5000: loss 2.073374\n",
      "iteration 2100 / 5000: loss 2.048195\n",
      "iteration 2200 / 5000: loss 2.001388\n",
      "iteration 2300 / 5000: loss 2.106977\n",
      "iteration 2400 / 5000: loss 2.107760\n",
      "iteration 2500 / 5000: loss 2.112892\n",
      "iteration 2600 / 5000: loss 2.085575\n",
      "iteration 2700 / 5000: loss 2.049613\n",
      "iteration 2800 / 5000: loss 2.129362\n",
      "iteration 2900 / 5000: loss 2.036661\n",
      "iteration 3000 / 5000: loss 2.129940\n",
      "iteration 3100 / 5000: loss 2.108059\n",
      "iteration 3200 / 5000: loss 2.004290\n",
      "iteration 3300 / 5000: loss 2.108136\n",
      "iteration 3400 / 5000: loss 2.099270\n",
      "iteration 3500 / 5000: loss 2.097699\n",
      "iteration 3600 / 5000: loss 2.081630\n",
      "iteration 3700 / 5000: loss 2.162518\n",
      "iteration 3800 / 5000: loss 2.142700\n",
      "iteration 3900 / 5000: loss 2.118191\n",
      "iteration 4000 / 5000: loss 2.127805\n",
      "iteration 4100 / 5000: loss 2.017707\n",
      "iteration 4200 / 5000: loss 2.081753\n",
      "iteration 4300 / 5000: loss 2.114626\n",
      "iteration 4400 / 5000: loss 2.195397\n",
      "iteration 4500 / 5000: loss 2.115236\n",
      "iteration 4600 / 5000: loss 2.100657\n",
      "iteration 4700 / 5000: loss 2.061332\n",
      "iteration 4800 / 5000: loss 2.099316\n",
      "iteration 4900 / 5000: loss 2.102872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 68%|██████▊   | 34/50 [19:39<09:15, 34.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1101.406209\n",
      "iteration 100 / 5000: loss 22.383568\n",
      "iteration 200 / 5000: loss 2.468671\n",
      "iteration 300 / 5000: loss 2.043096\n",
      "iteration 400 / 5000: loss 2.116490\n",
      "iteration 500 / 5000: loss 2.107480\n",
      "iteration 600 / 5000: loss 2.130904\n",
      "iteration 700 / 5000: loss 2.047602\n",
      "iteration 800 / 5000: loss 2.057443\n",
      "iteration 900 / 5000: loss 2.101140\n",
      "iteration 1000 / 5000: loss 2.132464\n",
      "iteration 1100 / 5000: loss 2.121017\n",
      "iteration 1200 / 5000: loss 2.086257\n",
      "iteration 1300 / 5000: loss 2.140581\n",
      "iteration 1400 / 5000: loss 2.057179\n",
      "iteration 1500 / 5000: loss 2.130934\n",
      "iteration 1600 / 5000: loss 2.198419\n",
      "iteration 1700 / 5000: loss 2.115243\n",
      "iteration 1800 / 5000: loss 2.169165\n",
      "iteration 1900 / 5000: loss 2.140378\n",
      "iteration 2000 / 5000: loss 2.144108\n",
      "iteration 2100 / 5000: loss 2.132381\n",
      "iteration 2200 / 5000: loss 2.083766\n",
      "iteration 2300 / 5000: loss 2.167589\n",
      "iteration 2400 / 5000: loss 2.099914\n",
      "iteration 2500 / 5000: loss 2.137337\n",
      "iteration 2600 / 5000: loss 2.109067\n",
      "iteration 2700 / 5000: loss 2.135510\n",
      "iteration 2800 / 5000: loss 2.109992\n",
      "iteration 2900 / 5000: loss 2.127103\n",
      "iteration 3000 / 5000: loss 2.130281\n",
      "iteration 3100 / 5000: loss 2.130550\n",
      "iteration 3200 / 5000: loss 2.097021\n",
      "iteration 3300 / 5000: loss 2.129109\n",
      "iteration 3400 / 5000: loss 2.097908\n",
      "iteration 3500 / 5000: loss 2.135669\n",
      "iteration 3600 / 5000: loss 2.178704\n",
      "iteration 3700 / 5000: loss 2.159307\n",
      "iteration 3800 / 5000: loss 2.165302\n",
      "iteration 3900 / 5000: loss 2.086523\n",
      "iteration 4000 / 5000: loss 2.086584\n",
      "iteration 4100 / 5000: loss 2.052523\n",
      "iteration 4200 / 5000: loss 2.142830\n",
      "iteration 4300 / 5000: loss 2.105439\n",
      "iteration 4400 / 5000: loss 2.179080\n",
      "iteration 4500 / 5000: loss 2.127218\n",
      "iteration 4600 / 5000: loss 2.079376\n",
      "iteration 4700 / 5000: loss 2.159523\n",
      "iteration 4800 / 5000: loss 2.105535\n",
      "iteration 4900 / 5000: loss 2.088898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 35/50 [20:19<08:42, 34.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1497.273852\n",
      "iteration 100 / 5000: loss 76.817173\n",
      "iteration 200 / 5000: loss 5.918627\n",
      "iteration 300 / 5000: loss 2.336630\n",
      "iteration 400 / 5000: loss 2.174218\n",
      "iteration 500 / 5000: loss 2.089967\n",
      "iteration 600 / 5000: loss 2.110261\n",
      "iteration 700 / 5000: loss 2.180923\n",
      "iteration 800 / 5000: loss 2.175868\n",
      "iteration 900 / 5000: loss 2.143756\n",
      "iteration 1000 / 5000: loss 2.167015\n",
      "iteration 1100 / 5000: loss 2.143376\n",
      "iteration 1200 / 5000: loss 2.092205\n",
      "iteration 1300 / 5000: loss 2.123354\n",
      "iteration 1400 / 5000: loss 2.122324\n",
      "iteration 1500 / 5000: loss 2.141650\n",
      "iteration 1600 / 5000: loss 2.135614\n",
      "iteration 1700 / 5000: loss 2.147884\n",
      "iteration 1800 / 5000: loss 2.137171\n",
      "iteration 1900 / 5000: loss 2.118325\n",
      "iteration 2000 / 5000: loss 2.136343\n",
      "iteration 2100 / 5000: loss 2.140678\n",
      "iteration 2200 / 5000: loss 2.115610\n",
      "iteration 2300 / 5000: loss 2.114387\n",
      "iteration 2400 / 5000: loss 2.126409\n",
      "iteration 2500 / 5000: loss 2.135048\n",
      "iteration 2600 / 5000: loss 2.136017\n",
      "iteration 2700 / 5000: loss 2.140299\n",
      "iteration 2800 / 5000: loss 2.123903\n",
      "iteration 2900 / 5000: loss 2.118953\n",
      "iteration 3000 / 5000: loss 2.098351\n",
      "iteration 3100 / 5000: loss 2.135512\n",
      "iteration 3200 / 5000: loss 2.173499\n",
      "iteration 3300 / 5000: loss 2.160576\n",
      "iteration 3400 / 5000: loss 2.123671\n",
      "iteration 3500 / 5000: loss 2.061418\n",
      "iteration 3600 / 5000: loss 2.172580\n",
      "iteration 3700 / 5000: loss 2.150121\n",
      "iteration 3800 / 5000: loss 2.137355\n",
      "iteration 3900 / 5000: loss 2.150150\n",
      "iteration 4000 / 5000: loss 2.119744\n",
      "iteration 4100 / 5000: loss 2.128459\n",
      "iteration 4200 / 5000: loss 2.145000\n",
      "iteration 4300 / 5000: loss 2.106364\n",
      "iteration 4400 / 5000: loss 2.145682\n",
      "iteration 4500 / 5000: loss 2.138450\n",
      "iteration 4600 / 5000: loss 2.150237\n",
      "iteration 4700 / 5000: loss 2.139148\n",
      "iteration 4800 / 5000: loss 2.122300\n",
      "iteration 4900 / 5000: loss 2.124398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 36/50 [20:54<08:07, 34.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 959.164308\n",
      "iteration 100 / 5000: loss 52.946622\n",
      "iteration 200 / 5000: loss 4.824810\n",
      "iteration 300 / 5000: loss 2.200428\n",
      "iteration 400 / 5000: loss 2.144914\n",
      "iteration 500 / 5000: loss 2.118361\n",
      "iteration 600 / 5000: loss 2.079816\n",
      "iteration 700 / 5000: loss 2.133709\n",
      "iteration 800 / 5000: loss 2.099072\n",
      "iteration 900 / 5000: loss 2.105123\n",
      "iteration 1000 / 5000: loss 2.154338\n",
      "iteration 1100 / 5000: loss 2.105702\n",
      "iteration 1200 / 5000: loss 2.073101\n",
      "iteration 1300 / 5000: loss 2.143624\n",
      "iteration 1400 / 5000: loss 2.132245\n",
      "iteration 1500 / 5000: loss 2.114743\n",
      "iteration 1600 / 5000: loss 2.091987\n",
      "iteration 1700 / 5000: loss 2.076750\n",
      "iteration 1800 / 5000: loss 2.074996\n",
      "iteration 1900 / 5000: loss 2.062518\n",
      "iteration 2000 / 5000: loss 2.140024\n",
      "iteration 2100 / 5000: loss 2.056712\n",
      "iteration 2200 / 5000: loss 2.078308\n",
      "iteration 2300 / 5000: loss 2.108727\n",
      "iteration 2400 / 5000: loss 2.046009\n",
      "iteration 2500 / 5000: loss 2.102008\n",
      "iteration 2600 / 5000: loss 2.095866\n",
      "iteration 2700 / 5000: loss 2.170897\n",
      "iteration 2800 / 5000: loss 2.100583\n",
      "iteration 2900 / 5000: loss 2.094124\n",
      "iteration 3000 / 5000: loss 2.037577\n",
      "iteration 3100 / 5000: loss 2.079888\n",
      "iteration 3200 / 5000: loss 2.115651\n",
      "iteration 3300 / 5000: loss 2.041178\n",
      "iteration 3400 / 5000: loss 2.135150\n",
      "iteration 3500 / 5000: loss 2.062372\n",
      "iteration 3600 / 5000: loss 2.135807\n",
      "iteration 3700 / 5000: loss 2.146336\n",
      "iteration 3800 / 5000: loss 2.077675\n",
      "iteration 3900 / 5000: loss 2.120427\n",
      "iteration 4000 / 5000: loss 2.079656\n",
      "iteration 4100 / 5000: loss 2.100525\n",
      "iteration 4200 / 5000: loss 2.091859\n",
      "iteration 4300 / 5000: loss 2.042743\n",
      "iteration 4400 / 5000: loss 2.082967\n",
      "iteration 4500 / 5000: loss 2.127921\n",
      "iteration 4600 / 5000: loss 2.153825\n",
      "iteration 4700 / 5000: loss 2.062163\n",
      "iteration 4800 / 5000: loss 2.080111\n",
      "iteration 4900 / 5000: loss 2.109495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 74%|███████▍  | 37/50 [21:31<07:33, 34.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1354.580708\n",
      "iteration 100 / 5000: loss 2.243713\n",
      "iteration 200 / 5000: loss 2.106058\n",
      "iteration 300 / 5000: loss 2.153319\n",
      "iteration 400 / 5000: loss 2.090283\n",
      "iteration 500 / 5000: loss 2.129751\n",
      "iteration 600 / 5000: loss 2.138882\n",
      "iteration 700 / 5000: loss 2.164761\n",
      "iteration 800 / 5000: loss 2.120707\n",
      "iteration 900 / 5000: loss 2.125647\n",
      "iteration 1000 / 5000: loss 2.110991\n",
      "iteration 1100 / 5000: loss 2.204853\n",
      "iteration 1200 / 5000: loss 2.190701\n",
      "iteration 1300 / 5000: loss 2.111554\n",
      "iteration 1400 / 5000: loss 2.117221\n",
      "iteration 1500 / 5000: loss 2.134471\n",
      "iteration 1600 / 5000: loss 2.165781\n",
      "iteration 1700 / 5000: loss 2.130626\n",
      "iteration 1800 / 5000: loss 2.101903\n",
      "iteration 1900 / 5000: loss 2.145679\n",
      "iteration 2000 / 5000: loss 2.125092\n",
      "iteration 2100 / 5000: loss 2.153102\n",
      "iteration 2200 / 5000: loss 2.158734\n",
      "iteration 2300 / 5000: loss 2.159604\n",
      "iteration 2400 / 5000: loss 2.182741\n",
      "iteration 2500 / 5000: loss 2.106007\n",
      "iteration 2600 / 5000: loss 2.168760\n",
      "iteration 2700 / 5000: loss 2.103067\n",
      "iteration 2800 / 5000: loss 2.162318\n",
      "iteration 2900 / 5000: loss 2.113701\n",
      "iteration 3000 / 5000: loss 2.176904\n",
      "iteration 3100 / 5000: loss 2.140411\n",
      "iteration 3200 / 5000: loss 2.085453\n",
      "iteration 3300 / 5000: loss 2.154041\n",
      "iteration 3400 / 5000: loss 2.136240\n",
      "iteration 3500 / 5000: loss 2.118777\n",
      "iteration 3600 / 5000: loss 2.142071\n",
      "iteration 3700 / 5000: loss 2.156334\n",
      "iteration 3800 / 5000: loss 2.121691\n",
      "iteration 3900 / 5000: loss 2.140321\n",
      "iteration 4000 / 5000: loss 2.119972\n",
      "iteration 4100 / 5000: loss 2.119497\n",
      "iteration 4200 / 5000: loss 2.164300\n",
      "iteration 4300 / 5000: loss 2.148591\n",
      "iteration 4400 / 5000: loss 2.126386\n",
      "iteration 4500 / 5000: loss 2.178817\n",
      "iteration 4600 / 5000: loss 2.119216\n",
      "iteration 4700 / 5000: loss 2.126768\n",
      "iteration 4800 / 5000: loss 2.102996\n",
      "iteration 4900 / 5000: loss 2.153241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 38/50 [22:15<07:01, 35.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 799.066692\n",
      "iteration 100 / 5000: loss 91.468404\n",
      "iteration 200 / 5000: loss 12.137939\n",
      "iteration 300 / 5000: loss 3.231607\n",
      "iteration 400 / 5000: loss 2.159429\n",
      "iteration 500 / 5000: loss 2.067916\n",
      "iteration 600 / 5000: loss 2.092160\n",
      "iteration 700 / 5000: loss 2.069168\n",
      "iteration 800 / 5000: loss 2.095429\n",
      "iteration 900 / 5000: loss 2.113316\n",
      "iteration 1000 / 5000: loss 2.124644\n",
      "iteration 1100 / 5000: loss 2.135084\n",
      "iteration 1200 / 5000: loss 2.069154\n",
      "iteration 1300 / 5000: loss 2.087703\n",
      "iteration 1400 / 5000: loss 2.079831\n",
      "iteration 1500 / 5000: loss 2.071923\n",
      "iteration 1600 / 5000: loss 2.118291\n",
      "iteration 1700 / 5000: loss 2.064095\n",
      "iteration 1800 / 5000: loss 2.043884\n",
      "iteration 1900 / 5000: loss 2.140656\n",
      "iteration 2000 / 5000: loss 2.078392\n",
      "iteration 2100 / 5000: loss 2.140134\n",
      "iteration 2200 / 5000: loss 2.074899\n",
      "iteration 2300 / 5000: loss 2.057251\n",
      "iteration 2400 / 5000: loss 2.064822\n",
      "iteration 2500 / 5000: loss 2.137979\n",
      "iteration 2600 / 5000: loss 2.122348\n",
      "iteration 2700 / 5000: loss 2.091849\n",
      "iteration 2800 / 5000: loss 2.072218\n",
      "iteration 2900 / 5000: loss 2.088153\n",
      "iteration 3000 / 5000: loss 2.076690\n",
      "iteration 3100 / 5000: loss 2.007537\n",
      "iteration 3200 / 5000: loss 2.133494\n",
      "iteration 3300 / 5000: loss 2.026545\n",
      "iteration 3400 / 5000: loss 2.134060\n",
      "iteration 3500 / 5000: loss 2.121631\n",
      "iteration 3600 / 5000: loss 2.100217\n",
      "iteration 3700 / 5000: loss 2.058678\n",
      "iteration 3800 / 5000: loss 2.022984\n",
      "iteration 3900 / 5000: loss 2.066124\n",
      "iteration 4000 / 5000: loss 2.077982\n",
      "iteration 4100 / 5000: loss 2.072742\n",
      "iteration 4200 / 5000: loss 2.122375\n",
      "iteration 4300 / 5000: loss 2.075833\n",
      "iteration 4400 / 5000: loss 2.059534\n",
      "iteration 4500 / 5000: loss 2.071318\n",
      "iteration 4600 / 5000: loss 2.072921\n",
      "iteration 4700 / 5000: loss 2.102286\n",
      "iteration 4800 / 5000: loss 2.115057\n",
      "iteration 4900 / 5000: loss 2.088191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 39/50 [22:52<06:27, 35.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 757.109786\n",
      "iteration 100 / 5000: loss 2.190812\n",
      "iteration 200 / 5000: loss 2.059647\n",
      "iteration 300 / 5000: loss 2.145597\n",
      "iteration 400 / 5000: loss 2.169502\n",
      "iteration 500 / 5000: loss 2.044595\n",
      "iteration 600 / 5000: loss 2.099366\n",
      "iteration 700 / 5000: loss 2.163721\n",
      "iteration 800 / 5000: loss 2.122595\n",
      "iteration 900 / 5000: loss 2.058754\n",
      "iteration 1000 / 5000: loss 2.112016\n",
      "iteration 1100 / 5000: loss 2.066058\n",
      "iteration 1200 / 5000: loss 2.030704\n",
      "iteration 1300 / 5000: loss 2.062901\n",
      "iteration 1400 / 5000: loss 2.103080\n",
      "iteration 1500 / 5000: loss 2.108493\n",
      "iteration 1600 / 5000: loss 2.131986\n",
      "iteration 1700 / 5000: loss 2.138983\n",
      "iteration 1800 / 5000: loss 2.091599\n",
      "iteration 1900 / 5000: loss 2.007429\n",
      "iteration 2000 / 5000: loss 2.148354\n",
      "iteration 2100 / 5000: loss 2.096807\n",
      "iteration 2200 / 5000: loss 2.084943\n",
      "iteration 2300 / 5000: loss 2.056769\n",
      "iteration 2400 / 5000: loss 2.130907\n",
      "iteration 2500 / 5000: loss 2.116648\n",
      "iteration 2600 / 5000: loss 2.118186\n",
      "iteration 2700 / 5000: loss 2.029118\n",
      "iteration 2800 / 5000: loss 2.062432\n",
      "iteration 2900 / 5000: loss 2.120505\n",
      "iteration 3000 / 5000: loss 2.078487\n",
      "iteration 3100 / 5000: loss 2.150812\n",
      "iteration 3200 / 5000: loss 2.114265\n",
      "iteration 3300 / 5000: loss 2.138867\n",
      "iteration 3400 / 5000: loss 1.989757\n",
      "iteration 3500 / 5000: loss 2.060534\n",
      "iteration 3600 / 5000: loss 2.121450\n",
      "iteration 3700 / 5000: loss 2.076291\n",
      "iteration 3800 / 5000: loss 2.044099\n",
      "iteration 3900 / 5000: loss 2.032175\n",
      "iteration 4000 / 5000: loss 2.099367\n",
      "iteration 4100 / 5000: loss 2.098963\n",
      "iteration 4200 / 5000: loss 2.108339\n",
      "iteration 4300 / 5000: loss 2.125929\n",
      "iteration 4400 / 5000: loss 2.075224\n",
      "iteration 4500 / 5000: loss 1.988927\n",
      "iteration 4600 / 5000: loss 2.133389\n",
      "iteration 4700 / 5000: loss 2.059137\n",
      "iteration 4800 / 5000: loss 2.126210\n",
      "iteration 4900 / 5000: loss 2.062010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 40/50 [23:36<05:54, 35.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 806.841720\n",
      "iteration 100 / 5000: loss 9.389429\n",
      "iteration 200 / 5000: loss 2.205374\n",
      "iteration 300 / 5000: loss 2.079603\n",
      "iteration 400 / 5000: loss 2.145544\n",
      "iteration 500 / 5000: loss 2.053839\n",
      "iteration 600 / 5000: loss 2.143615\n",
      "iteration 700 / 5000: loss 2.091827\n",
      "iteration 800 / 5000: loss 2.143631\n",
      "iteration 900 / 5000: loss 2.094090\n",
      "iteration 1000 / 5000: loss 2.089911\n",
      "iteration 1100 / 5000: loss 2.156277\n",
      "iteration 1200 / 5000: loss 2.116640\n",
      "iteration 1300 / 5000: loss 2.087760\n",
      "iteration 1400 / 5000: loss 2.089517\n",
      "iteration 1500 / 5000: loss 2.056785\n",
      "iteration 1600 / 5000: loss 2.103798\n",
      "iteration 1700 / 5000: loss 2.117122\n",
      "iteration 1800 / 5000: loss 2.083601\n",
      "iteration 1900 / 5000: loss 2.117286\n",
      "iteration 2000 / 5000: loss 2.136190\n",
      "iteration 2100 / 5000: loss 2.065638\n",
      "iteration 2200 / 5000: loss 2.097944\n",
      "iteration 2300 / 5000: loss 2.121957\n",
      "iteration 2400 / 5000: loss 2.085984\n",
      "iteration 2500 / 5000: loss 2.076589\n",
      "iteration 2600 / 5000: loss 2.123300\n",
      "iteration 2700 / 5000: loss 2.107155\n",
      "iteration 2800 / 5000: loss 2.086951\n",
      "iteration 2900 / 5000: loss 2.150346\n",
      "iteration 3000 / 5000: loss 2.063378\n",
      "iteration 3100 / 5000: loss 2.109264\n",
      "iteration 3200 / 5000: loss 2.116180\n",
      "iteration 3300 / 5000: loss 2.094710\n",
      "iteration 3400 / 5000: loss 2.123673\n",
      "iteration 3500 / 5000: loss 2.129898\n",
      "iteration 3600 / 5000: loss 2.178670\n",
      "iteration 3700 / 5000: loss 2.108237\n",
      "iteration 3800 / 5000: loss 2.050110\n",
      "iteration 3900 / 5000: loss 2.138099\n",
      "iteration 4000 / 5000: loss 2.087215\n",
      "iteration 4100 / 5000: loss 2.111717\n",
      "iteration 4200 / 5000: loss 2.114481\n",
      "iteration 4300 / 5000: loss 2.155576\n",
      "iteration 4400 / 5000: loss 2.102117\n",
      "iteration 4500 / 5000: loss 2.089039\n",
      "iteration 4600 / 5000: loss 2.075966\n",
      "iteration 4700 / 5000: loss 2.128810\n",
      "iteration 4800 / 5000: loss 2.105746\n",
      "iteration 4900 / 5000: loss 2.061542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 41/50 [24:19<05:20, 35.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1284.711540\n",
      "iteration 100 / 5000: loss 27.937427\n",
      "iteration 200 / 5000: loss 2.685417\n",
      "iteration 300 / 5000: loss 2.149761\n",
      "iteration 400 / 5000: loss 2.141140\n",
      "iteration 500 / 5000: loss 2.084161\n",
      "iteration 600 / 5000: loss 2.156012\n",
      "iteration 700 / 5000: loss 2.103675\n",
      "iteration 800 / 5000: loss 2.114448\n",
      "iteration 900 / 5000: loss 2.130384\n",
      "iteration 1000 / 5000: loss 2.155021\n",
      "iteration 1100 / 5000: loss 2.153104\n",
      "iteration 1200 / 5000: loss 2.131061\n",
      "iteration 1300 / 5000: loss 2.128165\n",
      "iteration 1400 / 5000: loss 2.160634\n",
      "iteration 1500 / 5000: loss 2.115917\n",
      "iteration 1600 / 5000: loss 2.139989\n",
      "iteration 1700 / 5000: loss 2.102861\n",
      "iteration 1800 / 5000: loss 2.090427\n",
      "iteration 1900 / 5000: loss 2.121517\n",
      "iteration 2000 / 5000: loss 2.149738\n",
      "iteration 2100 / 5000: loss 2.124190\n",
      "iteration 2200 / 5000: loss 2.086645\n",
      "iteration 2300 / 5000: loss 2.110596\n",
      "iteration 2400 / 5000: loss 2.159744\n",
      "iteration 2500 / 5000: loss 2.125901\n",
      "iteration 2600 / 5000: loss 2.125766\n",
      "iteration 2700 / 5000: loss 2.098727\n",
      "iteration 2800 / 5000: loss 2.115628\n",
      "iteration 2900 / 5000: loss 2.130562\n",
      "iteration 3000 / 5000: loss 2.167963\n",
      "iteration 3100 / 5000: loss 2.135533\n",
      "iteration 3200 / 5000: loss 2.188172\n",
      "iteration 3300 / 5000: loss 2.099976\n",
      "iteration 3400 / 5000: loss 2.130817\n",
      "iteration 3500 / 5000: loss 2.098463\n",
      "iteration 3600 / 5000: loss 2.060092\n",
      "iteration 3700 / 5000: loss 2.178480\n",
      "iteration 3800 / 5000: loss 2.119176\n",
      "iteration 3900 / 5000: loss 2.160753\n",
      "iteration 4000 / 5000: loss 2.093484\n",
      "iteration 4100 / 5000: loss 2.154594\n",
      "iteration 4200 / 5000: loss 2.175320\n",
      "iteration 4300 / 5000: loss 2.096507\n",
      "iteration 4400 / 5000: loss 2.129687\n",
      "iteration 4500 / 5000: loss 2.168309\n",
      "iteration 4600 / 5000: loss 2.103162\n",
      "iteration 4700 / 5000: loss 2.134574\n",
      "iteration 4800 / 5000: loss 2.145937\n",
      "iteration 4900 / 5000: loss 2.134766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▍ | 42/50 [24:59<04:45, 35.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 622.457108\n",
      "iteration 100 / 5000: loss 36.707938\n",
      "iteration 200 / 5000: loss 4.014092\n",
      "iteration 300 / 5000: loss 2.143097\n",
      "iteration 400 / 5000: loss 2.067073\n",
      "iteration 500 / 5000: loss 2.088116\n",
      "iteration 600 / 5000: loss 2.083704\n",
      "iteration 700 / 5000: loss 2.085629\n",
      "iteration 800 / 5000: loss 2.095746\n",
      "iteration 900 / 5000: loss 2.012435\n",
      "iteration 1000 / 5000: loss 1.974407\n",
      "iteration 1100 / 5000: loss 2.133897\n",
      "iteration 1200 / 5000: loss 2.100872\n",
      "iteration 1300 / 5000: loss 2.145598\n",
      "iteration 1400 / 5000: loss 2.074988\n",
      "iteration 1500 / 5000: loss 2.044133\n",
      "iteration 1600 / 5000: loss 1.964739\n",
      "iteration 1700 / 5000: loss 2.065171\n",
      "iteration 1800 / 5000: loss 2.021461\n",
      "iteration 1900 / 5000: loss 2.088790\n",
      "iteration 2000 / 5000: loss 2.031586\n",
      "iteration 2100 / 5000: loss 2.118950\n",
      "iteration 2200 / 5000: loss 2.054907\n",
      "iteration 2300 / 5000: loss 2.037561\n",
      "iteration 2400 / 5000: loss 2.046366\n",
      "iteration 2500 / 5000: loss 2.129166\n",
      "iteration 2600 / 5000: loss 2.059177\n",
      "iteration 2700 / 5000: loss 2.056561\n",
      "iteration 2800 / 5000: loss 2.107801\n",
      "iteration 2900 / 5000: loss 2.060475\n",
      "iteration 3000 / 5000: loss 2.016473\n",
      "iteration 3100 / 5000: loss 2.076366\n",
      "iteration 3200 / 5000: loss 2.101061\n",
      "iteration 3300 / 5000: loss 2.085694\n",
      "iteration 3400 / 5000: loss 2.113355\n",
      "iteration 3500 / 5000: loss 2.056740\n",
      "iteration 3600 / 5000: loss 2.073072\n",
      "iteration 3700 / 5000: loss 2.065217\n",
      "iteration 3800 / 5000: loss 2.060840\n",
      "iteration 3900 / 5000: loss 2.054222\n",
      "iteration 4000 / 5000: loss 2.020504\n",
      "iteration 4100 / 5000: loss 2.062284\n",
      "iteration 4200 / 5000: loss 2.068153\n",
      "iteration 4300 / 5000: loss 2.026804\n",
      "iteration 4400 / 5000: loss 2.139584\n",
      "iteration 4500 / 5000: loss 2.027240\n",
      "iteration 4600 / 5000: loss 2.101140\n",
      "iteration 4700 / 5000: loss 2.096768\n",
      "iteration 4800 / 5000: loss 2.085534\n",
      "iteration 4900 / 5000: loss 2.025360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 86%|████████▌ | 43/50 [25:33<04:09, 35.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1524.782589\n",
      "iteration 100 / 5000: loss 2.229247\n",
      "iteration 200 / 5000: loss 2.165942\n",
      "iteration 300 / 5000: loss 2.144408\n",
      "iteration 400 / 5000: loss 2.159497\n",
      "iteration 500 / 5000: loss 2.145042\n",
      "iteration 600 / 5000: loss 2.139867\n",
      "iteration 700 / 5000: loss 2.127073\n",
      "iteration 800 / 5000: loss 2.134668\n",
      "iteration 900 / 5000: loss 2.115768\n",
      "iteration 1000 / 5000: loss 2.193315\n",
      "iteration 1100 / 5000: loss 2.132877\n",
      "iteration 1200 / 5000: loss 2.104950\n",
      "iteration 1300 / 5000: loss 2.162765\n",
      "iteration 1400 / 5000: loss 2.155333\n",
      "iteration 1500 / 5000: loss 2.146470\n",
      "iteration 1600 / 5000: loss 2.167628\n",
      "iteration 1700 / 5000: loss 2.165579\n",
      "iteration 1800 / 5000: loss 2.146996\n",
      "iteration 1900 / 5000: loss 2.160932\n",
      "iteration 2000 / 5000: loss 2.193370\n",
      "iteration 2100 / 5000: loss 2.137544\n",
      "iteration 2200 / 5000: loss 2.099399\n",
      "iteration 2300 / 5000: loss 2.143291\n",
      "iteration 2400 / 5000: loss 2.127295\n",
      "iteration 2500 / 5000: loss 2.089410\n",
      "iteration 2600 / 5000: loss 2.153748\n",
      "iteration 2700 / 5000: loss 2.160116\n",
      "iteration 2800 / 5000: loss 2.136110\n",
      "iteration 2900 / 5000: loss 2.149902\n",
      "iteration 3000 / 5000: loss 2.185687\n",
      "iteration 3100 / 5000: loss 2.133503\n",
      "iteration 3200 / 5000: loss 2.135763\n",
      "iteration 3300 / 5000: loss 2.149472\n",
      "iteration 3400 / 5000: loss 2.156349\n",
      "iteration 3500 / 5000: loss 2.081316\n",
      "iteration 3600 / 5000: loss 2.159032\n",
      "iteration 3700 / 5000: loss 2.136957\n",
      "iteration 3800 / 5000: loss 2.118671\n",
      "iteration 3900 / 5000: loss 2.135328\n",
      "iteration 4000 / 5000: loss 2.128219\n",
      "iteration 4100 / 5000: loss 2.162259\n",
      "iteration 4200 / 5000: loss 2.099611\n",
      "iteration 4300 / 5000: loss 2.156741\n",
      "iteration 4400 / 5000: loss 2.124617\n",
      "iteration 4500 / 5000: loss 2.149138\n",
      "iteration 4600 / 5000: loss 2.116072\n",
      "iteration 4700 / 5000: loss 2.166871\n",
      "iteration 4800 / 5000: loss 2.167183\n",
      "iteration 4900 / 5000: loss 2.135476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 44/50 [26:10<03:34, 35.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1004.111936\n",
      "iteration 100 / 5000: loss 17.298215\n",
      "iteration 200 / 5000: loss 2.320743\n",
      "iteration 300 / 5000: loss 2.165595\n",
      "iteration 400 / 5000: loss 2.152756\n",
      "iteration 500 / 5000: loss 2.067953\n",
      "iteration 600 / 5000: loss 2.118647\n",
      "iteration 700 / 5000: loss 2.143702\n",
      "iteration 800 / 5000: loss 2.054989\n",
      "iteration 900 / 5000: loss 2.141532\n",
      "iteration 1000 / 5000: loss 2.105112\n",
      "iteration 1100 / 5000: loss 2.108779\n",
      "iteration 1200 / 5000: loss 2.126539\n",
      "iteration 1300 / 5000: loss 2.117691\n",
      "iteration 1400 / 5000: loss 2.125509\n",
      "iteration 1500 / 5000: loss 2.089611\n",
      "iteration 1600 / 5000: loss 2.093953\n",
      "iteration 1700 / 5000: loss 2.059681\n",
      "iteration 1800 / 5000: loss 2.072739\n",
      "iteration 1900 / 5000: loss 2.156328\n",
      "iteration 2000 / 5000: loss 2.049045\n",
      "iteration 2100 / 5000: loss 2.107864\n",
      "iteration 2200 / 5000: loss 2.091921\n",
      "iteration 2300 / 5000: loss 2.094735\n",
      "iteration 2400 / 5000: loss 2.150511\n",
      "iteration 2500 / 5000: loss 2.124949\n",
      "iteration 2600 / 5000: loss 2.135019\n",
      "iteration 2700 / 5000: loss 2.121509\n",
      "iteration 2800 / 5000: loss 2.165498\n",
      "iteration 2900 / 5000: loss 2.090235\n",
      "iteration 3000 / 5000: loss 2.126845\n",
      "iteration 3100 / 5000: loss 2.091885\n",
      "iteration 3200 / 5000: loss 2.108555\n",
      "iteration 3300 / 5000: loss 2.153796\n",
      "iteration 3400 / 5000: loss 2.177255\n",
      "iteration 3500 / 5000: loss 2.166750\n",
      "iteration 3600 / 5000: loss 2.126535\n",
      "iteration 3700 / 5000: loss 2.110223\n",
      "iteration 3800 / 5000: loss 2.126409\n",
      "iteration 3900 / 5000: loss 2.098981\n",
      "iteration 4000 / 5000: loss 2.093222\n",
      "iteration 4100 / 5000: loss 2.166192\n",
      "iteration 4200 / 5000: loss 2.096015\n",
      "iteration 4300 / 5000: loss 2.115344\n",
      "iteration 4400 / 5000: loss 2.064213\n",
      "iteration 4500 / 5000: loss 2.076530\n",
      "iteration 4600 / 5000: loss 2.119441\n",
      "iteration 4700 / 5000: loss 2.075431\n",
      "iteration 4800 / 5000: loss 2.128670\n",
      "iteration 4900 / 5000: loss 2.130644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 45/50 [26:50<02:58, 35.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 946.666032\n",
      "iteration 100 / 5000: loss 163.946688\n",
      "iteration 200 / 5000: loss 29.938305\n",
      "iteration 300 / 5000: loss 6.883089\n",
      "iteration 400 / 5000: loss 2.912624\n",
      "iteration 500 / 5000: loss 2.272153\n",
      "iteration 600 / 5000: loss 2.111290\n",
      "iteration 700 / 5000: loss 2.109469\n",
      "iteration 800 / 5000: loss 2.157149\n",
      "iteration 900 / 5000: loss 2.080507\n",
      "iteration 1000 / 5000: loss 2.077237\n",
      "iteration 1100 / 5000: loss 2.202333\n",
      "iteration 1200 / 5000: loss 2.077259\n",
      "iteration 1300 / 5000: loss 2.127354\n",
      "iteration 1400 / 5000: loss 2.065351\n",
      "iteration 1500 / 5000: loss 2.146043\n",
      "iteration 1600 / 5000: loss 2.050849\n",
      "iteration 1700 / 5000: loss 2.057208\n",
      "iteration 1800 / 5000: loss 2.122097\n",
      "iteration 1900 / 5000: loss 2.081077\n",
      "iteration 2000 / 5000: loss 2.139418\n",
      "iteration 2100 / 5000: loss 2.118201\n",
      "iteration 2200 / 5000: loss 2.129320\n",
      "iteration 2300 / 5000: loss 2.066147\n",
      "iteration 2400 / 5000: loss 2.087590\n",
      "iteration 2500 / 5000: loss 2.130093\n",
      "iteration 2600 / 5000: loss 2.076695\n",
      "iteration 2700 / 5000: loss 2.108932\n",
      "iteration 2800 / 5000: loss 2.087465\n",
      "iteration 2900 / 5000: loss 2.108837\n",
      "iteration 3000 / 5000: loss 2.076779\n",
      "iteration 3100 / 5000: loss 2.091603\n",
      "iteration 3200 / 5000: loss 2.037790\n",
      "iteration 3300 / 5000: loss 2.142410\n",
      "iteration 3400 / 5000: loss 2.129419\n",
      "iteration 3500 / 5000: loss 2.095124\n",
      "iteration 3600 / 5000: loss 2.120342\n",
      "iteration 3700 / 5000: loss 2.061829\n",
      "iteration 3800 / 5000: loss 2.125625\n",
      "iteration 3900 / 5000: loss 2.119951\n",
      "iteration 4000 / 5000: loss 2.064771\n",
      "iteration 4100 / 5000: loss 2.118784\n",
      "iteration 4200 / 5000: loss 2.106879\n",
      "iteration 4300 / 5000: loss 2.077707\n",
      "iteration 4400 / 5000: loss 2.145965\n",
      "iteration 4500 / 5000: loss 2.124670\n",
      "iteration 4600 / 5000: loss 2.054495\n",
      "iteration 4700 / 5000: loss 2.056076\n",
      "iteration 4800 / 5000: loss 2.118304\n",
      "iteration 4900 / 5000: loss 2.110062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 92%|█████████▏| 46/50 [27:27<02:23, 35.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 1453.738676\n",
      "iteration 100 / 5000: loss 2.106632\n",
      "iteration 200 / 5000: loss 2.145118\n",
      "iteration 300 / 5000: loss 2.167919\n",
      "iteration 400 / 5000: loss 2.125356\n",
      "iteration 500 / 5000: loss 2.082503\n",
      "iteration 600 / 5000: loss 2.190256\n",
      "iteration 700 / 5000: loss 2.128886\n",
      "iteration 800 / 5000: loss 2.152118\n",
      "iteration 900 / 5000: loss 2.156944\n",
      "iteration 1000 / 5000: loss 2.162399\n",
      "iteration 1100 / 5000: loss 2.156993\n",
      "iteration 1200 / 5000: loss 2.138711\n",
      "iteration 1300 / 5000: loss 2.091106\n",
      "iteration 1400 / 5000: loss 2.115909\n",
      "iteration 1500 / 5000: loss 2.199814\n",
      "iteration 1600 / 5000: loss 2.140546\n",
      "iteration 1700 / 5000: loss 2.196260\n",
      "iteration 1800 / 5000: loss 2.120308\n",
      "iteration 1900 / 5000: loss 2.078308\n",
      "iteration 2000 / 5000: loss 2.109719\n",
      "iteration 2100 / 5000: loss 2.167216\n",
      "iteration 2200 / 5000: loss 2.152319\n",
      "iteration 2300 / 5000: loss 2.140771\n",
      "iteration 2400 / 5000: loss 2.120174\n",
      "iteration 2500 / 5000: loss 2.096179\n",
      "iteration 2600 / 5000: loss 2.152586\n",
      "iteration 2700 / 5000: loss 2.178236\n",
      "iteration 2800 / 5000: loss 2.140021\n",
      "iteration 2900 / 5000: loss 2.136885\n",
      "iteration 3000 / 5000: loss 2.166529\n",
      "iteration 3100 / 5000: loss 2.174288\n",
      "iteration 3200 / 5000: loss 2.167358\n",
      "iteration 3300 / 5000: loss 2.168763\n",
      "iteration 3400 / 5000: loss 2.121942\n",
      "iteration 3500 / 5000: loss 2.148387\n",
      "iteration 3600 / 5000: loss 2.128856\n",
      "iteration 3700 / 5000: loss 2.148073\n",
      "iteration 3800 / 5000: loss 2.154605\n",
      "iteration 3900 / 5000: loss 2.140475\n",
      "iteration 4000 / 5000: loss 2.179527\n",
      "iteration 4100 / 5000: loss 2.105376\n",
      "iteration 4200 / 5000: loss 2.170055\n",
      "iteration 4300 / 5000: loss 2.141570\n",
      "iteration 4400 / 5000: loss 2.162221\n",
      "iteration 4500 / 5000: loss 2.147523\n",
      "iteration 4600 / 5000: loss 2.186817\n",
      "iteration 4700 / 5000: loss 2.122500\n",
      "iteration 4800 / 5000: loss 2.130123\n",
      "iteration 4900 / 5000: loss 2.116768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 47/50 [28:02<01:47, 35.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 508.665569\n",
      "iteration 100 / 5000: loss 32.277402\n",
      "iteration 200 / 5000: loss 3.842601\n",
      "iteration 300 / 5000: loss 2.129260\n",
      "iteration 400 / 5000: loss 2.038312\n",
      "iteration 500 / 5000: loss 2.037397\n",
      "iteration 600 / 5000: loss 2.023713\n",
      "iteration 700 / 5000: loss 2.060207\n",
      "iteration 800 / 5000: loss 2.052396\n",
      "iteration 900 / 5000: loss 2.075234\n",
      "iteration 1000 / 5000: loss 2.059329\n",
      "iteration 1100 / 5000: loss 2.066919\n",
      "iteration 1200 / 5000: loss 1.996907\n",
      "iteration 1300 / 5000: loss 2.067938\n",
      "iteration 1400 / 5000: loss 2.097240\n",
      "iteration 1500 / 5000: loss 2.035222\n",
      "iteration 1600 / 5000: loss 2.045871\n",
      "iteration 1700 / 5000: loss 2.029668\n",
      "iteration 1800 / 5000: loss 2.022442\n",
      "iteration 1900 / 5000: loss 2.030632\n",
      "iteration 2000 / 5000: loss 2.037480\n",
      "iteration 2100 / 5000: loss 2.099662\n",
      "iteration 2200 / 5000: loss 2.022084\n",
      "iteration 2300 / 5000: loss 2.029118\n",
      "iteration 2400 / 5000: loss 2.036430\n",
      "iteration 2500 / 5000: loss 1.965244\n",
      "iteration 2600 / 5000: loss 2.116619\n",
      "iteration 2700 / 5000: loss 2.056182\n",
      "iteration 2800 / 5000: loss 1.992414\n",
      "iteration 2900 / 5000: loss 2.035752\n",
      "iteration 3000 / 5000: loss 2.071472\n",
      "iteration 3100 / 5000: loss 2.036487\n",
      "iteration 3200 / 5000: loss 2.016171\n",
      "iteration 3300 / 5000: loss 2.095248\n",
      "iteration 3400 / 5000: loss 2.034594\n",
      "iteration 3500 / 5000: loss 2.022771\n",
      "iteration 3600 / 5000: loss 2.095183\n",
      "iteration 3700 / 5000: loss 2.059262\n",
      "iteration 3800 / 5000: loss 2.055705\n",
      "iteration 3900 / 5000: loss 2.038665\n",
      "iteration 4000 / 5000: loss 2.033168\n",
      "iteration 4100 / 5000: loss 2.076227\n",
      "iteration 4200 / 5000: loss 2.041536\n",
      "iteration 4300 / 5000: loss 2.076666\n",
      "iteration 4400 / 5000: loss 1.984613\n",
      "iteration 4500 / 5000: loss 2.045760\n",
      "iteration 4600 / 5000: loss 2.037443\n",
      "iteration 4700 / 5000: loss 2.021616\n",
      "iteration 4800 / 5000: loss 2.031246\n",
      "iteration 4900 / 5000: loss 2.080547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 48/50 [28:38<01:11, 35.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 585.102924\n",
      "iteration 100 / 5000: loss 27.713916\n",
      "iteration 200 / 5000: loss 3.193293\n",
      "iteration 300 / 5000: loss 2.105809\n",
      "iteration 400 / 5000: loss 2.076018\n",
      "iteration 500 / 5000: loss 1.989847\n",
      "iteration 600 / 5000: loss 2.082236\n",
      "iteration 700 / 5000: loss 2.011971\n",
      "iteration 800 / 5000: loss 2.085182\n",
      "iteration 900 / 5000: loss 2.059302\n",
      "iteration 1000 / 5000: loss 2.004727\n",
      "iteration 1100 / 5000: loss 2.151587\n",
      "iteration 1200 / 5000: loss 2.117485\n",
      "iteration 1300 / 5000: loss 2.009970\n",
      "iteration 1400 / 5000: loss 2.034121\n",
      "iteration 1500 / 5000: loss 1.965976\n",
      "iteration 1600 / 5000: loss 2.125996\n",
      "iteration 1700 / 5000: loss 2.063036\n",
      "iteration 1800 / 5000: loss 2.067848\n",
      "iteration 1900 / 5000: loss 2.086786\n",
      "iteration 2000 / 5000: loss 2.046268\n",
      "iteration 2100 / 5000: loss 2.068515\n",
      "iteration 2200 / 5000: loss 2.085775\n",
      "iteration 2300 / 5000: loss 1.970366\n",
      "iteration 2400 / 5000: loss 2.133016\n",
      "iteration 2500 / 5000: loss 2.098029\n",
      "iteration 2600 / 5000: loss 2.104584\n",
      "iteration 2700 / 5000: loss 2.121781\n",
      "iteration 2800 / 5000: loss 2.088784\n",
      "iteration 2900 / 5000: loss 2.070513\n",
      "iteration 3000 / 5000: loss 2.082500\n",
      "iteration 3100 / 5000: loss 1.987308\n",
      "iteration 3200 / 5000: loss 2.036703\n",
      "iteration 3300 / 5000: loss 2.066569\n",
      "iteration 3400 / 5000: loss 2.057222\n",
      "iteration 3500 / 5000: loss 2.025542\n",
      "iteration 3600 / 5000: loss 2.084652\n",
      "iteration 3700 / 5000: loss 2.003391\n",
      "iteration 3800 / 5000: loss 2.061064\n",
      "iteration 3900 / 5000: loss 2.032845\n",
      "iteration 4000 / 5000: loss 2.056530\n",
      "iteration 4100 / 5000: loss 2.096195\n",
      "iteration 4200 / 5000: loss 2.161649\n",
      "iteration 4300 / 5000: loss 2.058785\n",
      "iteration 4400 / 5000: loss 2.092393\n",
      "iteration 4500 / 5000: loss 2.086016\n",
      "iteration 4600 / 5000: loss 2.070153\n",
      "iteration 4700 / 5000: loss 2.080085\n",
      "iteration 4800 / 5000: loss 2.020520\n",
      "iteration 4900 / 5000: loss 2.084086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 98%|█████████▊| 49/50 [29:12<00:35, 35.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 792.332851\n",
      "iteration 100 / 5000: loss 11.753384\n",
      "iteration 200 / 5000: loss 2.262154\n",
      "iteration 300 / 5000: loss 2.080222\n",
      "iteration 400 / 5000: loss 2.086906\n",
      "iteration 500 / 5000: loss 2.132645\n",
      "iteration 600 / 5000: loss 2.103745\n",
      "iteration 700 / 5000: loss 2.030669\n",
      "iteration 800 / 5000: loss 2.112562\n",
      "iteration 900 / 5000: loss 2.094863\n",
      "iteration 1000 / 5000: loss 2.054539\n",
      "iteration 1100 / 5000: loss 2.054014\n",
      "iteration 1200 / 5000: loss 2.072840\n",
      "iteration 1300 / 5000: loss 1.985354\n",
      "iteration 1400 / 5000: loss 2.100439\n",
      "iteration 1500 / 5000: loss 2.109970\n",
      "iteration 1600 / 5000: loss 2.111011\n",
      "iteration 1700 / 5000: loss 2.049056\n",
      "iteration 1800 / 5000: loss 2.138803\n",
      "iteration 1900 / 5000: loss 2.044818\n",
      "iteration 2000 / 5000: loss 2.091654\n",
      "iteration 2100 / 5000: loss 2.127684\n",
      "iteration 2200 / 5000: loss 2.043034\n",
      "iteration 2300 / 5000: loss 2.138372\n",
      "iteration 2400 / 5000: loss 2.088105\n",
      "iteration 2500 / 5000: loss 2.127663\n",
      "iteration 2600 / 5000: loss 2.085877\n",
      "iteration 2700 / 5000: loss 2.113574\n",
      "iteration 2800 / 5000: loss 2.100532\n",
      "iteration 2900 / 5000: loss 2.159153\n",
      "iteration 3000 / 5000: loss 2.103690\n",
      "iteration 3100 / 5000: loss 2.065541\n",
      "iteration 3200 / 5000: loss 2.112897\n",
      "iteration 3300 / 5000: loss 2.069235\n",
      "iteration 3400 / 5000: loss 2.050038\n",
      "iteration 3500 / 5000: loss 2.084050\n",
      "iteration 3600 / 5000: loss 2.093707\n",
      "iteration 3700 / 5000: loss 2.036928\n",
      "iteration 3800 / 5000: loss 2.103395\n",
      "iteration 3900 / 5000: loss 2.080453\n",
      "iteration 4000 / 5000: loss 2.094189\n",
      "iteration 4100 / 5000: loss 2.113750\n",
      "iteration 4200 / 5000: loss 2.128864\n",
      "iteration 4300 / 5000: loss 2.078667\n",
      "iteration 4400 / 5000: loss 2.000315\n",
      "iteration 4500 / 5000: loss 2.089944\n",
      "iteration 4600 / 5000: loss 2.077382\n",
      "iteration 4700 / 5000: loss 2.116062\n",
      "iteration 4800 / 5000: loss 2.096295\n",
      "iteration 4900 / 5000: loss 2.087665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 50/50 [29:56<00:00, 35.92s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.145508e-07 reg 1.975925e+04 train accuracy: 0.338653 val accuracy: 0.348000\n",
      "lr 1.154908e-07 reg 2.019600e+04 train accuracy: 0.337184 val accuracy: 0.354000\n",
      "lr 1.249047e-07 reg 1.899970e+04 train accuracy: 0.340286 val accuracy: 0.353000\n",
      "lr 1.288046e-07 reg 2.366685e+04 train accuracy: 0.326347 val accuracy: 0.343000\n",
      "lr 1.304170e-07 reg 1.768900e+04 train accuracy: 0.343388 val accuracy: 0.357000\n",
      "lr 1.335615e-07 reg 4.627612e+04 train accuracy: 0.308551 val accuracy: 0.325000\n",
      "lr 1.389251e-07 reg 3.323714e+04 train accuracy: 0.326429 val accuracy: 0.341000\n",
      "lr 1.429744e-07 reg 3.060066e+04 train accuracy: 0.323551 val accuracy: 0.343000\n",
      "lr 1.488613e-07 reg 1.775280e+04 train accuracy: 0.343327 val accuracy: 0.354000\n",
      "lr 1.527558e-07 reg 3.866439e+04 train accuracy: 0.321306 val accuracy: 0.328000\n",
      "lr 1.537399e-07 reg 4.825049e+04 train accuracy: 0.302857 val accuracy: 0.324000\n",
      "lr 1.863552e-07 reg 2.907740e+04 train accuracy: 0.321408 val accuracy: 0.337000\n",
      "lr 1.912938e-07 reg 3.822370e+04 train accuracy: 0.313469 val accuracy: 0.327000\n",
      "lr 1.915288e-07 reg 2.270543e+04 train accuracy: 0.320918 val accuracy: 0.338000\n",
      "lr 1.917811e-07 reg 4.454830e+04 train accuracy: 0.303204 val accuracy: 0.318000\n",
      "lr 1.926743e-07 reg 2.335723e+04 train accuracy: 0.329327 val accuracy: 0.342000\n",
      "lr 2.132159e-07 reg 2.543064e+04 train accuracy: 0.326735 val accuracy: 0.340000\n",
      "lr 2.160989e-07 reg 4.776821e+04 train accuracy: 0.302429 val accuracy: 0.324000\n",
      "lr 2.326351e-07 reg 3.121602e+04 train accuracy: 0.312653 val accuracy: 0.333000\n",
      "lr 2.329274e-07 reg 4.140886e+04 train accuracy: 0.313878 val accuracy: 0.336000\n",
      "lr 2.418320e-07 reg 2.481433e+04 train accuracy: 0.329020 val accuracy: 0.342000\n",
      "lr 2.494901e-07 reg 2.824733e+04 train accuracy: 0.322837 val accuracy: 0.330000\n",
      "lr 2.589575e-07 reg 3.534716e+04 train accuracy: 0.312571 val accuracy: 0.327000\n",
      "lr 2.695461e-07 reg 3.458224e+04 train accuracy: 0.301367 val accuracy: 0.321000\n",
      "lr 2.788896e-07 reg 3.535163e+04 train accuracy: 0.320592 val accuracy: 0.323000\n",
      "lr 3.152577e-07 reg 4.309870e+04 train accuracy: 0.325796 val accuracy: 0.329000\n",
      "lr 3.160269e-07 reg 3.273954e+04 train accuracy: 0.324673 val accuracy: 0.341000\n",
      "lr 3.336786e-07 reg 2.421324e+04 train accuracy: 0.332061 val accuracy: 0.347000\n",
      "lr 3.452959e-07 reg 1.928280e+04 train accuracy: 0.337714 val accuracy: 0.350000\n",
      "lr 3.555864e-07 reg 2.008451e+04 train accuracy: 0.322204 val accuracy: 0.333000\n",
      "lr 3.712918e-07 reg 4.943892e+04 train accuracy: 0.295510 val accuracy: 0.302000\n",
      "lr 4.043186e-07 reg 1.908750e+04 train accuracy: 0.336653 val accuracy: 0.355000\n",
      "lr 4.193877e-07 reg 2.120204e+04 train accuracy: 0.334408 val accuracy: 0.349000\n",
      "lr 4.236808e-07 reg 1.640911e+04 train accuracy: 0.341327 val accuracy: 0.354000\n",
      "lr 4.261987e-07 reg 2.548099e+04 train accuracy: 0.327286 val accuracy: 0.325000\n",
      "lr 4.461531e-07 reg 2.600939e+04 train accuracy: 0.327204 val accuracy: 0.331000\n",
      "lr 4.681457e-07 reg 4.931328e+04 train accuracy: 0.307673 val accuracy: 0.320000\n",
      "lr 5.166115e-07 reg 4.326802e+04 train accuracy: 0.311388 val accuracy: 0.321000\n",
      "lr 5.424368e-07 reg 2.630505e+04 train accuracy: 0.334061 val accuracy: 0.340000\n",
      "lr 5.603047e-07 reg 1.944317e+04 train accuracy: 0.332327 val accuracy: 0.342000\n",
      "lr 5.791000e-07 reg 4.733602e+04 train accuracy: 0.303673 val accuracy: 0.312000\n",
      "lr 6.002435e-07 reg 3.227349e+04 train accuracy: 0.317306 val accuracy: 0.341000\n",
      "lr 6.527314e-07 reg 4.152699e+04 train accuracy: 0.316265 val accuracy: 0.332000\n",
      "lr 6.610549e-07 reg 3.137828e+04 train accuracy: 0.315367 val accuracy: 0.329000\n",
      "lr 7.016082e-07 reg 3.623771e+04 train accuracy: 0.315980 val accuracy: 0.325000\n",
      "lr 7.797581e-07 reg 3.114944e+04 train accuracy: 0.313347 val accuracy: 0.323000\n",
      "lr 8.292742e-07 reg 3.291624e+04 train accuracy: 0.302592 val accuracy: 0.323000\n",
      "lr 8.437310e-07 reg 4.012125e+04 train accuracy: 0.300673 val accuracy: 0.316000\n",
      "lr 8.876001e-07 reg 4.699575e+04 train accuracy: 0.284531 val accuracy: 0.278000\n",
      "lr 9.070794e-07 reg 2.427889e+04 train accuracy: 0.323408 val accuracy: 0.343000\n",
      "best validation accuracy achieved during cross-validation: 0.357000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [-7, -6]\n",
    "regularization_strengths = [4.2, 4.7]\n",
    "\n",
    "n_points = 50\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "for __ in tqdm(range(n_points)):\n",
    "    lr = 10**random.uniform(learning_rates[0], learning_rates[-1])\n",
    "    r = 10**random.uniform(regularization_strengths[0], regularization_strengths[-1])\n",
    "    softmax = Softmax()\n",
    "    softmax.train(X_train, y_train, learning_rate=lr, reg=r,\n",
    "                  num_iters=5000, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    acc_tr = np.mean(y_train == y_train_pred)\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    acc = np.mean(y_val == y_val_pred)\n",
    "    if acc > best_val:\n",
    "        best_val = acc\n",
    "        best_softmax = softmax\n",
    "    results[lr,r] = [acc_tr, acc]\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.360000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*:\n",
    "\n",
    "Yes\n",
    "\n",
    "*Your explanation*:\n",
    "\n",
    "A data point which is well classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAF8CAYAAAAAZIWVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzsvXm0bNtV3jfn7qrOufc1SAoYCUm2wWDTRdjBmAQDBgKmC4rAOBhDgEDiBBkTj5guShADATY2JibEHY0ZERaNZYLBZiQMAiFg7DimMRg8FINRi4wR6t5791TVblb+qPPO+s3S2vfe/W7Vue9xv98Yb7x96+zatZu1Vq2a3/rm9JSSCSGEEEKIu6e63ycghBBCCPFMQxMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUGbm7h/t7m+83+chhMi4+2vd/eMKr/9Rd3/NwmN9l7u/4nhnJ4Qwe7D7liZQQohnFCmln0opvd/9Pg9xvcxNqIW4X2gCJcQM7t7c73MQy9AzE+KZzzOlHz9QE6jLXzBf6e6/4u5vc/e/4+7rwn5f4e6/5u6PXe77n+Jvn+fuP+3uf+XyGL/u7p+Ivz/i7t/h7m929ze5+yvcvb6uaxQZd3++u/+Au/+Wu/+2u3+ru7+3u//45b/f4u5/190fxXte6+5f7u6/aGZPPFM68u9gPvSwvx5K7qVn5u4f4u4/d9mHv8/M3qWfi/vH0r7p7q80sxeY2Q+7++Pu/mX39woeXG7Xt9z9U9z9F9z97e7+M+7+wfjbc939718+81939y/B317u7q929+9293ea2edd60U9RR6oCdQln21mn2Bm721m72tmLyvs82tm9kfN7BEz+xoz+253f0/8/cPM7DVm9hwz+0Yz+w5398u/fZeZDWb2Pmb2IWb28Wb2hUe/CnFbLiet/9DMXmdmv9vMnmdm32tmbmbfYGbPNbM/YGbPN7OXH7z9s8zsk83s0ZTScD1nLGa4m/5qhmdm+3HtB83slWb2LDP7e2b26Sc/U3FXPJW+mVL6HDN7vZl9akrpZkrpG6/9xIW5e2czfcvdP8TMvtPM/isze7aZ/S0z+yF3X7l7ZWY/bGb/wvbP+2PN7Evd/RNw+E8zs1fbvg//3Wu5oHslpfTA/GdmrzWzP4N/f5LtJ0sfbWZvvM37fsHMPu1y+/PM7Ffxt3MzS2b2u8zsPcxsa2Zn+PtnmdlP3O9rf9D+M7MPN7PfMrPmDvu92Mx+/qCNfMH9Pn/9d/f99fCZmdlHmtlvmJnjtZ8xs1fc72vSf/fcNz/ufp//g/zf7fqWmf0NM/vag/1fY2YfZfugw+sP/vaVZvZ3Lrdfbmb/9/2+vqX/PYjyxBuw/Trb/9oJuPvnmtmft/2vIzOzm7aPNj3Jv31yI6V06zL4dNP2M/LWzN6cA1JWHXymuB6eb2avSwcRJHd/DzP7a7aPMD5k++fztoP36nk9fbhjfy3s91wze1O6HJnxXvH04F76pri/3K5vvdDM/nN3/7P4W3f5ntHMnuvub8ffajP7Kfz7GTfuPogS3vOx/QLbz6avcPcXmtm3mdlLzezZKaVHzexf2j68fCfeYPsI1HNSSo9e/vdwSukDjnPqYgFvMLMXFNYwfb3tI4YflFJ62Mz+tL3rs00mni7ctr8CPrM3m9nzIKs/+V7x9OCp9k31y/vP7frWG8zs6/Dd92hK6Tyl9D2Xf/v1g789lFL6JBznGfd8H8QJ1Be7+3u5+7PM7L83s+87+PsN2z/I3zIzc/fPN7MPvJsDp5TebGY/ambf5O4Pu3t1uTDyo453+uIu+We27+x/0d1vXC4+/o9s/8v2cTN7h7s/z8z+wv08SXFH7tRfS/wT269D/BJ3b939JWb2h095kmIRT7Vv/qaZ/d7rPVVxwO361reZ2Z9x9w/zPTfc/ZPd/SHbP/PHLs0eZ+5eu/sHuvuH3qfrOAoP4gTqVbaf5Pwb26+nCAnAUkq/YmbfZPuG8ptm9kFm9o8XHP9zbR+2/BXbh59fbWbvedt3iKOTUhrN7FNtv5j/9Wb2RjP7k7Y3BfxBM3uHmf0jM/uB+3WO4q64bX8tkVLamdlLbL9e8a22f+56zk8T7qFvfoOZvezS4fXfXd8Ziye5Xd9KKf1zM/siM/tW23/3/erlfk8+808xsxeZ2a+b2VvM7Nttb9R6xuJRyvydjbu/1sy+MKX0Y/f7XIQQQgjxzOVBjEAJIYQQQtwTmkAJIYQQQizkgZLwhBBCCCGOgSJQQgghhBALudZEmp/zZT96Fe5K03T1ekL6h6rOc7qqyiXk9pngn/xH3uRxJkTTppnIWoX0FSGVBY/J44wjzjPiFeef+QD86Ar7OI7gM9lN+NlVhXONOxXPiYfkdfIcvv0VH3s3+azuyN9+2VdefTTvo+OZ8T5wHz5vXtc0jdjm+WO7zsevm/Zqu2lyUx6GfJzdjrn60FamPl5Q/pPVdT5Wg8/jfeS5jngv2yyfzcQ2jtfTlF/vh3xOvHcpNJa8/dKv/9qjPEszsy//mo+8+sR21eVP4zWP+aSGId9Xnt40sW2yP+ImgdCnsIuz7bOfzfStw9454gaOeEBx3OHbecPLY5Mbnz/2wf5Nm9tOhTEr/FLl+IL79c2v+JmjPM9XfManXB20rjn+cIzCc8L5z50zHocNY372oY+H/fO7eQ95/w/ekTcPBlr2f/ZHn+mPU2gj+b0t3ssP4b1w7MNnw+fN1/sp34vtLvffl//Q/3G0vvmpn/0huW92eczrWvRT3O8G+8RbWe5TbcsxLu8T+g2OMuCa+XrblcfNmDLKbBw5zmN7LG+nVL6VQ78rn2voynzO+ZiOffhsR47BeO8/fNUvFU9CESghhBBCiIVoAiWEEEIIsZDrrYVXBZ2suO3Obb6ZoV+EpRkqZCga4cq7ke08nBs+FSHGGH42c8qNCKGOE6+hLOGFEHcqh4pjzLAsQ3Cf8F6EaKsQuj4ODMM2dQ4ZB4mF97oqy1Btg5D5OCPzsH1AImMDmQuxE763Oghu9wwfD5ShIM+E8yhLjEGqxDk1aCsMb+92OQxNiYXHGdGuhyHvf1RwbyhhVtgecC8cjypKdXOSN+59TZmTbQS74/rrauZ3npclGTOzihIbJIfJ8/lFSbbcvypec5Cl+DrOFe2Z512HdgH5F+3uWJyf3yieQxi/eD5cNkDpH/e0wSPYbnMbHAbIH5D26rosCwUJNXlx+3DkqynhtXms4fACRdlq5zbbdfmcOO4E6RjNa8BzGmakLVsdf5w1i7JdC4mY217l7RqvDzNLUIIs7uyPqbhP+H7E+BjGYOM2djl4niks55h5f/jOLn+XNXzOU1nCo7Q3jewLucFQ2m1wzXfTNxWBEkIIIYRYiCZQQgghhBALuVYJLzjpQli37J5jKHYKzrOyRJPogAoRR4Qlg3TIc2PIkMdkqDOGIkOIk3Ie45fR6lPc36aptEu4zjpcG6UrOhoo7TG+aUdnu9nmz4WCdxCtvSLcx5l7SgcbH+BAGRVR1RSk1hySHXo65MpOsObgp0OiLIzXnTID3XZevgb3chib9HAJ9n3Z0USbSAoSHl2Fx6Pr1lfbVUMXD58b2i/i55TLvaZ7Bh8QJDk2yBmnrZX7KUlBBo9/q9AJ62ZG/sZ5UH6iE4dPkY620OSDnEtZH9JTWCKA59kfuEGPQNOsrrY7OrKC2w4SMdoUpWaOlWnM59nC8eqUMnEOQV4Kn0VZhK6wvP+h6yrNLKPgbnz+zYwc3TSU/2fey32Cqzu/3FLbw3n7YSM8Eg0Gq7outy9+F7F/0Tka3LxVWf4O3zloO9XM+D3ZjPw5c277v/FQ/A7ldKS8BIfPZHRec3k5B78L2P4n9nGOCTiFvpeEJ4QQQghxdDSBEkIIIYRYyPW68Kgq0RFB2SOVJZfgoGKys3D8qrwdHDaUdxCe50p/hhXnHHwWw4Z8Tz2T+IsuwZh8sLw/zzW4RpgQbww6Sflcj5bSLXOx2VxtjyOlnbKjY8RJMAEmZZgW4Wle1Zjy6zSAQLWz3UCZC86gGedRkFQsSp6UKAY8S0ccv5uTmyC98b7UM20wzSZ55HGQGHR7Ghde2zF5JqQttuWggNAZQ/ka5x2cMZAJeO94f70s+0QXT9m5ddjlOHa0NY/FQ0EmhkRVUZYJysWdE1FG6Sq/l+OLh6Six++cVUXXVpZmw/DIcRaya5Dk0E655KJB/x0gQW+3WdanGzEsdcD1sn8EmXqMUtiIzwjnFx9mPlW0zRbyF6XmcNvpwoKMxO+iJjg+0Z449p2oKlrbUkorXwM/mksh+B3lXLdAuWxmmQP7Nd3xwXWcyvIindAHQ63V4Xow1tC9Hp5nhnIz92nQx+NSFiZGhZOS7d+YFPjO3+NEESghhBBCiIVoAiWEEEIIsZDrdeHN1GKKSd0oueTNqi6H60OiSroPajpsuBK/7DIJLoaZMPxhQrBQoy28Xq4nRYmmnkkuNzDx3cjQN98LmSyEPRF+5XFOIBOw3lxd43MT5VUmF83vTSEpGzancpug24KSWo+DDlN+ZiPCtkz+SEdZHbRfsyAB9Hl7vUKCOqrCXn42vDTKXzuEnhOddGyblPZgN6Tzjq69o+IzSflCjbKyVBlC95QbkfSTsrOHw5QT5lHO47NhPT7u867R9rL8H529Zbssx5rghuR7w/HLTiJeZwpuU8jZwcJ6HLrVeT5+cFTierk8gNJOWGbARKBlCZr3jYbCKDVDLsG4we4e6ssdJNSlbBMSg858b3DsawY6BtFmMUa0TZavQ01Nyv9oE0F6nMquwmPC2pQcgyijN91Z3sb1xKUfmSkkeaXMBadacKCWz21kOwrfy3Q8H7jwQm3T8lKIkFQYHa/p8L0eXHs411BPFvU70UDnanMGl+NdJKBWBEoIIYQQYiGaQAkhhBBCLOSaE2nmbUpPQQqDfBZy+HGqxxX3Xt6JiSoTw54zTj2G60JSzLn6fXZQN4rRW4YQg1ViKm6H8PNMsre5hI5dB5lsxqFwmMjsGNDdwFAyJZaR9wv18iY8kN1EXSxv0oU2IBw8UBZMdfH1LcLKfTDhMWx9eE/KTq1+yP/qmEiS9bPo1sE+BjegMYEg2n7i60jmyIfMsPo4Uy/wXpkSJSaGrsuSd5Qq8azYTllTjzIBXVasW0V1En2cz2qaqe3FGmlmZnU7k3wQj33sKTOVJb8EHW6yslzD4zctE0KW20j1Lm3vuDRtTqRJt/PAGxwcxHhzGLt4/khUu8U2Xc147w73lolJ+x3qEqIpj8HVGK9nLqFwNVN3MDROOjDpKmvKxwm1VYNzkq5xfBSswI2fRsILSS95PRzPGn73WXEfnl01c65hSciMyxy3IvS7hPUYY08pOH4Gk7WG5845AetlcqkNXm9DnUeOCzxv1pnl+A3XLZ5/qLV4F5KsIlBCCCGEEAvRBEoIIYQQYiHXKuHVwUnHUKkVtzm9Y42iqkOyM8pHjF1CMqpCHS7KdjxOJs053g5CeqGmDx06IxPT4XUkzEyQd+hKa4KTEM6PXQ45MqTN82Y4NOSJO0GNJsoQoUZcCKUzmV52hvA5DYgH97g/g5W3JzivBjTfDWShDdyLPD5dfocSXhPqouV73Xh+li1C1DXknBbNrqP0hLAyJb+G8k9wwOTP9aks1aUT/eZhX3D0naDvTOVwO2WCKaGdJsrxZTdUqOU4Uj5CMsSQ5JMuPybLzPvs30NJj7IyZDiHuwkSSEIdPUp4aWLCvXyYpqFMRPcQpHme3Iy0dyzYpkJCwmCTopO3nMyVg1qoXxgSH0NeTmNxews5r8c4NoSkq2VH9P680Z/p+kLfXp/lhKF0j43YbtgG8X3CZSAhIWVNOQfXj3bKOnXjiWrhhWUqHAuZzBbPZEXpnNfAOnJ4zhzLWeMytn1KrLTRUh5HnwsJgmNNuYZJtI3fr+Xac5Tt+HkcO2I7Dxlj8VZKnnTal2v+3Y3UrgiUEEIIIcRCNIESQgghhFjItUp4DDOmGflrHMtOMi79Z2ictY5GBGBTxe1y3a4QtsfnBskvhGjj9QTjHT67makzlHZwCvD6g9uBMhzey+R+oVAcEjQyLBtqB57AhRdCt/l6WyR0YwLMhH2ClBkk2Ly9QQh4C6fejiY3PCfKdhcjXocbhDko64MCTTVDvXg4HUL9a8ozvNebLEucYf8VHxncR3Tz3VyVE1jy2Qd5eCYB3D2DNtt0kEOCiRTOQCphfP6Qxbxn+ByheqqCY66fVtV0W8HNhucfzbhlZ6qZ2RDsl3mTdRWDW4cuJmqyHEd6nCvrgeG8+zHLVUzKWEX7X96ckWrvhVDLj4Y07EN5ZmDS1qksO/IaZ5cNMB8n96Ecz2fGe043lkVYwy3IKjjWlolqcYbh7rIj4dk0LccyOjDLUtNI56xzXCtLUPfKalWWJ4NzGAlT61V2YfK7iH2kxj9iPyo/B95TjqOsWclapn5WdsiZxb5NaZjfKbGGableHr9z6RiM38uZZsbhP/E4ITnrnSVZRaCEEEIIIRaiCZQQQgghxEKuOZEmQ3R04iD8miilQd4ZKPvkkGBHKQxh2QT31BjscmXn3RSL3uGzWEsthvSYpG6i64DSAGU4JISkqYUOMETKQz001lsLIVFKj7gvDON37fHrbTnCqm2bZZu2y+FjSrAXPd1T+TgDnvEtuKouIKNtId/eGvN1XUDa2UyU8+BeNLr/8PoQpZMVZIImaBd5k2aQ2vI9HcfN1XY1lJ1ajvYbkrvh0awbhN5DolI61U5TC6+BBBBqQFHOxstMpudGqYoJXCET4X43lMgo5+4oR+ddBrQdJmUkOzwDM7ORsj3afxgjQl21vL1eZ8kk1MKjUxGvj/1Ffj1IQOW6bez804zz6F6om3z+dA7aTD2+kKcRx2k7SmeQcHrKsTPbXMYQTJ3lthXq5R1IJ3M1PqcwVuZtdH/bYaxsKTcyISsKY9Zsy7xfM/lxmYB2OpVDtqPkVV7KUc3Ul6ub8ndUA1c0V4ewxqmHzNd0yKLtMyGlzTyQAyWMLvU6JKMuy4dTSLYZMiNju1ynk99BwVEfxq+Z874LdV0RKCGEEEKIhWgCJYQQQgixkGuV8KZ38VfsoSRHh12ojcZQNGK0A8s7hXxbrIWFfWiQCwk8y8nagivwsDROCOsigSBClKzXE3JM0kHB6D5rkkFWSbA+UMYZh+z6CckNKcmcYJpMWbCtuZ1lqAEh42GAU2lC8ky4orbbvP8OEuFmzNtb6AEbNN8tHuYOx2QizSk+8HA9DjcN5eUKUnANG2abKG0xKWF+HtOUZaUO96VFMsgax2FnZDg8yHYncG2ZRdk59FMm5UPnaZj0FM92YHg/uGLLCVbZxncb9iHIKqwLCIlhgOQ3Jta1MzPWg8O51kw+iGSbIZEuE/LO1eDEUoMxJDQsS0AhAWSo+Xj8+mlVk+WZHk5IuoqYzLAO9f7K7kU62Cg6MsntDvdhx/qIdEHyc2dcij5FmZpjP7twSOLIZRN0nnHJBuU/nje+RHhfuDQhpfL1h/M5xUBrUfIONVuDU5e1+sr1DJsZN2Os5YkPDu45rmWge708TvHcDpPF1sFtnAnJVEPd2Lw5W8MvLJUp70Onbcvrp6QevmfvLK8rAiWEEEIIsRBNoIQQQgghFnLNiTSxXYWged7kwn/uj71bJMQKSe+asvtgCFn2mARtRrbAh8WEizEUWaFuUMMEb7Rs0AXCpIx0BmGfhASAQbqBVOdI1scQ7cAafHQzzkin9wLdDQ7JbKIFJs0kTGM4HPJPXd+42m5RV+kCYeK6Pb/a7mjvCdkZ8Vx7Sm1s7geJNBm6x+tNRSckQ9T5GdSQbc6Q9G41ZQmgg7R3hmOe4cM6g4QF+W+ckMBxJjx9r4QWwvA25NkgbQWDDl05+R4NIeEta9XRqcbQO6TXkc479g/UWNvm+0U3lJlZx3qL+IwdnK0r9McG9bAGtB/eb6d0Qe0G1+bO8QVSSiofk86oY8HktKGuYSqPd5QRmRhz7MvXsoENbQtJucdxttju5+4ha+3x9YMaZJTbGuwX6jcGCZqJkMuuSzozW8i948Q2yFqLtBIyeWZ++TQ906yDzB1sklZuU3T50tXOZQpc7jHiOisry2gheSj7NU8nSHV02cc2nrDcJbihZ1x/QQ4MdWDp3ER9SbYRJmtlQumQCJo2VNRa7cuOX6IIlBBCCCHEQjSBEkIIIYRYyLVKeF3LcBrCiUyOxwR6COMPWB0Pdcea4IChFEgnHWSuDVxrrLfFcDBDfXTeHIQi6WSokQEzhFCZvAzHWkPRcCh1PcKG/fZWPk7/RP4s3KTR6VaYcTOepH4a5Qw4bnYM++eLXOP+MhzeIcTeV1n+6mvcoCYfc+M53L6Cs22kBEMHH/MIInkcnRf7a8jbFcLeHSTfJuVns8JDa1DP7WG0zdWYEyyeoT12kAJry++tp7w/k6W2dK2cICmqmQUHEe8N69xRGqJi5ujXNSSQHnL0wK4ZaiSWa48NQ74vuyfy9rjL2wyxU4Ldfx4+sMc1rNCuVhgLwjWwLzMxKLbxHGokYmzQHx39sUIDa+gqPCyweQT6IAuW606mUI8vn+fFNt8T1vXjtVPNgcnPRvYpSNkeCifivay7h2UZh/eEdUCn8Dy4RADS21BeNsFEulRgw+u4R6wLGOr8sc2yrdSn+TptggxZrkkYnMCJ0pZhf8h2TPg7196N43peUrDb5nGqoSyK86E0PaboZgt17vBdUM8k82WtQrpZeZ0871Ajr6G0OZOEFedWY0lQfRffm4pACSGEEEIsRBMoIYQQQoiFXK8Lb2a65gjRhjpvSKDGsOxwK4eWO8+h6BbOPjqmekqBDDnTfYK5ZAd9LeQSqw58FkjeV4Vwff4QOn1qhPeZsMsge43bLFH0m8fz9SDk3oSQM0KaLD8VMqIdXyagc8XxwUw4xwgoXXgtar6t25tX21N9drW9hWOEtZpuwfdBmW/CMSs49aAiWVQ2YmNkUkZKeC2eWYP7uIZk2+LZnMGdd2b5PM4rSjg5HD7s8jNmvcehp9MS8mdwsx0P1jas6Lyj5sIEmEw+Bz2ERj1DSL4PJh7UdaSjC/LR7iLfo4tbWcreYXuC9H+YX7RZ5bZUo7ZdS4ca3X2sjQZ5o0M7b4LMSSkF0gDNeV6WNyjBtydIjEp3IuuZJWNCynKy4Fu7/N4dnL8N6rFRIqRiH2QxyE50yHJ5REcXJLYpL5pFKSmFGmtMVArZJhQapfzFBL55ly3a2pQgKRnbOPojliZMwYF8/HHWzGykm5PLS1hPdsa1FmoE4sts4j2mlIrnuUN/vHg8j1MDJLxdqC+Y3xuStnZxzOLjmVgjFcsZKOl2Z3kcpWuX0mBCwmTKcJQLgzsvLFOgyxNjnN0ZRaCEEEIIIRaiCZQQQgghxEKutxbeRJcYkiNWlKToYoFshbBfD21o6JFw8IKhyBw23LKmDcJ4w0xiuc1ISSJvr1YHoUg4cVg3J9TA2tHFxxA64t3QFScmB5wo28ENBp2ELjwPIfF8eLfjywQtr31LqweeAV5ewaHSwW3Xtjl5ZsL2DuHzFZ16dN61+Tg7WDOrLss3lGxDPauD+kx0pdAxtcZFdGg7Lfap4RhjwsyughQIubceIEkhJD2gjW+QSLN/IofP00HCyOMBmSUUZ2T9sAydrROljqq8vQs17NDG4Yq9uMjSwPbiFrbzvXjicfT3XX7vcFC3qlvD9QhppaPLaJfbT+rgANzgdyXbOeWT4K6lkzB4kfLrqSz5hUSXR2IYKdtBtgljBevW5feyNuUO0vw4UqbE4NKxHaDeJZ8H3cp0L3ZMTpk32yr3cbMDNxgTtQ7lJRFc4mCoZxfGe7q9e8qZkFfh7B2Z9LMqO8H8RKk0KVtZSD7K5RJlmTOF2rJ0reFlPKsd+mboX1hSsNvkPkgZ3LFegrX5vI990+CMo/t9Cklcy05HJs4OkhxrBFK2xPnRedd0+Rza4ATGed5F0mJFoIQQQgghFqIJlBBCCCHEQq7XhYeQdshVyXkcQsusH0VzT8v4YwtZxukyyLIKQ+wDV+4jptcgNNhDkhngRGg8S0NmZq1lCaBhEjW6IyjtMaTPsOHIUD9kn5r6E8PMOcwaayAh/L7NiTcNUtKxGCBH9luGtPM+DsdchfB2h9DzTUgYFSS5DZpmNyE822WZb0Cof+jyduqQxI8JPOEG2eyi02eAo8sR329x32smKuX+rMOE+zJe5LYzOWsTQsJJSO4JmW/iNYcEoKdx+iTWK6MMxbp4lEBYt461B3FtTG7K0DjbzmaT+9oGksEWz2cDCeAJ2L4G6rNTDLdvce9XSLi4XmF8oaMPrh+G9Ce4YlcV5Xy4RCH/NXg+DhfbgASgQdo7kB6PAaUqHp4JKUdIXjtIYT3kue3EWpyQLJkQucl9rcLxKxyfz5sSVEhCSWnvYKlEcG3iPnpQVMtFVHujtINng/1Zjo3jFOXYEdLmCKmK9TWrE0l4LRyQFdey8F6G2pF0npVrufI7h8sXtpDRE55hqEeJ7ZouNzpZ8fp4UO+RMlyzQpthHbrg7sPylTkJr6UFncsIKBfCqYf72PKY7DuS8IQQQgghjo8mUEIIIYQQC7lWCW/oc/gVEVGrWEeONbaYcA6h8Rar+EceB9vbIHPhmAwt8nxQw2sb3AcIvQ8Mw5sNcHicnWdpqUWosAu1gliLCLW+WJ8PcliLEGVdjtyGsLHTMYUw85SOLxNQ5plC3T0mIoOEgySRCbafvmYi1Lzd4d62kOQYVt1ARltBnlt3+dyaFRJ+Qi7sm9j0N7ty4lGH62fcZFk0oU5Y2tEJequ4P60hiU5TPr8JSf9G1lpDMsf1abosXUlMnslnSymR0gVdX3TS8OcZJbwUktzCYYVBYcDQtKGkDnfPBLnfDxJSTnQ34n7vIEtsIM87+yDlJJ44k+xhn4rnwcS2lD/x3mmgvHF8SXaENDYy2SRrE8KRt4MUSrl0CLeU8k9+tYZ0wv7VMOEr3MQcvFi/rOrmJTzWIK1xTym1VjWXFKCeH5dpzEgyVIKDQh5sp5Dgne1jKu9/RKrgNmMyV36xIRkkvgcoz/XYnvgdh+1Epzi90WlQAAAgAElEQVSueYILjxIe3ah0QtZwRR+OWA3apNP1iOU4rBtLGbIKX37Yn30QbXLEXKHGd0SD9hZkTki7/V30TUWghBBCCCEWogmUEEIIIcRCrlXCG3s4lyBJ0R2S4KRzyh6s48NQJDS8nrISPrdiLSGEKH2X5ZaLW/ncbiEEzND1sIkh4At8HhP/UX5iEkg6CbmdEDZ3Jg2jg4QJ1HhM1INzY+K7fMztcHwXXrfOkuXmsSxVQdkKtbSGC4Rkb8K5A4egV5TzaB+CdAqXotM9E+4PHTP5mENwjh0k0sR+DIcnyEeU5Ca4VfrH3pG3L5D0EtfP50dpmlJujwSbmyHLgs15fu8Z7vsxmcI2w+esw0fZCkkG0bxG9i8mt6M2gs7foNbcLWeNLUp1aO9r9OWRUmh8nu2q7FxKeA6Ut2gUYi21GrJUi37d8F5Qase1pZn6ZOzjU318eX0KTjI4kDmGYmwJDjOurYAjL80k7K1ZExNuJhrSaoxLrFO2hrze4B42hzVHce9a1Dhk7bmEOmrBkUenH+IFlNHbFc47lGLleXDJAmRR2hyP/yjNLCa2rSkjU3pKZYnVKYtzXAtu6Uwd3Lj59WqmT3DAp9TqbC8HyX8pExplMixhqBsuYQiNKR8X4zxrvzpku4oSHtyj/I7gDaBjkDUi51AESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFXOsaKIc4y6zZoXgp1lwk2A6ZfZs26B3WrmyZooCZvmF33+E4G9jPt7Bp9rRssjDhQQFa6sIXm7IFf4X1Amew469gg3YWWQ5ZyZH9F5m2q4ZrnbBGi9fZ0756fKt0i4K9VuX1B7duQRNH1ugLZIYfNvm6Ls7zeT78MIq+9lgnwnUlzG7NtARbFN/FOqQevxG2LFZ5sAaKa+zqYDnP1zNu8xodpisYbuW1USz8O6LQdSiCijbb475MFdYNtHn7JvrEoweFVo9FChbsmazk6LNhLQZTCATXP+39uPds4+j7LLA9ctlEw2zPmW7FMSQ+zyasfSgXha0aZnjHWi/a8XGd3F61TOmQP7di0W8+Ky7LCZVsj79whuu5wj0Na52m4ushVUVV9vczG/QK49LEpoJ7ssZYQfv4GukKgpP8IDt7+K6ouO4UqWCM/Teu6HuShOvk+iGuvxqxHnXHBZ1eXlfFRzmdYJzdk4/LPsUM5Uw5wMzvXD+0XqMSA66T77WE4tlT8eWwpolrSb3BvZ5ucy9SeS0W4fq4alVOS9Dg+rnmMeH11YpF5nkR5XPgWr+7yUuhCJQQQgghxEI0gRJCCCGEWMi1Sng9JLMaRSiDRZRZVFFQdUJYljZjxsaZDoAZkSfIM7sdUiAglMgwMY+52TJb+YFVGmH8kIqBxRwRTmwghzCDODOXM6txA+mh4WexqCvtqyEWye3jp8iljZth/y0yt996LEte24t831dNbgc3zyGjIt1Ch0KvCXbqilm5IR8YC5zifl4grB4KVNbxtwOfbT2TxoCW3eEinzczkfdIdXAL0t5b35ZTHUwIDVc47xEyxPoGwtA3c+qCdKIuO4U2lV8Pyhhi+kwlwqLfzII8IRzONBMJUv4OEpZDqgtpO/CsVpCDQlHxQ3mdNuqZftSe5WOt1pCZmDaB2cRhj58rbl7jhiGxsqURMjSkw8ljUetjMOCe0iVO636UsJhmABIOpCPawWn7p6zJYu7sQ+GZQVLhuBds6AcpKXqMpwP1SUhyDYsaowICM2gzy3SN75CGSwQmLN8wpljh+ZXH+ulUxYSRWbujbIXXd1zuESpYcHzB99063yMua+F3SI97zT7O719m1R92PA5TZsT7wntZhe8+pC7AtTWQevnVz31WzF6Pe1SjvRmW04wcy3BuTcO2rWLCQgghhBBHRxMoIYQQQoiFXKuEx6KStSMsx0j8jOOggqOF+1BiGHfIDj2V5QM62JxJZOFESJxXQj7a9QfFhOF26DoUEEbBVxbCDZIczmM1484L23Qi4ZoHSJJ0Io1wd6UdM6sfBw8yRL521Cu1xzb5HB5/HPKt59cvILvSRdk077za5vNgkVLKMQ0cmwaX0BZ6VEK4uD4sLDrSIZpfriD/pl0+780tOOwgE+zgBnz88ezI++13ZAmPoWq2lQouvAbyknP/s3M7BUHyhu6DUwpyjUHaCi4mGHp6ZiXns6WzDWH4todDC5LaxCzItAbhmdVV/C3IjNWU8FiotjuHhHcD2yxMjfd2zMCN6w8VBljflU43nGuiW3g8vry+3dKxDNi+0KbO2jz+MBE5E3FzaUFos5RUKIbgOB2qJawbuKtwdh1ckPWBvF5BSquZ7hvqDL9PnDIUxiBKTA7LoAcpMB9ztWJ2b2bMzpsDq2KcoGi7WczMTlmJFTa4D4pzWE3ZDu3Xmal/zPduB5mzOccyDbi6BxTethouZXwPjOk2WbyZ7ZxLdtAetizi3rNoMJZw0MlO9y/uiwcXJu4F5xDOc8g09Z3jS4pACSGEEEIsRBMoIYQQQoiFXKuEl+BiqpwFZVkUEvvTWROy1eXNFRxgO4ZTd3BfURaEZMBV+ZQb6JALfj+P803HibRwsgRXC7YdSSBrSFHdOt+LrmESP76X4cdM26FAZqJUx6Rmxy8mTGmT+eMuIHltJiY5zdtM3LZBWP0Wwu28t3TMrHCv6AYJBV2RMK2nA4QOx4MEbhXC7wkh4Bpthy7PW7fgMIRTc8dt7H8Bp16FZzMgNN7W+bPOKvQPXE8HaeqYDEwgCAnIKZNSSoXk0jVMiAj3DfTcoc7HPD/DtSHsz+LADWSuntI0Cn2zjVQHEh7D+yvInqsbebs9y+d9fjO/3sG5s8Y+LZ2ebC+UdyiZWFky6ENS1eP3zZ7HhLTTMrFpSGZINzKK/WKsrCCLM4EjJTxKWE3YP8suHCfpwqNseiiFseA2C6lThhvhupyY3BFLK9KY+yypqFtis8UgMaEN9jsu98B9LB793mHb4dhB5ynVJrqQ+QXPpS8VnWoTZVW+F247jNM9+v5ui2e+Kzvb0kHfDP/Gd/AEeZLf2UxyHGRxvN5Bz41+es4tcP28YZQR4Sqs36Wo9buiCJQQQgghxEI0gRJCCCGEWMj11sJDmLUJUbxg97jabOHeoPQ2ccU9pIeByQC5LJ/1pvBZKybxwv5rSBK3mhw+brqY9I6SXofQctfRYcg6TnAfhBAiHBR0B0DGYOiaLpVpJkTJ5JnTdPxkfTWukSHZgXJL2IYkg3PboF5cj/Os5pKO7ljPCJJSR3k0b2/47HGfuwOHhUPmTEwIiM+mJLWB3HixwTXw8yBPTbjmCl6PUPerLrcDOkd534/JBs6aZgV3KfsUwtstDXl0vGKbCSbPUduK0kgNKdhYv9HoKoLUjv7ouHd0te6vIcuE5w/dvNo+u5GTklaQd1pcUMdab9heMymf00mH/kX1CfuwHbF+2HQCFx4XHoREhSHBImqHcYkD2myD8ZfDaYNnRpmDCW9DzTa8HrYrJtVkouCDe4KMpLxfNZ4fX99C5g2G6lW5LXMNwoQvF285zkKOptOSNQKb08QjeFguKYk17PImpdqQFNjKYyGdqS2++5iFdaTTFu23OYMrrudYNn8vEl3xeH+N5J4t6vZVLdsVnjmTYQbXPeQ8jEE9HdVT+V7w9Wm88/emIlBCCCGEEAvRBEoIIYQQYiHXKuExJMawd1XBGQOXVUiOiDDbpkdCTsQ3z85Zhwsh5JAkE+FahD3peBvXWN2/pmMsJtJkPTDWAOtYx4fyC11jrJ/F+jt0NNENSEcMro1h3JA0jLLd7ZKaPUXobGKIlaHuzYA6dxPcaQyTIlx/AVksJHPEfaCrgo6h7oz1tnJYmNIUw/wrSH5mZis6rBCWr+B02W3yfXziiVznbotEmlHBQ+I6htUrtF84gBwJVevgvGPyuNNIeEzoGMrKsS4eHY0hn2W5Ll7HxKW4vw0T3TEZIhJb9gy913mnAdsTxoEgPZjZCsc6P0eiWrjqeI9Z6y3UfcN5hJp3kFv7hHGNDjiuIsDrI2vVDceX8BITFWK7xr2jxBb3wVKEutyv6UYNSW5byn9wQUKeo8u4ZZJS7H8o4dX4jN6RVBPj4A417ChDMkRAuXGC47XHd5FXdBjicweOobx3kJRO5MOji9oT6zTi8yjJ4Xmy/upAeT240fMm+3iQMynJITFzd84E1+gTG7Sdw+fJGnbomw1qLK4o7VHC4/Id5k6GzFsFFzWoKKOXk9lOTJZ9F+51RaCEEEIIIRaiCZQQQgghxEKuV8KDdNP3lFaQKAv7J9YxY1I6p0SG46Nm0qqlYwjyHJK00Q3D5Imsz9OuIPv0Ody4PxhdhZQ3WKMLTp/g1Mshyo61vurydbLWF2UshqVDwjVPxdePRbfOEl6Da0mQP5g884JyHqQXRnenoexOYoi1Zz0rhKrPp4fyuQ1Zvtlc5M81yEs3PSak7JGUj22BGtbFJn/2BjXvpmGHveEew/Ww5tsKtcd6FK7qElxPlGwbShIhKH08mJ8P93tAUk0oibFIHsLhIbklXaR0g+GhJ1wb5fsddMRQzxDXP4VaWPG3IJ10dDG13EZbde5EUzBrBLLt4THwfvEsEsa1kW5h6rwn6JusaxglWG6XlxAwsSm7AaUgOvJgNIx1E3F8SoEtXX6U//j8Dp4l3WD87JEyL5cy4PY2+IqjVFehTbVMDAkpkN8/vI/VTJLf3QmWSpjFpS896sJx+UqsG4s3oy0knB/lySDNM6kq+hAVvAE1K+OSirxP1ZQdcmZmNfrt2c3c588g5wXZruXSF3xGTZe6F/eh85/yH5fyMCEnl+X4XfRNRaCEEEIIIRaiCZQQQgghxEKuWcKjC4/1oOBi2iHBG5wrFdxHPpMojBdDx8xAFwP2Ya09HpOupwnv2O1iYq2QBDEFW1I+J5wH6z2dIdFfqNcT5D9IDCFpGOuNoTZYolsD53aCRJrtmjXFshxG91O1wrPcIFkZTi24IXCeA9sH3CB9oiwCd+UWYXjKP3CMJDpStrHOEaUaC3Jufnm75fnBSRdCxjgmk4GiZmGwhjCujFC1cxtuMW9O48JLA9sUJLwdHFd0ks0ksZuYeBMSSIf+29JFGmoVIlRPbb5h0jtIauHnX3yeXUiAiX4HacDjYJA/YkaOH1Bfc6KEFyRM3IuR70UtSMgE6QSJNIPMHep/leWfFnXEuFQgSJOsIwnZKsh5dM5S2kObYA2+IP2y7OmBTM2lGZTVRrTT1OMAI127rGvK7K9IDEnpjedalduK4+I8yEJ3rp32VOC4njAWpqnsknSOKexedMLy2pjkNSQexib65jlq5wVzHpyNDRzFwxC/f5gU+uYjqAO7zsfl8ow1amdaqHmIPoUkoWznHpJkcp0C2g4dlvjemQa58IQQQgghjo4mUEIIIYQQC7lWCW+EW4nb/S47miileV1OGtYYQ8gIRYf6O3w9h6iZEI2JGBmiZpKxka6EgxAtQ+VMaFlDrqlD8j1IOjwUXUYhrN0X9wnSANxgw47bOSHYCJnvWNSQks5v5lpjTIxWo3YYkxbWlMsqSERMdLdhqB771Aw3530eu7iVD7nN174NSVfzOdxCDT6zoAwFPYHJAfnIKIGExIV0cLasycQ6jXAAIQHcGknp1iH5Y5ZFmxPVwuvpEoNsN3V5e/PExdU2+wgThiZKeJB06IAK8gH2mSgf4HU6t4LtC8+T7rH9+SGRIySDNaTEoCYF+Tu/zv7IZssEmEx0yHFghHRxcWuD11Hzb3f8vslElJTCW0g+dLYOGE+Y4JcJEIMjiYMXb1Zw52FMG5l4k5oX2g0cVVMdx9ngesNzpvTGz6PESMkvtFPKOUyeyRqHHIuZhBSOr2mbxx3300h4lKqCBAo8VsnD63Bs01Xn5e+cmuUeaf5lstUq96GgtOO9Q19jO8ZpQgLkNZPtlhN31h0TvfK46HeUgHHeI9rwyIS3A/ssswBQ5pOEJ4QQQghxdDSBEkIIIYRYyLVKeKwt4xND3TmMvYM7inW/HCHHyVnTCeFXuuoQuquDtAdZMNRxgqyAMPaIELsfJgSjAxDxSzrpEkL9fPdgdOVQCoSUgvBrh7A2w49bJIrcoD7bLWxfXGTp5VjwHtElcX4Odx7kJspzrJ3HeOsQ6lzlXRKSstVUDCARUlKhrBscVTz/A5lgmqnPV1dlafcMSR9b1jJEW2N9xYSn32D/FZwnDz9y42r7oUdyYtDzm/n1bnWQzPVIUEoasN1DImZyVptQUw7XSTnEJmpk5VA63VqU7SZKilvI7uhzHWX3Okp4Tr1th7EGdQh5eo7si5R9Qh2vGYfaxSb3L9agpPvo4oISHs7hBPJ6UMko+aBfsIbdROWUDrNw7ThmcA7OSWR5D7o6d7xejA+Uzg7U2NBfGshnaaRsh+MyCW8qS1VhMOb3AF2uXr5OC9Impa3TfJ02dJXRhTbyuwnPk8+QbRbXMFhZwqJDLkiBTAi9xjHLxjarmLE2rpawCuslaoztLRL1cklNMiz9CXIwC48y6TaugDU+OcaPZTkv1h1kwtsyikAJIYQQQixEEyghhBBCiIVcr4THFfEzbpUQZk4zDg8vh1ODSyaVQ8tMbsjQKOvF0UzRUulJUcJr6rI8ROcez2Pa5lDkNm3xDtYrwj3quX85QV+/geMMUt0O0ss4Hj+RJhMgtkg8en4zS3gPQ4ba4lo2SEjJ0PAaMtwGTroBck4Pp1YFF+QO95YyT5Tq6H6DZcRibUaeE6W6c8htLVyF67BP3m4QDqeMzHqHK9y7Zz3rWVfbjzzr0avtGw9ll2NzcN5HIyReZaJHJpmDAwqKSUJInjkQWW+NEmmsz4WPZbidDhtsU5KgPNscjGQcIliTj/2F9cCaFaShcH50gzLUz3PN7XC7K0t1fO/AWpCnSKQJyazr6FqijJz3p+wcaqHRzcZxbCzfH0p+HfTRoJxxSMf5TJB1V+soU0+Up9D/tzuO/eVrSCExIp4lxpohSLZlSW6ixMu2xQZfHf9Zmh26PHHNTNo7I58GSZ3Piq469lmnIy2Vt1nPkP1uBccykpmOB0qYhwKKkPPpdIQrM+H7K8pwOFdcBBP4pjQj242sxVuWM+XCE0IIIYQ4AZpACSGEEEIs5FolvB4h7e0mh7q58p8r6GuELikxpJFJzeDCQxiPTqwgGYTXsRnqX+XPbUK41gJ06DBsyNC9h3pYSG45W2cHcgPcUHTqsW4ZHVNbJJPs8Vk2nSC0jJtBZ9gjjzxytf0e7/HuV9s1HJW3LnBuTIwXnE15n1twOd2Cm6nCPi3qnTHcPOJeUZrtDhJSsu4RXT/nSGJ5BrchpTS+fvMsS5isqchjssYh3/tu75Zlu+c8J8t5NyHhVYeN8EjUtCVRwoO0meC2G2lhqymx5JcpEzBhZkjKCCk4JNLENlXYjolpcZRxG91sI2tKtqwlSOct2h7D/nQ6UcJjzT/20yA3w80IqY4JfEN/PIGERwm6qnDtTkkOEguvHc+D42l0nuH4dIgFByo+F7/TuVSCddfoCuwOkkUOkNt2m3wfN9geIRlR8t9uKeGhXYfnmj9rwvdDqHNH6ZHONi9vH5O+x3gJSY7ffaFuapDeyk5CJqQM8h/3oXOadfGgj1MiZnJOg7zadvF58rMdtuqq5mfPLN+BVNuHpKeokRpqc/J1jGXBwReswPl8eD0zKAIlhBBCCLEQTaCEEEIIIRZyzbXw4EQJtfAo6SBsNpOckqvmufq+CXIei/owLMv6SzgO4rVlgSGG881iUq+JiQKHobgdnDtzEh5dA5ADxp6JN1FLa7st78P6VuVPuicY3l6vsxz2bo9mCY81tm48/PDV9gZh9S2diZAjLyDz3eI14h4Gpx7kAN6fUM8KJ90e2rYAkzKeQcJbQQrq4MI7W5dlPjrsWshFPP75eX7vQ6gp+MhD+X6dUzo8SBh5LFivjDIUEwUmyHashejOtkz5CH2TEgMkI7ZNOtX6HeWWfJy+wrPlpx5Imzv0hQbPqu3yc2fS3glHo4QX6uVhZKB8v+vZDiHbzfTHOiwXuLNMsJSa8llI5gqpFc91gIbVj5R5MG7i+IkuLEpBkEfpFuOAyqUOvJ9MNLvdRDmWks+ti11xv3FGLt3tygkTQ21R3BeHdIRbEcaRLdrpju7K4cBudiTo9q6D85ROOspt1B7xHEKtvvL33RTaI+7RRIkQ26ksQU9I1HmYX7RBO6nYPvEdv8N3fBNcgjyn8jOPEh6eJ1x4U6hxmbeDbNdE6bGEIlBCCCGEEAvRBEoIIYQQYiHXm0gTYVqGundIPpfmHFR1WRpwhish1dWQ/yjnUcZhqDO8jtB1FepHRQaEDUMtNdZiGsvumzkXXpT/ZkKOrAc047CjpJFmwqz3At1gvNdnN3Ldtmfj3t14OEt72x0TD1LOQ1JQyHkMk1M66Ydy+Jzup/D6RJdH/O1AKYLbK7gHKdtRnuuwD+vlcZ8V6vaxdh5lwTX2X7PmHd09fuew8lOhR3JTOrR2db5/TcOElnDfIAzvDhmdNQnp7gol1sr9nZJBSJCLZ17jHLqDBKMDZX6ML5RSK8i4AxN9Mmkibncf6iWW5SeOCWFpAgjmtsMsg0eA/aIKDmfcU/Spls8MjreEep2UvGsmtkQyTB95H8rur7AMgpJ6S/fx4diY99vAhduHfs6xmEk/y05pJgMdkFSxH+l4g3SMtkl3dE/ZedZZfW9wTKlm6jGOlKArSlLodxh32NfqmWTUE+5vFWr+8RxmlrFw6cRBmKaHVZf1ZMeZ79MmSMnl71BK9sFVymceXi+3Cybe9CQJTwghhBDi6GgCJYQQQgixkGuV8LjKnrWIULYtSC5MWEaZoJ5xIgX3XJD5uP+MWwEEN181L5/QhceQZah1RemNIU7W8aEDcJxxE0wMLZZDlJQLea+5fSwY9uQ9Wq2yq4xJMtsun9s6SG9lSa6fuQ8pJOLjPuV7wuMHWcfiswwuFsScWTOvDjLfTJ07tLUV3V8Iwzch4SClsHJCuyADn0CO3X8Gek9PGRn74F5WFfsI7yvlb7jcWD+tL7froWdov9xnqTHQbXbLDkEyQbh+WjqAIGmEumKs7ddQws+v93PyOp9P6PtlKYGJ/o4F5e+EZ9C0lPPoTmXixUyQc4IiVx5DeUwmjuX9CQk58WFMilk38Xc929o4IxOGdsp6cSFhJvsUJTw+V0q53J9ORS4LYHLVU8GngvaI6+nDGM/ng3uJWoCsEcpnxU+iKzZmnZ6pRRu+gVk7Lz7POJ7TcVn+Pg7ewVCnkm2BSVnLtTbZbodduY0EOe8u5HVFoIQQQgghFqIJlBBCCCHEQjwdJIcUQgghhBC3RxEoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gRJCCCGEWIgmUEIIIYQQC9EESgghhBBiIZpACSGEEEIsRBMoIYQQQoiFaAIlhBBCCLEQTaCEEEIIIRaiCZQQQgghxEI0gSrg7t/l7q+43+chluPu7+fuv+Duj7n7l9zv8xF3h7u/1t0/7n6fh7g+3P3l7v7dt/n7L7v7R1/jKYn7hLsnd3+f+30eS2nu9wkIcWS+zMx+IqX0ovt9IkKIp05K6QPu9zmIjLu/1sy+MKX0Y/f7XJ4uKAIlfqfxQjP75dIf3L2+5nMR14i76wehEFb1hPsAACAASURBVPeBB7XvaQJlZu7+Ie7+c5eyz/eZ2Rp/+yJ3/1V3f6u7/5C7Pxd/+3h3f427v8Pd/7q7/6S7f+F9uQhh7v7jZvbHzOxb3f1xd3+Vu/8Nd/8Rd3/CzP6Yuz/i7v+ru/+Wu7/O3V/m7tXl+2t3/yZ3f4u7/7q7v/QytPxADg73gRe5+y9e9qfvc/e12R37YHL3L3b3f21m/9r3fLO7/zt3f6e7/5K7f+Dlvit3/yvu/np3/013/5vufnafrvWBwt2/3N3fdDnGvsbdP/byT91lf3zsUrL7D/CeK1n3Uu579WW7eOxyvP7378vFPIC4+yvN7AVm9sOXY+uXXfa9/8LdX29mP+7uH+3ubzx4H59h7e5f5e6/dvkMf9bdn1/4rI9w9zc8E+TbB34C5e6dmf2gmb3SzJ5lZn/PzD798m8fY2bfYGafaWbvaWavM7Pvvfzbc8zs1Wb2lWb2bDN7jZn9h9d8+gKklD7GzH7KzF6aUrppZjsz+1Nm9nVm9pCZ/bSZ/c9m9oiZ/V4z+ygz+1wz+/zLQ3yRmX2imb3IzP6gmb34Os9f2Gea2R83s99jZh9sZp93uz4IXmxmH2Zm729mH29mH2lm72v75/yZZvbbl/v9xcvXX2Rm72NmzzOz//F0lyPM9usSzeylZvahKaWHzOwTzOy1l3/+T2z/PB81sx8ys2+9zaE+zfbj87PM7FVm9oPu3p7otAVIKX2Omb3ezD71cmz9/ss/fZSZ/QHbP9M78efN7LPM7JPM7GEz+wIzu8Ud3P2Pm9n3mNmnp5T+r6Oc/Al54CdQZvZHzKw1s/8ppdSnlF5tZv/v5d8+28y+M6X0cymlre0nSx/u7r/b9o3gl1NKP5BSGszsW8zs31772Ys78Q9SSv84pTSZWW9m/5mZfWVK6bGU0mvN7JvM7HMu9/1MM/trKaU3ppTeZvsvXHF9fEtK6TdSSm81sx+2/UTndn3wSb4hpfTWlNKF7Z/xQ2b2+83MU0r/KqX0Znd3M/svzey/vdz3MTP7etu3B3FaRjNbmdn7u3ubUnptSunXLv/20ymlH0kpjbb/EXu7qNLPppRenVLqzeyv2l4p+CMnPXNxJ16eUnrisu/diS80s5ellF6T9vyLlNJv4+9/wsz+lpl9Ykrpn53kbI+MJlBmzzWzN6WUEl57Hf725LallB63/a/Z513+7Q34WzKzEL4UTwvegO3n2H6y/Dq89jrbP0+zg2d6sC1OD3+A3DKzm3b7Pvgk7Ic/bvsoxv9iZv/O3f+2uz9sZv+emZ2b2c+6+9vd/e1m9r9fvi5OSErpV83sS83s5bZ/Jt8LGfbwma9vI5nzOU+2H2+fO7OvuB6WjJHPN7Nfu83fv9TMvj+l9C/v7ZSuD02gzN5sZs+7/IX6JC+4/P9v2H5RspmZufsN28t1b7p833vhb85/i6cNnBi/xfYRihfitRfY/nmaHTxT23d4cX+5XR98Ej5jSyl9S0rpD9le0ntfM/sLtn/2F2b2ASmlRy//e+RSjhAnJqX0qpTSR9j+WSYz+0tP4TBX/fFy3eJ72b59iOsh3eG1J2z/I8XMrkw7/IHyBjN779sc/0+Y2Yvd/c/dy0leJ5pAmf0TMxvM7EvcvXX3l5jZH7782/eY2ee7+4vcfWX7kP//cyn9/CMz+yB3f/HlL6YvNrPfdf2nL+6WS5ng+83s69z9IXd/oe11+Sdz0Xy/mf05d3+euz9qZl9+n05VZG7XB98Fd/9Qd/+wy7UxT5jZxsymy4jFt5nZN7v7u1/u+zx3v5u1G+Ie8H1uto+5fH4b209kp6dwqD/k7i+5HG+/1My2ZvZPj3iq4vb8pu3Xjs7x/9k+gvjJl/3vZbaXbp/k283sa939912aPT7Y3Z+Nv/+GmX2s7cfg//rYJ38KHvgJVEppZ2YvMbPPM7O3mtmfNLMfuPzbj5nZ/2Bmf9/20Yn3tss1Eymlt9h+xvyNtpcU3t/M/rntO7V4+vJnbf/F+m9sv6j8VWb2nZd/+zYz+1Ez+0Uz+3kz+xHbT67H6z9NYXb7PjjDw7Z/jm+zvfT322b2ly//9uVm9qtm9k/d/Z1m9mNm9n6nOXMBVrZfT/gW20t27277tWxL+Qe2H5/fZvt1iy+5XA8lrodvMLOXXcrfn3H4x5TSO8zsv7H9ROlNth9nuazlr9r+R+qPmtk7zew7zOzs4Bivt/0k6iv8GeBo97j0RzxVLkPKbzSzz04p/cT9Ph9x77j7J5rZ30wpvfCOOwshToa7v9zM3iel9Kfv97kI8SQPfATqXnD3T3D3Ry9D019lZm4KKT9jcfczd/8kd2/c/Xlm9tVm9r/d7/MSQgjx9EMTqHvjw23vKniLmX2qmb34Lu2c4umJm9nX2F4i+Hkz+1emPEFCCCEKSMITQgghhFiIIlBCCCGEEAvRBEoIIYQQYiHXWiT1sz7i92e9cMppQBwpQdomz+mqKue29Cm/tWty+SOv8iVc1oQ1M7NVm19vuT/2sRn5kjk1varweh32a5r873EY8mF5bTjWNOV9Jnz2FPbJ7x2H7J4feryXucvqGq9n+N4e5/ZdP/lLTBj6lPnGr/iYq5OocE+HMX/uxPPH9j4lz+U+vFd43k2dnxnvFSXnaSpnF6jxzBLu1YhzSwdZaEa0r7phm+I++f3jiAPgcVT47KrO202bn1Nd5W3nbxh8VminfB336Kv+0v95lGdpZvaXX/ljV1ex2eyuXh/G3HYcJ1Kj3VVNuQ2G7jWV+1poy9ic8LnsW7wZE47pB3eCbYC3sqpjH36SBmNEXZefYRWeW/7AYchO+r7vsT/Go1VuU03N7XzMl37GRxzleX71t//i1Y1hHwnPAzeM/YL3lPuEfhD60VTch/06fmz5mLdbScLPmNBxORbwuHV4xhw7+HIq7BHHMn7/ED5XPr+uy8/1q7/gRUfrm9/3k2+/OkX2Rz4rjqm8tmnkWDsWt3k9HKfHscf++TjsWzX6fojGhA4ZbwXvK8+bYzC/dyt874YhBe2Qj6oK55f7ddt2VoL9l+/1lA/6p/7jdy8+T0WghBBCCCEWcq0RqBZRIYYAPOWZZIOpJPdvOOtH1MkwO+UvjxWiCCvMPMPsFO/lL37+CqkROWgOZrD8Rdrvcv5M/nrmr4Fdn3/Z7wZGlDL8ATh43p8zbO7vuOaE8x4bRPVOYBTounX+LPzKqWYiBFWNX0sT9gk/eMu/Thv+8uevJdznCm+oGGTEb5YKj2U6iEDxM2pECBj9ahg5G8u/vPkP/kJerdAG8ZxS4jV7cZ/w8E/k+XjisXdebV9scltmJDNEC3EeTZd/5TEaFSK2vGZ8bmJr5i9qfBZ/IVoq399xiPkUedymxa/QFRMjs81s8SofKNpPiEBl+l3up/xl32D82m5wDrhHK9y7Y8GIM/umz0RmBzyCsD8btpf34fjmjHywr4RIEU6UfeU20cRodCpHLL26c7uYCwnNRaDqVI5k2UxEzE8UjthePH61PYZ7jHvGN3Bww/mxnW63bO8Z3q9+QLseGO1Hn2jKfSJE5Q8eKCO804xaY2HM53ZT3J/HDNErRKDGmWjsENpIuU3uc7++K4pACSGEEEIsRBMoIYQQQoiFXK+Eh/B5jVCZI7TGsNwasgcXbDNUzAW5lOrWCNtz0TkXwLVN3p9yAxfGMRzYNPF2dV1+/4jFo1wYx0WlFxc5x+YW4dQdFsMNISyL+S2kuhHHr7HYGuqGbXdceHt86iY/yxHlqBgCbykZUJLhvaooW/ETuEASC5kZVufCyRmZj/JwqhkitkBYbI7F/nx+9cy5UmpOXPAY5EnKJ+VwM9sypWabWcx6TMZdbpvDlhIeFpJSdsb2zsuLNikvO2TRGn1zoBGA8kQwZeR9eI9uJ+HxGU4jpXcuns3HGvCsxr4swVczq565v0PbHCH/92gjI8YNnygpHocdlhPEhblcjAupggvKeaAZgwefWZpZyJzmFnvzc2f0aD9s77NmH54r7nuikQUyfzUnGWJJgZclMo5HweAxs0TgmAzbW1fbPZdL1GX5bEJf4LjLsay/yMe0sDQh78PvtGGHMZ5yGb4Tefwggo/R7MMxj3LwNNceMKY06/OrbZp9uOyCkr3XXAhfNh2NMwan8IU6gyJQQgghhBAL0QRKCCGEEGIh1yrhnZ3duNqmWysNDNHn/ZlXg/k2mJ+hQxjvbJ2dYTfWOTTeQdpjfqF1l/dpEfYLeYAYwj/IIcP9GCqmnHALst2E+WpCyLGivIXtqoGMscVxEBKlZJLCudppoRsCDjsLjspyPiXeR17XSNcH8w/R8YXnRJXLvewS8SoV9z+MFlNW7CF/MqRb1eXrCQFrynaU6uiw4zMLlkpeEPNGUSI7TZe9dfHE1TYdOhOco442zpB+kLDwzHm/eM3tWe53jg5PV2WQxCnh4R5FxTc+0IpyLWW1DfLOULoZ4ZaFCzENbIdllyClXcr/iePXRNky39PNmD/rWAyUWunIqrCN+8g8dGOQ3nBQjLmUQvhZNuPCC3mZZmQ+Sv+eDvX1kF0M22VJNXoo8czoHgzHn8lNVd7bKsd4jb3qE0ntaWB/xDgXJLyZJRIzbdawLGLCcpJht8mv89nShYfr7HfYpoRHp+IQtU3Kx1ueaxgj4X7H93Rw2Fl+PahtdOpSIuzR3ri8hHnQ2NZmmgtRBEoIIYQQYiGaQAkhhBBCLORaJTwmoqQMlyyH+FqE9GNJhRzSa6FPreEGO1udXW3fOM+r9c/PsrRHCe98lV9frfO51TOJCw/NIAwnMqy9hWzHiOBIeYfuBUh+dM00LdxgCIPWuHezSR9bJpazoxMdjLgPM4nlKMNMKYeMwzOG/DMOdIbQpcnEa3OlDJiokyVzyiVh9p+NxIsORxfddixbQEcTjjsXxZ9zHFUhqRw/luWEIAU15XIE98pu+1jehoRlCLGPW4Tb8brNlLWZ6MgMiVfhqoEzt8KbgyROiRDnzO3gWjQzn8olJoKTEO0qlD7aQMagI28qS9VtBykFiTGbKl9zC7NdUJKH41u3KKVwPGH+YbZZyh/cn6+zew10ao0zzibIsQ20EJ9JZ1nhfvpBnSX28yAHzZRv8ZDMNmjkha35qiNpznXJMjU4fPSBHo9+kx1zLLUSzpvSaEj0iX2YqBj3m7JdjyUVCdJeQ5mX119WyuMzPHC/hXJBcMvSnTzBDzqg/WzzSgMbh/x932L5TuIyErhfh0RHNR3iHAe4fMPuiCJQQgghhBAL0QRKCCGEEGIh1yrhhdpYQaLA6ntWeZ+R80JttFA/jTWmKO0h+RaOv4aEt0ZyuwbTymmuUrmZ9SGhXDkZaIuQ/hlD5TgOQ8UM6TNsumaIEqHVkIgRp1fT8XcCDS9Kh/l1KikM9W/h2mLoOdZHxHvhqhudbh0mQsz704XHZxHq5cGZxXM7vAg+/xq6R41MeWyPfH5TaNdMfoqPYvI9m5Mh0FdoqfTTdNnG4MRJTKSJmll0pDH5KyQ/1szqIJfXlLycCXXx3Kj5stI6awqiXwd5/aC4YQqV2iE/UHKiHIQ20/a4froNQ7JOupuYoBLX1mFcG/hbFQkKD4syHgEuCUjBjjsjeXMMrTmesl3PJMOcZqQdLm8INR7LMlpwyx3I3R4LqxXPbwy1VcsSXkjCGj6b/8A1hByvdPOV25bP+vbujQ1r4fGz6W6cSRJJWYzPJ8jXU9lJyRtANydr6oVEopRU6WT3g/tCByBvMr+zxvL3F5cFjGhAvBWj8TqZCBvjcTgf1IUcyxL/HIpACSGEEEIsRBMoIYQQQoiFXKuEV1VIgsjEaXRABVeW43Uk1qrKSTXDMZnEEMHiFRP6MSkhQpShPg9DozF74mwcmKFC6kxprvYWJJBhKtdx67rsOOBxphnHRW0Mkx4/tOyQZyixBEchnHRVkKHgkqjnwvvlGnQTws1tw3A+66Kx/hVC3ti/mhiGPpD60I46uBmrINtBekL7YhtnLbgg/9K5gn1Y15DOxoo1n9B+j0nlTIKXXTnVlF9vEPjuIXNtH3vn1TYT/TVrtFnKBDhmnfI+lGdYKzMk4cQ5NzN1uMxiO2yCMohrYBJXOv1QS86RSJRJRYNzrUe76PO9a3ymHbYcB04h4THpIV11ZfkrDA9zyyn4bCAvU4IMyykoZc+M6XRq1SGJbLyeCeNxlLlZX3Lm7TP1zMaRcmZZnhynsoQVZD7sP5xKwnsi9y9eM+9FPzN+cZ8RfTOM0zOStWOfuD/6Gr6jQi3PmVqIZmbDrpw8lmOnMfcqxwKMQXWiMz2fU0s33xgad94MSXRxrqyLNyOLhnO+4x5CCCGEECKgCZQQQgghxEKuVcIjjOqFJHiU4ejeCDW2EBJGZJUhxJBMkWF71vaC7NPO1GuiHFQf1MJLMyHrmLCN4WGEVunumnGWMT9hjSSkLZ0IkEbosmAdn1O4QzzYHsoJ5yhl8v6wfhZddSErHULDK8hoA8OtITEanRp0KeL+49zqKko+TUd5I79O6SUk1jPKEnCcUCakXEEpN8jXZVmFLjweP2RDPCJpysn60pilLTrVmEizZ7LYnsn3mBiv/DyZPK+lTN+xziGScHp+fYJ8UEMWbQ/C7TsmX6WbCPJBHRx2eM6QQzY964RBaqfTB+PRCNfPRQMJu4NbaaI8e/yilT2Tf860u+AiDXZG6ss8DPpyVT5O3VQz+yAJMiV7Ouc4Th4slQjy98xyhCDtWVnOj1IgJUwuEcBHcTjdcbwo37vpRBIeE2nS2buhDBkcaRw72Dbz/rc26LN0nvE68Trd4QlyHpPfUnYbKIUeyNQD2icldWd/Qc3TpqMDn9817NdZOh+tLL3FuoVI2jzNSHh3Ia8rAiWEEEIIsRBNoIQQQgghFnKtEh5X04doJ91KCNFWdNJVZfdF3ZSls11w5aCWTo702QqSwfmNG/nUENIbKBnUBzWaqLcxGRfCnQNfRyiaYXZKQ3S+MLg/DtwnxNavNilVshbeCXL1BamVYfyRofHg0MF76dpDQstphDuDkg9kvhrOprplkjzsTnkFTg0Wa2oOitZR/qUbLiXKpYZttJEhyzz9Nr/erSBPNUiESqkuuJ5yd0wJSf9wDn6iLjtscrK+CdezvUCH2SD0jjB+S/Ubz6Ea83GqvizdULY7qyirYJ/QfilPoC0cPM81a31BeqQM16E/BsdVqCuWX6ecx9UCNQrdUTyYkHCwh2RS4yZ11fFrG9JFzDGxsbKkTCfsOCORBWcXa0KGZJt4ZnBIMpkwJZ/Gy7LdofspyGrBMTcjmQVpMJ83x2ILDj4mMoYkx0TJrMcXJMVyDc5jwrbDxJj8HjQ+wx3uEZP5hmSY+P7hdfI7JNzr8ljbhe/l/Dodls1BncohLOcoJ0nmwRLHxSAfMpkzatHCVchlJJRbxyBVow1Dwtv10aldQhEoIYQQQoiFaAIlhBBCCLGQ602k6WUnXXCwIVTKxFoJoXvWWQoOvoqJxSjjwOWGz+oRPtzCSRZEQfxjgivBLMoMPhPiZTCdkd8tQpGIuIZaalPQQ8qJ70Kwnk4MJpk8gYQ3jAzv0nnGkPGMww5nvWPdLiZlc0qfTLxJlx9qDiIHW7eCa2tie8r71AdhZcoEbcN2l9ssDSo9JKkdJC+KChPrvMF55qFmVDlR5zQxNM4Em6fpspTtpiBJZvnAEd5most2BZcVahv6Nu+/hsSwxnWuIBmchXpjkAys3EYo57QHbjY6INnvRjz3HhIjpeSRkg7Oe4sm09P1hWveUcRDx5tw/Ip1vk7SN3mPcDqsZUgnc5DkaI/Om9FhV05ePCfrt3hvjR4SHHnBLRb7Jp1aoZbhTAJMSnKhLl5d3mcIkidrHJaThHIJxTSUZcGjgnqUHM/ohIYh2Qb00yrU/+Nzy/uH77WpfC+ihMcxleMxpLaw/CbeF96/tivXx03BGYe+Cds96+XRVcnvl2lqCq9GR2blZZlzu8HyhRkUgRJCCCGEWIgmUEIIIYQQC7lWCS/W/ULoLrgauMmQK10TeC/r2VE+Qnh3B8dBFQQ6amo5Wdl6nR1TlAJ2u7gqnzWBggtoprZUP3KFfz6nkKKtobOAUiDuhZfDsqGOFUPRBw6lYzDSJTZiG0kCmaAuuJx4g0IsmdcLlxsdHdi9aRHCbRlu55mizeHnQqiDaNFhF2S4kAsSoWtYz7rz7KTaDfm4W8g2jvvSdXn/FJw7lGwhhTlr4R3ftWVm1tRwrcHpuOLHoT3SfbNiXUTmy2SiP8hHrGsJo6JlL1t0D9EZNMAZE55ZE38L1nB+OR5ixfEF7W0Yyk6kxFqAlG5owHU6juD0CTUfG2zn93q6s9NnKT3uO2tHTjuOIXn/FNxz6FOUrYIEXa5tRwm66/L1rlZYukHn3YzbkQ5iM7Oe5xFkHiZGhNsuLN8oa6Rp5rPZ+bn0IyQpZq21VG5Dx2RAIs0xSHj87oMEiqUp0bU3kySVyUDp1IN0NjGhNB8Plkt4Kjvy7EDC47jNc+ItZo1FyvastTlSqgu1GvNxoNKHOcHEezTjlL/YyoUnhBBCCHF0NIESQgghhFjI/UukGWSosvwyMVQaLCGQQCjb0cGGUNx2h3o7OE50DeC92J+JLYfp0B2Ckx248h/vZzgxJJDE/iGyygSbITskzpUSGCQ/yifGMO7xJbymOb/a3vDag6ODzpr8cnA5UWqkw8qYJJFyZLn2UjXjZmPSP1qeWGtpf1II45bLtoWQ8YA/JNZVouQLbZYuloT71eN+rZlgM0iMuI+nKbdlTZ1PdrWC44bPjRIenXRMVreFhIVQ/YBaVQMcjL5C36TrCecWJAkmvxxmdFczq2tqdWXpwtmAsM9um11PuwlhfOrHoQ5mvoY21ELE/UItvBa65UF5zaMwBhmK2V8hk1EWRRtnQlnKcy1fh2TJvkZX7BrOzK4ru/woifP+B6edmVGpSvi8HZLWcgkFHXkcoinhUy+iTBtNftyH4ynHCrTZE/XNi8fefrU9oc12KyTnZUJlOg/DGoSyBB0cq6wd2NP9Smka14/teqZGYIXElvsDcEnNDAPbJxzCdKPzhrPualW+/vDc0Me5zwbLfS52kvCEEEIIIY6OJlBCCCGEEAu5XhdeSOpVTrTF2kqUsJouhytryC+Uv+hmY8rLEQk5KeElZ80z1jZDcjA6FFKcb0KtsHFXdikwtBxkyJbJGpmMDe+FjENXwhCSgOX9WWONoeiqPsE8Gc6wCfFgqiqsZbjDCfVTOTEiXY0wUVlFZxBtW5TwIBe1HWt+IcTOMP+BM4SyWgrOEEqGTLKX92HYdwunE52KTGjHdkRH6cUOz3Wac3CdRid46Dx/Ro9RYUTbYfLIxvIDWrWQ1Fu4X5m0Fu6ZqCqUZbHoNmKCUTYwjCGHWpjT6saPYH/OIfoNwvVbOP122GeklhTOGzIkXj87y/uzLmJVU562oxMSSTK5Je9XGKNYm4+yXbluXQsHah2cd+iDGASDQZIyenhESFh6ULyTrk26VplIscfzC4kk6eDDiQwXSH7LRJXBRZq3mXQ5SmT87jpNPOKJd2YJj9+b0zovo6iQ/Ne5xIUXwYce+g7OOySwhHOW4yu+u+oZZx+l9uCyN7OEtQ1NV15Gw03K0BXvPes8Yp+WSbc5plCSpSSJ6xwvsuNxeysnEZ5DESghhBBCiIVoAiWEEEIIsZBrlfBqhBYrhMNH2p5m5LwJ2/1UlrBCLR26OoLchPPB/LHG+TASu6UMcVALr6fLDLIMZb/Ul6+N9daixEAXD2Uf1nSiDMlrxr1jWDZmljwKA51k3IYMZUjIuIPj5mLD2nFwaCS4MygjQY7ZMpyLB9UFGQmn0PB8KKMcNn3UKptJ3OchwSKcG312mG1DTSqElRn2DlIKnx/DynSOZoLz84g8dJ7PbwdpZUAz2lF6RLNm4kNG64OiQRmOfRnXvKUdjPcCknUfbIiQiHfR6bOhSzLkbaVUD9mH0h4T/aHf1ZCGmzWuB02pXqGPY3t1lu8vmmpwmB6NmZp0bHcNxpY1JLkW94dyHiW8zui8QhJONIoWiyiqqSydrLq6+PphUtRQJ4+JHjkWsFYb66mG/kVHIjoqX6fyhLY2IvHxEIYUfG/Up0lye+vxd15t8zu0Rmdbn53lN2B5QZDYgqkbYxO/Z4ztN39WhzbS8LsrlcdUuqW5HMHMbGJCbY6vXKrAcwqyIpObojYnx0WMF1weMw0sZorEsJB/uT3tJOEJIYQQQhwdTaCEEEIIIRZyrRIea1eFlf+hrlTZKRAShTmlLbh1ELpmcs4Brw84fEfnTnAI4phMtnkg+zCBIjWnHi6eFBK8sQ4UpTqG3JnsjK4fvrfs2nOEdHnMaTq+TJBmkpxSqmPNuwlNbUN3C5x3Ibcl4uQVpNIzOCpH3iuElSck9FudUyZgws94TxwyFNsOk/3VXa7WNjHsWwdNJp8fEkaOO4ae83vjs2fywXzMHuHm6TQqgZ2vma0Q8gvrR/VIRIdt1nzrIePQwTZUlF7zdb5z88TV9mabpVAKlcHBF+orlmXk/YeUM1SyD9ZoJ0yS2kPObVHn0CHbWUvpIr/crFkjMbfJDq+vzyCZnWAE9plkmJSjW46/kKZHSls8DhNG1kx8jD7Ys6/cyO+d8nF2kFEGuKC5tmI8qEE2zbgwayZeRd/hso6hx3FxnZQnefwBfS3UROQ23dj8MC+3uXtlBzfYel2WvKoRy2Mgw3JpSpAzcfwVlyy0dXH/NiRDLUvcdAJyrK2rmLTYV2V5zkI9Q/R5uOJHfB4T76IUapTqcBxH257gnOb+FRyZJglPCCGEEOL4aAIlhBBCCLGQ65XwIIdQnaJjjgkOmTCzquliyUk1GepnDboU3BRYrc/kW2c5ERlrQPE8GRr2g0Rp9RqSEEK//SaH/tKYw9F0h9DJwARvDeuK8cP4jxk32MSEfrz+A3XjGDgeYNVAV8LrFxtKL0yeycSbN3eCjAAAHPtJREFUkFFYOw735GKL0PtZltEc7cPRJlpIJ+26HP7fXGS5yMysYhJASDvbAQ67kbIwrh/h7QkuxCBnQrbc9nT85ddbXLM7rjO4Xuwk3LwBeRKJTjd4bu1IyQthdVzcFrId3ZAJteA2SKT3OByZFY8P2W4HSYcJNilt7vrokE2stYl71iFD6/mN3GY4Ejr0gJtsY3Tb0elZlV9foe2xFh7UWetYX+9I0L3LOn0V65PBpcicsnVw4cHVO5UdTOzLLZMiwrFMmZ5LLujNYsLHiQOBmY2QBtlfpoHjIPbBe+upLMPBIGkDa5eyHiflRrrwEpx3GPuqEzxLM7OB3yd8JmjLjvvtkKCZoJS1Xym9wlxqHfah/LteQY5u81jR4xlEdzFce22UNuuU79nFrZy4krU2G/bf/7+9u1tzE9mSMIwA/VTZ5XbP/V/lbrtKf6A52LOd79JA92gelY/iO1KrKQlBkuAVGbEYz9ceudnxrAxdb/7tc3Q2u8yG5RgTc9Ptml54IYQQQghPJw9QIYQQQggP8lslvB55x550OmCU0lzVr9Nrs+JWEh06Q3EKtH3Y7lv4mPumTKCBzR50XXcXUodmdjm1z50J+BsNF9MxqGMBGWdake1mSrQzjq7itjMzzsZ4T8KQSCVPd+GCA+ZKibmoUOgi9io647zY4dY5Uc4/4O7ZjE2O2SHN9vSO21DOPby2MnTX3cmcjJfdxe/mWJOYeaXsz0/uPpTqKCuX8MR+uezvmBiHZQfMMxnszzbyGjlrRG4dDcNEtjRsUglg4vr9mNo18T43eUL58+Nn2+bnj+bUszp/0Ulzp1Pr1pm5bt/evra/3yINOy9gjTsPhEMWCeTK+/YJY9zS/87ejls+U2nkWShn2WtyNly0BAfrBF1elmDW5PmE/MccOHZt3jvrqnO4s0TBHmf2Dz3czemXsh/8NkRALwt7pPU2S+XlmWMxMUdfua7ni0sidFMvh3OeL1V6fBbKoR6nrcHMtuxUSlPCs0eckqdOY+bLHefqBafxK27kCUltYJB7H7t3gTsPd6MymQG77V3vxwZWX5CbbyVgWfmwbX80GPXWzrkzMObablyZmyUVqBBCCCGEB8kDVAghhBDCg+QBKoQQQgjhQX7rGqiSXaAGb3LqzobD2NG3y5q9n2kEgHEFt6KJ0iBxj/Wd9VA+V2rTPN0l5LoGysaYVxKVLyQt92r+pt+ahOr7rsU6Nc22pC5vlrVmNe7rfUrzE7hyXC7YQK80hNXKuh3QzUtDUMdB+/xpcn2LybWuHaNJ9I3jjx3eNXVD3869lt5/fxZrGUgNN4F3LLo+NnsXPpmuXJLYsbQzrg/EL4y836+kSXef45Tu+r4db1MTDpxDU/J7Fl2c2X6kebNrZWb/qYZNfTq2//HOOPoxtWvor0tbJ3VlHUNZV3c3xLXLdyWBvr09dMwvRF+MXxmTJD/3xBJsjCRxfRNrwPZ45V0PZfr4Yf/8f8Nei72/HetxZD2U6fauGdq5Px5U8wOw/bt+inVFLKUp611tdOu+DRyUl7tmwhduU66H2u2W7wknrOhTaTjMPhmA7bzMmrESOzM4D7QTbpq663OeietyhpW1S5Pr0lifZ+PuwXXEfqa/jeMysu5r5HsH1hTujT2wa4dJ711dg1uvQedq/p71Z+V3OrYZC5P3ONdPuQDP88PvKR1CNsvzxhqpQIUQQgghPEgeoEIIIYQQHuS3SniWVmekp2FcTvvekgK8HY0uIKV5UMZp37VFqjscmjw3UH4dSVTdHUglxlp/xU6/HWp69Yjcdthjx8fO+UGp9GaJm99zwUarNKTFfbhaNteObCQyabnKJJ8g4ZWoByS5HpnnC6nhNyS2U2ms287T1SRaG0mXCjAl8yty57l97/lIuZkydGkYPdR/Oyh/znacnrG0n9o2H+9Iux/aadv+7RmnQ49cjJxpqu9Q5D9K4BwXJZNnoqK5VzNQxjHhWrm8xHMYV4F0Zqrxrf3mKyXzGenmLxp52pRZ7WHs2+ecz3cWcnZ7v0UmJb5i+N7G3v7PNke8fmvbHBjDppgrQ9qH+fBCYvNKur2p5EP//IgRo8WNZ9lyXrfEFYy9cQKmQXNelcI4l7uybAJ5hYgJl1lsaCzrkouRv/V7u67rBqzoN+ZjYxBMJT9/tLFzKdECNj6maSzz8rjy25wHrlwHc2me/jn6+t5xx1z78d7uRyPn84pePnek9RPRsN167Btb5ktXJpyUZE0hcOIokj2fw7KOruu6iYRvz1Vf7rs2dG9feKRzwY+//uI76PjBFGm0hGn3E+9v/EE3v/efYylSgQohhBBCeJA8QIUQQgghPMhvlfAurKw/vOBQQW5zJb6JorqS9vtWVt/uW+q0UtIeN98rydT7vSnVyw6CiRLomdLylv35976216MRsZaBWeF/Mcnakvht2Vlk+vigtIlrwpLjpqSbt03uS6jPwZoucmy/7KqrbjglL0rMVv2RhTy005mmtMpoe1KlOf6jzgs+6D7QW3fX5dT27+d7K+N+/KRMftZNZAxye//1oITnGEfG0G1XfULtI29V0vgMlOFMzL8hn4zFDYVzq+wfY5PP2ToukLvHA5Ln1ya1TaSh79/bsduyva8/jkgyXU2WH9gnE8e/vX359fr7n2+8j+S3Xf43Zj/hwGWTHa5NBZ2+UzJDmh+e/29YXcfazWbdRu4DF+oVKeSGzGU/2L0NlnG8nkiP98f3W5uN27gXNxsS//kuAbqMfmSYC65oJdyrzadLk2klvLaNPYCvHiPdeezT1QbuqD9TV+8Pz2K3Z5kDjdXnazveEx0phg9kKyVclq94z9Xx1uGw2wzt8/96x126U4bF2cx9yU4A17tOGMpnRxslM/9dy4Ftrz8+2j79+FeT8By3fZEPef/m+KejhUsWWIJQTu4KqUCFEEIIITxIHqBCCCGEEB7k9wZpltAsyuorIWWGXo6WDXE3Kecpnyjh7Qkr/PrayvZKeBPlvTOdDCdK4O5D13WdeW816xD5xYA3dKMJa9mJ755K40UbJCI/FanOJpfLEt758nynj/swchxPlpJLyKmSF41YcaSVyj3OiBtBoxdKzAPH5yfNcMeNIWy4HUt4XJUJDLo8YrY8ItsdcdttkB8Gvm+3EozZEfo5ss2G9yfkEJuX3gyPvHyGHHsnMWk+MnCOa0EXntddGY+EI/aE1o7Y1nYvBCgeCWHd8nr8s23zrUnwOxxvNnruuq6bcGtdkHq1Fh24npXw3r7SDBz54IxMuCnNrtvH75gUNrrS+JweCbc0ln4StfG6wcSG97ZBfrwoc7Tf2M9IeMju096QW35X0cWdgHAfc03cdjiFnYuvd5KPjYmZC7wWDNg8cQHrPNMNWBsct/dtxKtL0IDcy8Rtc3Lsf0494uW1XTv/Or63rzaQmHF95HgrpelMH5l3L8ifhiLPzAkGkupq9x6t49OGztPdEgQlvCuvPeu3kobZvvtEoPQ7LsQ6DtufnnHzeh8xFLl7ZYnA1mVA/9zoOxWoEEIIIYQHyQNUCCGEEMKD/FYJz3K4pUKdMfaVUpKy9DtaliZ8S91qg6tj1PVEqdiS5vsH5UBca709gO4eN7cGMxrqhgNIRW7D71QCsGec0maPNLRZkT8NDFRuuSLbfZbs8x8s3eowG+xxiAQ5c84OSDtnHHYz7jx7e20ovd4I0nz/F8Fo11a2nb8sh3N2fT0mN+SHI06P0s/tzN/bb4/+XlsuqcHXI+Gvjlle32bPMX3xGB+3TwrrU0pynBsyp6y4sceWr/lpO/tXonMNWLpev7Rt3n8yJxyapPbljybbffneXm+R8I6n6sK7cN6OfyFRvOO+Yjz88UcbhwfckwY0fjBmrh8cFy54peGtY/WGu23SSfX8a/NMCKEOMxUmlwq8I3kNc7t2BkIYO92IJcCyHdvd1jFLmKOOY6T2jkBVm7DNlxpgOE3OrSx3YF4rfRHto1ZWRCwvlbiypKD0TcVhdtu01xv7cfLxn7BS4n/2wyBSJcO2jQ7LM2P2aACzNudNO8bvzHcfx3ZOTvR09Xt3e2Qx+9syN13Lja/OWc4XRZ71/s0k1Hv+uXZ+/mA/CL0sx4hxcT03+fPw0s7nC2G5e54zvhy+df9EKlAhhBBCCA+SB6gQQgghhAf5rRLeERdLCePSrYU8dx1wBFjGxELRUwIsUlhxhCw7hizpGrI13JbLj8NdH7KN/21PJPuqafew3st3GIw50jPNQLCJ+vDkZ+r4U97xd36C7LMtrhkdcAahEgTKb+wPTYaxyZJBqPbO8/jc5laqHXQdsg/HH5Rzldp0Qc7VGWKQpuPUgL6NYZDKc+xr6duGZmLInIGZ9nwa6POnrD3zOZtP6oWnB2bGSWlbMiW8Or7svdbePbxwPRqe6fVrfz2cqTp9Xt8ot79teY1bbkYO6rrunVDH9y3yGSGZ9nd72SM/IcPNviY8dVSuUY5H9nK+UOKv4bfd07H1pV0wnTY2yllcOwMSyY7Qy0lZV/mL0NlT3465vT57xr7zQBn7XuNTlTWvk7+C81HmXCRVlzh0y3LRmb9VvteC6nV3Yv79iVv0nev9vRpBn4a9HF8OhkoSGMq1c/F3qisy2C5K093yfDet9GXt6WV64tzUPp3r85TXl0tTZnsMzsv3Y93o7wSmXk8s2+C7XI6wt/cicvPItX/A8TiMftIyqUCFEEIIITxIHqBCCCGEEB7kt0p4uqkM/rpudXVMi6+7lZIzmZfFqdchBRrENVPSnCkfjoZTGjhH+XC+D3izFI8+d6OEasnR32xfwC2lRSUaY7x2OwL92I9hVDLTFcjvmZ+vE1hi1dnYcxym2XIwJWBC6bZjK5kawnilD1cJCGUfen+X8hyl3XdcQjfH0F1/Jp2Tx7u+av/hcDA0TlnR3cDl2RMSapAmMqdukw3HxaDS2iPvc/ri3ZDt5knHWNnq1ytH1FhcmAYRtm22O36PahY/Z0fJ/FL6QCIxIZGNusTuglGvqDiTeabKAUp4TBcD1rUTgsB+6xykdN5QhjS479YRVsm0O1+fr/uccbGdncsYRlvmyoFg18HzPTUXlq4q1W+P58YwVq7NDedGB2ZX5ve2z/N855DldfluxlFtn7csz+mwm5Ddlbau3FA8ju988cdV2a69f7p+gh7bdd3bW3OkbrgrbPqf7bun5jDTFTywTxdlSz6/OPVK2KTHq23vcgnV1SLg1WTpQpnDeW3PVo99OdG8vJzpBeg+MXfscQJ//dIk/x2yna+3vN7v/vl8pgIVQgghhPAgeYAKIYQQQniQ3yrhDabs6b4xPFI5iPKrxTTdF0og9l7blLovoZJnHUYmkSHHGYxnAORdIKWyjyVLP9eePoaA6UBxVw3iK72VlHSKvGOPuRWJ6hOsPldKprrktGFZoZ+K60X5r8lcO0rPN0rpJbRxq4vHsr+/nbK9bdA4f31x6lQX0LDxu5FUB51ehlvSF22wFx7hmRwXpeayHyuhmrpbxrv9fhZeF2eCFW/252M47uwTxTVY+l4RbqeNazvq4EQiM1+Usv3ASdwged2Oy9dZ13Vdz+cqS8w4vOzBad+3jm16vntESuzRunTY3XAx6Sbre5cmEB74Cdfm8bL8G0/MUVtkyoEx63WkW+oyr0gqOjCV75HIStgxIZTFdTu5HOJvjslKT1Cvr7lM6/ZkcxtkO8bsFY3oQhDwmWN34nPOfNn5ztn7LL5///7r9dA3qU6X4IXzcJl//Hp9wiE6u6TC4E3ev+GqPHRtXHTDskOu3luWz0G/uTsuHjNuEqfzSjBqCdLkHrzS/85xuyUAdrtbdt5t9/TNJUR3v/vn+lIqUCGEEEIID5IHqBBCCCGEB/m9Ep5hmJ3SwLi8jU6ntb549BgbizOKMrnuP9wwlqJ1y1larqGYNVirViYpcdrryvIw+3HvAvu1DbLHBSeCDsBSNC37sFxC/ozKsuVz5RNl2gN11Su/94bLxmNljzh7wXleS1Ah+3DFdrVR+lyRY/u+Dn0djPvd8mWhhOfpK/8KuVnqpn8UfRcH3XYcL8f4zeC6jTL150h4/qBNKZ8bRIpDRamZQXgx0NCwWD2l8/JY2CKZ2M9qq6SKHDcdvR7vQu+QLpTwvBiUy90ndd/RIMa1sFGlKx2MyI1zr9zg0oTnn09DH99t/6i8OrbXB45DcQiWzOFl+5vyzI1jaNDuyLipcxHSr/eDu3lsU2Qi52nnZnaPrb0GXTYxKzf5mUpVXLPnub1/xE39wbE+1xUeT+PLly+/Xrss4sTvOXIsftDbrldSPutyRS5jvHuvvLm0Zi0PmnPjOS+j+u7+o+TtfdAwVKW6sjRlRbYbcdvpan85EJL5pQU4v35tjryvb19/vX771o71bh8JL4QQQgjh6eQBKoQQQgjhQX6rhFcdRwYIWpb1L5AJzFybViQ2yslrpeVO5czSMPVAZQXLvvduNnvP6TKzLFncQSshcIYA6kq6UBM2sK5IhGpJuv9sW3Z7voZXHIgcF3scDrizToSenU6eGy0zyCslnLFtMlE+v1FWHzh/I/twK4GfBpCSnNjVQMe5txbvPrXP9Rz3xWHXPnez0oNRd6UOpdIXztL4uByw+UwMx1PO03xlAqYuuV75jN278f5m0glX7JltG6S2QVes4bf8qbJt6dvVVafu9aQkidyoO89+lLoEmWsux9ZvayrHi1BgAmANANXNdz46ETx/Cv5xpEcYQYoTwYDzbvn9HWP5VEI1Hb/tbWdE5+ie8zEgpypNb4qjzg/qCs7fJcC2TB38NvbKwOLrdWXHmWv8HB2l78ixP+3/R5DmuQ7Bp/H2rQVpTroHuewm5gWdbcqwZVnAOC1vX+Zg5Ez2x3uL7e80Yxuou7ndyev29iw6cXtZeqp6sjYsheDG8MKcf7C3Hcsx3pDqvn9vr//8rz9+vf72tcl8+8M/X5upQIUQQgghPEgeoEIIIYQQHuS3SnjFuVX6KfGaEqLl9pmS+fFnC/rrcT1tCMTqSpCbThrrvpSGKQfOKy4B5bWuq2VNA0B1E1yRInSiaei7EXxn2ONFB8WKU6I4BpVDPkG2k3Gr4xHJcjlXrbgkJpqTzStl78lxcLOs3Jxwt9L7b0U21cFlr8Q7F54OPWW40v+tBJh6ApE32CelN11/m41jU5vUcnBs4ZNO6+XSJIrpoksSaUTnDlLVTcm6SKFK7Y4RJWskkNOys21zY7zg1LtN7TjaR+vfn4v0djIEknHFNb/dbtm+bX7lWCjb1aDe8h+L39UVp559u7qnc74o6zOuGVW6qnR27XqPL/MPEtbGa6oEBXN82B/Db+2vpvRdlOm7ucvruXftgP0veb/0WWV8XaflecE5y1DNCefkmQDQD4bTBzqnveaeydvbt7ZPSF4Xrx0G0ttbc5Kt3aP2e9ze0/IcZzjpWi9E749b5viu9Ei8c5o61zoe0ACdU3T3uURkNAyWc/hCz1L733392o7L21uT6nThfS0SXl3msUQqUCGEEEIID5IHqBBCCCGEB/mtEl5x3uEImEv5FfcGbihlON8/Iz3IeMGtcFFWoZw8rAglGgN0Otz1wrtQZhy2hsVR+jUM09Jnqe4vO/iUVQwfO1+WXYI6j/yy/u96S/2/WS7pXtEmbwYjsg9FzqOUXnefz7EcXFxrlJg5VgaQFhedNpG7fzv0vXJbt0jJIWSb61pvRoL49vsmPeow7EuQJp+JXHQp4XF3jpZnUcbjsjx5xUk575S8kP+K6ZTz73W3XZZwzwQAKhMVeXZlKJ/vJDzH20yfP2Vu+9DN7FO9HpEzGegbezva0439MHhzt1O2MCT1+dem7rEzMo+j3/5iR9yCA/t8m9AysWNWCU8IrVUeNQi2WJGXJb+/b4XHvOY1iA55LbKoMhTbr/QKXTEedheWhFxw/yp/na+foMd2XXc4NBlqfyR4dteWsuyYX/b7Nt6/KL1xYLd7j5d9WR2bbR90ma85FZ2Pp+t6qmi59zOPGGDsspA98+gLveq8l2sB3SMZK8m9vrbefs7HWz7f43g40AtwhVSgQgghhBAeJA9QIYQQQggP8nslvH5ZQrE0XkrxNyUg+moRdGePuNPUSppng/6QbraUCe2FNQ7LboCZuu99WVLnx8T3zSuSnKXP85qTbsVVN+GIUcLztaF/ymq3T5AJDDNVXvRcUqH/X/2t/sOau+OGzFVDRFt5WkluM7Zy64BYcUWCMeltvgukvPjdysjDimRo0KNjxzBMZEHDMx0HmxIAqEtIx6pl+M9hi+Ry2bivOjsJpLQn3XlNDlDa5DfPrWReLndcp0q79Zxz3pReimzbdTWEd1rcru+9HnWZGaqrpGWQKF/l9Y6UvFWqY6otzsv7kMEn4JwwlK9q+3m6cRyQ50bkvI2yXRl5K9c+Y8Vju0MeHYtTy5eGIN/NV7r1VuayMu94Ta28VnZ3qcHsddop23Hts80ZnW+aPufq3HCvdA4aR8Mjm/T08uLEuyydH3Gmnq9es40Shsn15JytZN+vzVl3Vut+5V6gHLj1nm3ILQPa195fdgZsvrTj8uUV5x3uvBfeH+1ZyvFdIxWoEEIIIYQHyQNUCCGEEMKD/FYJT8nFcuLGnjsGT1KtPdMXbtjoTtOdZ3m/vRw7pBTceRsqi8NASRcvhqGV013qnSXxzUDpWwlkXi6P6gIpTj9dPwbusdFpRcLTQdGVUu/zT3PpyWb5vOhwbOLf8vpmLyTKzXPpf2S5vZVYPSYlj3Krs6O9X12NXWHgu4t8ptR8833kGZ10RRpUhvT1cu9HrwMdMMXl9Vn5qErepCwWGcoTN7djaUl+VNsqr/lbJINyDbq90qGt9nrL+Y6XKq+Xw6RrrF+WmeZZR9hyY7bqKtUZyv7Rb3EsEohaJT0cP6G34aU4AZGnOK/KczckUt/vO+fclRDKeVnCk5G+a4Ytej9Q8rnv3dkXWyySmfJ/uUbWpDonieXr0et6LstMeI0TuC416D6FcddkqNdX7ke6greGZLbtjzhQT0jk7zheLyv9XvuVvoXKpaqWTv393zQ39NoxSFNKf9HN8j1eE32R9vhM5eOXl7bM4w133gtuux2uwBIMukIqUCGEEEIID5IHqBBCCCGEB/mtEt684kQqDgq2L9IFZWb7XB0pS1q6s+TYI2Hp4LNcuSny0bIkZZm466oDsKhnBjbeVmQMnTt8hwGKV5xOHpcL0p7l+rMuppVj+ixKmd0yLjXd2l/QvVgumRcJsphtlt1cyk7z8ub1/HXLYYld13X9hiDN5apyOaY6YG5F8mvYk8wAUDcy9NMyvCXpymeEonbdfo+kruzj+TQQEdlyu1uWbb2+dFnNuriKTNL+9DrpvFou+Svn/a9MXMNwS+NJXZIr20jp6eX7bqL85BhTLuTYkcK62/9zv61HmSfngWU574ZUN+NAnK+GZyprrsiXqmJevs6HLJvoT9oCeWlw7t2MtdY3dV5x8HZlCYXzoEs8liV7x9StbOO4oa/lsNzv8pm84hhz/hu5Zl++NBnKOcX7gyHVynk6Zw253JRlCsvX/tq9e7M2ld9tWZbdKHmvuFyH4rxrjOOytD9yrXnv17VoeKbb/F/um6lAhRBCCCE8SB6gQgghhBAeZHPveAghhBBCCH9PKlAhhBBCCA+SB6gQQgghhAfJA1QIIYQQwoPkASqEEEII4UHyABVCCCGE8CB5gAohhBBCeJA8QIUQQgghPEgeoEIIIYQQHiQPUCGEEEIID5IHqBBCCCGEB8kDVAghhBDCg+QBKoQQQgjhQfIAFUIIIYTwIHmACiGEEEJ4kDxAhRBCCCE8SB6gQgghhBAeJA9QIYQQQggPkgeoEEIIIYQHyQNUCCGEEMKD5AEqhBBCCOFB8gAVQgghhPAgeYAKIYQQQniQPECFEEIIITzIfwOGrZ6p3/TNGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
